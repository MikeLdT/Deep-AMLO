{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310df869-d5e3-413f-a72d-3142e7cec6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 4032\n",
      "Validation set length : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv('tweets_clean.csv', header=0)\n",
    "reviews = df['text']\n",
    "sentiment = df['category']\n",
    "sentiment = pd.DataFrame(list(map(lambda x: 0 if x==\"M\" else (1 if x==\"N\" else 2), sentiment)))\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(reviews, \n",
    "sentiment, stratify=sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/jupyter/pytorch/\",\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=115,\n",
    "                   truncation=True,pad_to_max_length=True)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce28988-73aa-468b-be9f-efd9e87bccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5041 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     1\n",
       "1     0\n",
       "2     2\n",
       "3     0\n",
       "4     2\n",
       "...  ..\n",
       "5036  2\n",
       "5037  0\n",
       "5038  2\n",
       "5039  2\n",
       "5040  0\n",
       "\n",
       "[5041 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d384ef-dad7-4070-bedd-ae235aa929eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 32\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler,\n",
    "                 batch_size=batch_size,\n",
    "                 num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks,\n",
    "                   y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, \n",
    "                 y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ecb7b2-7598-477b-b14b-b85a83083130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jupyter/pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/pytorch/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# set random seed\n",
    "def set_seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed_all(value)\n",
    "set_seed(42)\n",
    "\n",
    "# Create model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        \"/home/jupyter/pytorch/\", num_labels=3, output_attentions=False,\n",
    "         output_hidden_states=False)\n",
    "\n",
    "# Fine-tune model\n",
    "#nn.Sequential(\n",
    "        #nn.Linear(model.fc.in_features, 1000),\n",
    "        #nn.Linear(1000, 128),\n",
    "        #nn.Linear(128, 64),\n",
    "        #nn.Linear(64, 8)\n",
    "    #)\n",
    "\n",
    "model.softmax = torch.nn.Softmax(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed961fa-81c0-4cb9-b13f-01fb1c760dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9417968b-9fe1-49b8-8f90-b33137c32bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 4e-5,\n",
    "                  eps = 1e-6\n",
    "                  )\n",
    "\n",
    "if run_on == 'cuda':\n",
    "    model.cuda()\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "            num_warmup_steps = 0, \n",
    "            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71750184-c6c0-4add-b549-bd3dd6771234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1edf75-9151-43a9-809d-55b66b26190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Training =======\n",
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "batch loss: 0.949037492275238 | avg loss: 0.949037492275238\n",
      "batch loss: 0.9402865171432495 | avg loss: 0.9446620047092438\n",
      "batch loss: 0.8370596170425415 | avg loss: 0.9087945421536764\n",
      "batch loss: 1.062182068824768 | avg loss: 0.9471414238214493\n",
      "batch loss: 1.0722718238830566 | avg loss: 0.9721675038337707\n",
      "batch loss: 1.0196813344955444 | avg loss: 0.980086475610733\n",
      "batch loss: 1.0485737323760986 | avg loss: 0.9898703694343567\n",
      "batch loss: 0.9936376214027405 | avg loss: 0.9903412759304047\n",
      "batch loss: 0.9918193817138672 | avg loss: 0.9905055099063449\n",
      "batch loss: 1.040952205657959 | avg loss: 0.9955501794815064\n",
      "batch loss: 1.0775227546691895 | avg loss: 1.0030022317712957\n",
      "batch loss: 1.0637165307998657 | avg loss: 1.0080617566903431\n",
      "batch loss: 1.020261526107788 | avg loss: 1.0090002004916852\n",
      "batch loss: 1.1152516603469849 | avg loss: 1.0165895904813493\n",
      "batch loss: 1.1550445556640625 | avg loss: 1.0258199214935302\n",
      "batch loss: 1.0106092691421509 | avg loss: 1.024869255721569\n",
      "batch loss: 0.9846792817115784 | avg loss: 1.0225051396033342\n",
      "batch loss: 1.0140899419784546 | avg loss: 1.0220376286241744\n",
      "batch loss: 0.9420929551124573 | avg loss: 1.0178300142288208\n",
      "batch loss: 1.0938514471054077 | avg loss: 1.02163108587265\n",
      "batch loss: 0.9550119638442993 | avg loss: 1.018458746728443\n",
      "batch loss: 0.9645761251449585 | avg loss: 1.0160095366564663\n",
      "batch loss: 1.0625078678131104 | avg loss: 1.0180312032284944\n",
      "batch loss: 1.022933006286621 | avg loss: 1.018235445022583\n",
      "batch loss: 1.0650261640548706 | avg loss: 1.0201070737838744\n",
      "batch loss: 0.9581549763679504 | avg loss: 1.017724300806339\n",
      "batch loss: 1.0118439197540283 | avg loss: 1.0175065089155126\n",
      "batch loss: 1.0298274755477905 | avg loss: 1.017946543438094\n",
      "batch loss: 0.9233887791633606 | avg loss: 1.0146859308768963\n",
      "batch loss: 1.0093828439712524 | avg loss: 1.0145091613133748\n",
      "batch loss: 0.9796217083930969 | avg loss: 1.0133837596062691\n",
      "batch loss: 1.056976556777954 | avg loss: 1.0147460345178843\n",
      "batch loss: 0.9551446437835693 | avg loss: 1.0129399317683596\n",
      "batch loss: 0.8908758759498596 | avg loss: 1.00934981247958\n",
      "batch loss: 0.821384847164154 | avg loss: 1.0039793848991394\n",
      "batch loss: 1.0089266300201416 | avg loss: 1.0041168083747227\n",
      "batch loss: 0.9349642992019653 | avg loss: 1.0022478216403239\n",
      "batch loss: 0.9350659847259521 | avg loss: 1.00047987856363\n",
      "batch loss: 0.8530768752098083 | avg loss: 0.9967003143750705\n",
      "batch loss: 0.9433697462081909 | avg loss: 0.9953670501708984\n",
      "batch loss: 0.8398751020431519 | avg loss: 0.9915745636311973\n",
      "batch loss: 0.9827205538749695 | avg loss: 0.9913637538750967\n",
      "batch loss: 0.9482002854347229 | avg loss: 0.9903599522834601\n",
      "batch loss: 0.9886072874069214 | avg loss: 0.9903201189908114\n",
      "batch loss: 0.9658170342445374 | avg loss: 0.9897756059964498\n",
      "batch loss: 1.094347596168518 | avg loss: 0.9920489101306252\n",
      "batch loss: 0.9160338044166565 | avg loss: 0.9904315674558599\n",
      "batch loss: 1.0441505908966064 | avg loss: 0.9915507137775421\n",
      "batch loss: 0.9082154035568237 | avg loss: 0.9898499931607928\n",
      "batch loss: 1.1205618381500244 | avg loss: 0.9924642300605774\n",
      "batch loss: 0.9716697931289673 | avg loss: 0.9920564960030949\n",
      "batch loss: 0.9202784299850464 | avg loss: 0.9906761485796708\n",
      "batch loss: 1.0334844589233398 | avg loss: 0.9914838525484193\n",
      "batch loss: 0.9099773168563843 | avg loss: 0.9899744722578261\n",
      "batch loss: 0.9515393376350403 | avg loss: 0.9892756516283209\n",
      "batch loss: 1.1058505773544312 | avg loss: 0.9913573467305729\n",
      "batch loss: 0.8885264992713928 | avg loss: 0.9895532967751486\n",
      "batch loss: 0.9198745489120483 | avg loss: 0.988351939053371\n",
      "batch loss: 1.031572699546814 | avg loss: 0.9890844943159718\n",
      "batch loss: 1.0043418407440186 | avg loss: 0.9893387834231059\n",
      "batch loss: 1.0120683908462524 | avg loss: 0.9897113999382394\n",
      "batch loss: 0.8364017009735107 | avg loss: 0.9872386628581632\n",
      "batch loss: 0.8559800386428833 | avg loss: 0.9851551926325238\n",
      "batch loss: 1.026045560836792 | avg loss: 0.9857941046357155\n",
      "batch loss: 0.9454997181892395 | avg loss: 0.9851741909980773\n",
      "batch loss: 1.0538181066513062 | avg loss: 0.9862142503261566\n",
      "batch loss: 0.9631201028823853 | avg loss: 0.9858695615583392\n",
      "batch loss: 1.077269196510315 | avg loss: 0.9872136738370446\n",
      "batch loss: 1.0371811389923096 | avg loss: 0.9879378399987152\n",
      "batch loss: 0.9312911033630371 | avg loss: 0.9871286009039197\n",
      "batch loss: 0.9883409142494202 | avg loss: 0.9871456757397719\n",
      "batch loss: 0.9865350127220154 | avg loss: 0.9871371943089697\n",
      "batch loss: 1.0935691595077515 | avg loss: 0.9885951664349805\n",
      "batch loss: 1.0502510070800781 | avg loss: 0.989428353470725\n",
      "batch loss: 0.9303761124610901 | avg loss: 0.9886409902572632\n",
      "batch loss: 1.0025155544281006 | avg loss: 0.9888235503121426\n",
      "batch loss: 1.0441575050354004 | avg loss: 0.9895421731007563\n",
      "batch loss: 0.9107230305671692 | avg loss: 0.9885316712734027\n",
      "batch loss: 0.8839547634124756 | avg loss: 0.9872079129460491\n",
      "batch loss: 1.07382071018219 | avg loss: 0.9882905729115009\n",
      "batch loss: 0.9083561897277832 | avg loss: 0.9873037286746649\n",
      "batch loss: 1.032626986503601 | avg loss: 0.9878564513311153\n",
      "batch loss: 0.921841561794281 | avg loss: 0.9870610912162138\n",
      "batch loss: 1.0183154344558716 | avg loss: 0.9874331667309716\n",
      "batch loss: 1.0099748373031616 | avg loss: 0.9876983628553503\n",
      "batch loss: 0.9281365871429443 | avg loss: 0.9870057840679967\n",
      "batch loss: 0.8062898516654968 | avg loss: 0.9849285894426806\n",
      "batch loss: 1.0395324230194092 | avg loss: 0.985549087551507\n",
      "batch loss: 0.858414888381958 | avg loss: 0.9841206134035346\n",
      "batch loss: 1.0145560503005981 | avg loss: 0.9844587849246131\n",
      "batch loss: 0.9504430294036865 | avg loss: 0.9840849854133942\n",
      "batch loss: 0.8671382665634155 | avg loss: 0.9828138254258943\n",
      "batch loss: 0.8278726935386658 | avg loss: 0.9811477917496876\n",
      "batch loss: 0.8135239481925964 | avg loss: 0.9793645593714206\n",
      "batch loss: 0.9216188788414001 | avg loss: 0.9787567101026836\n",
      "batch loss: 0.9253005385398865 | avg loss: 0.9781998749822378\n",
      "batch loss: 1.0305908918380737 | avg loss: 0.9787399885580712\n",
      "batch loss: 0.8329778909683228 | avg loss: 0.9772526202153187\n",
      "batch loss: 0.8343038558959961 | avg loss: 0.9758086933030022\n",
      "batch loss: 1.0245397090911865 | avg loss: 0.9762960034608841\n",
      "batch loss: 0.8936573266983032 | avg loss: 0.9754777987404625\n",
      "batch loss: 1.133993148803711 | avg loss: 0.9770318707999062\n",
      "batch loss: 1.012609601020813 | avg loss: 0.9773772856564198\n",
      "batch loss: 0.8238614201545715 | avg loss: 0.9759011715650558\n",
      "batch loss: 0.720376193523407 | avg loss: 0.9734676003456115\n",
      "batch loss: 0.8472153544425964 | avg loss: 0.9722765414219983\n",
      "batch loss: 0.73192298412323 | avg loss: 0.9700302464939724\n",
      "batch loss: 0.9083379507064819 | avg loss: 0.969459021532977\n",
      "batch loss: 0.9724403023719788 | avg loss: 0.9694863727333349\n",
      "batch loss: 0.7538392543792725 | avg loss: 0.9675259443846616\n",
      "batch loss: 0.9817834496498108 | avg loss: 0.9676543903780414\n",
      "batch loss: 0.9280967116355896 | avg loss: 0.9673011968178409\n",
      "batch loss: 0.9532274007797241 | avg loss: 0.9671766499502469\n",
      "batch loss: 0.8338847756385803 | avg loss: 0.9660074229826007\n",
      "batch loss: 1.0772210359573364 | avg loss: 0.9669744978780331\n",
      "batch loss: 0.860392153263092 | avg loss: 0.9660556845623871\n",
      "batch loss: 0.9102615714073181 | avg loss: 0.965578811800378\n",
      "batch loss: 1.0076408386230469 | avg loss: 0.9659352696548074\n",
      "batch loss: 0.9446731209754944 | avg loss: 0.9657565961364939\n",
      "batch loss: 0.8908313512802124 | avg loss: 0.9651322190960249\n",
      "batch loss: 0.9748514294624329 | avg loss: 0.9652125431486398\n",
      "batch loss: 0.9553630948066711 | avg loss: 0.9651318099655088\n",
      "batch loss: 0.8085110187530518 | avg loss: 0.963858470199554\n",
      "batch loss: 0.7556423544883728 | avg loss: 0.9621793079760766\n",
      "batch loss: 0.9594920873641968 | avg loss: 0.9621578102111816\n",
      "batch loss: 1.0378551483154297 | avg loss: 0.9627585827358185\n",
      "\n",
      "  Average training loss: 0.96\n",
      "  Training epoch took: 0:31:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.55\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "batch loss: 0.9625533819198608 | avg loss: 0.9625533819198608\n",
      "batch loss: 0.7609685659408569 | avg loss: 0.8617609739303589\n",
      "batch loss: 0.8911246061325073 | avg loss: 0.8715488513310751\n",
      "batch loss: 0.856967031955719 | avg loss: 0.867903396487236\n",
      "batch loss: 0.9026412963867188 | avg loss: 0.8748509764671326\n",
      "batch loss: 0.9828451871871948 | avg loss: 0.8928500115871429\n",
      "batch loss: 0.963981032371521 | avg loss: 0.9030115859849113\n",
      "batch loss: 0.9091833829879761 | avg loss: 0.9037830606102943\n",
      "batch loss: 0.9472875595092773 | avg loss: 0.9086168938212924\n",
      "batch loss: 0.8149052858352661 | avg loss: 0.8992457330226898\n",
      "batch loss: 0.8919275999069214 | avg loss: 0.8985804481939836\n",
      "batch loss: 0.9424400925636292 | avg loss: 0.9022354185581207\n",
      "batch loss: 0.8078049421310425 | avg loss: 0.8949715357560378\n",
      "batch loss: 1.075574517250061 | avg loss: 0.9078717487198966\n",
      "batch loss: 1.0184359550476074 | avg loss: 0.9152426958084107\n",
      "batch loss: 0.9008070230484009 | avg loss: 0.91434046626091\n",
      "batch loss: 0.8795148730278015 | avg loss: 0.9122919019530801\n",
      "batch loss: 0.9221628308296204 | avg loss: 0.9128402868906657\n",
      "batch loss: 0.8786671161651611 | avg loss: 0.9110416989577445\n",
      "batch loss: 0.8845943212509155 | avg loss: 0.9097193300724029\n",
      "batch loss: 0.8654415607452393 | avg loss: 0.9076108648663476\n",
      "batch loss: 0.8355104923248291 | avg loss: 0.9043335752053694\n",
      "batch loss: 0.9341881275177002 | avg loss: 0.905631599218949\n",
      "batch loss: 1.0190486907958984 | avg loss: 0.9103573113679886\n",
      "batch loss: 0.8678686618804932 | avg loss: 0.9086577653884887\n",
      "batch loss: 0.8357164263725281 | avg loss: 0.9058523292724903\n",
      "batch loss: 0.9203876256942749 | avg loss: 0.9063906735844083\n",
      "batch loss: 0.9036358594894409 | avg loss: 0.9062922873667308\n",
      "batch loss: 0.7911776304244995 | avg loss: 0.9023228164376884\n",
      "batch loss: 0.8850489258766174 | avg loss: 0.9017470200856527\n",
      "batch loss: 0.6845504641532898 | avg loss: 0.8947406795717054\n",
      "batch loss: 0.9797062873840332 | avg loss: 0.8973958548158407\n",
      "batch loss: 0.97824627161026 | avg loss: 0.8998458674459746\n",
      "batch loss: 0.7881961464881897 | avg loss: 0.8965620521236869\n",
      "batch loss: 0.7367815375328064 | avg loss: 0.8919968945639474\n",
      "batch loss: 0.6969038248062134 | avg loss: 0.8865776426262326\n",
      "batch loss: 0.7013238668441772 | avg loss: 0.8815707838213122\n",
      "batch loss: 0.632325291633606 | avg loss: 0.8750116919216356\n",
      "batch loss: 0.6580154895782471 | avg loss: 0.8694476867333437\n",
      "batch loss: 0.6000897288322449 | avg loss: 0.8627137377858162\n",
      "batch loss: 0.8524404168128967 | avg loss: 0.8624631689815987\n",
      "batch loss: 0.9072229266166687 | avg loss: 0.8635288774967194\n",
      "batch loss: 0.7423852682113647 | avg loss: 0.860711584257525\n",
      "batch loss: 1.0035641193389893 | avg loss: 0.8639582327821038\n",
      "batch loss: 0.834871232509613 | avg loss: 0.8633118549982707\n",
      "batch loss: 1.0766942501068115 | avg loss: 0.8679506027180216\n",
      "batch loss: 0.6617958545684814 | avg loss: 0.8635643314807973\n",
      "batch loss: 0.9454765915870667 | avg loss: 0.8652708368996779\n",
      "batch loss: 0.7970081567764282 | avg loss: 0.8638777209787952\n",
      "batch loss: 1.0122628211975098 | avg loss: 0.8668454229831696\n",
      "batch loss: 0.9332171082496643 | avg loss: 0.8681468285766303\n",
      "batch loss: 0.6216087937355042 | avg loss: 0.8634057125219932\n",
      "batch loss: 0.8855637311935425 | avg loss: 0.8638237883459847\n",
      "batch loss: 0.7456831336021423 | avg loss: 0.861635998443321\n",
      "batch loss: 0.6784335374832153 | avg loss: 0.8583050446076826\n",
      "batch loss: 0.8599033355712891 | avg loss: 0.8583335855177471\n",
      "batch loss: 0.6376964449882507 | avg loss: 0.8544627584909138\n",
      "batch loss: 0.7816369533538818 | avg loss: 0.853207141160965\n",
      "batch loss: 0.8388838171958923 | avg loss: 0.8529643729581671\n",
      "batch loss: 0.8030245304107666 | avg loss: 0.8521320422490438\n",
      "batch loss: 0.7972769141197205 | avg loss: 0.8512327778534811\n",
      "batch loss: 0.6286153197288513 | avg loss: 0.8476421736901806\n",
      "batch loss: 0.6214004158973694 | avg loss: 0.8440510346775963\n",
      "batch loss: 0.8359824419021606 | avg loss: 0.8439249629154801\n",
      "batch loss: 0.8587587475776672 | avg loss: 0.8441531749872061\n",
      "batch loss: 0.7824808359146118 | avg loss: 0.8432187456073184\n",
      "batch loss: 0.8830782771110535 | avg loss: 0.8438136639879711\n",
      "batch loss: 0.9898315668106079 | avg loss: 0.845960986088304\n",
      "batch loss: 0.8402261734008789 | avg loss: 0.84587787286095\n",
      "batch loss: 0.7182249426841736 | avg loss: 0.8440542595727103\n",
      "batch loss: 0.6391598582267761 | avg loss: 0.8411684229340352\n",
      "batch loss: 0.74934983253479 | avg loss: 0.8398931647340456\n",
      "batch loss: 0.8939427733421326 | avg loss: 0.8406335703314167\n",
      "batch loss: 0.9049786925315857 | avg loss: 0.8415030990097974\n",
      "batch loss: 0.682966411113739 | avg loss: 0.8393892765045166\n",
      "batch loss: 0.7294227480888367 | avg loss: 0.8379423484990471\n",
      "batch loss: 0.922160804271698 | avg loss: 0.8390360946779127\n",
      "batch loss: 0.6998625993728638 | avg loss: 0.8372518190970788\n",
      "batch loss: 0.56107097864151 | avg loss: 0.8337558590913121\n",
      "batch loss: 0.8873600363731384 | avg loss: 0.8344259113073349\n",
      "batch loss: 0.7466140985488892 | avg loss: 0.8333418148535269\n",
      "batch loss: 0.9044058322906494 | avg loss: 0.8342084492125162\n",
      "batch loss: 0.7505820989608765 | avg loss: 0.8332009028239422\n",
      "batch loss: 0.8040958046913147 | avg loss: 0.8328544135604586\n",
      "batch loss: 0.9759995937347412 | avg loss: 0.8345384745036855\n",
      "batch loss: 0.6833332777023315 | avg loss: 0.832780274540879\n",
      "batch loss: 0.6333033442497253 | avg loss: 0.8304874362616703\n",
      "batch loss: 0.758407711982727 | avg loss: 0.8296683484857733\n",
      "batch loss: 0.6446791291236877 | avg loss: 0.8275898179311431\n",
      "batch loss: 0.7600432634353638 | avg loss: 0.8268393006589677\n",
      "batch loss: 0.7913700342178345 | avg loss: 0.826449528500274\n",
      "batch loss: 0.5120775699615479 | avg loss: 0.8230324419944183\n",
      "batch loss: 0.8438287973403931 | avg loss: 0.8232560587185686\n",
      "batch loss: 0.6061049699783325 | avg loss: 0.8209459407532469\n",
      "batch loss: 0.6231718063354492 | avg loss: 0.8188641077593753\n",
      "batch loss: 0.7111307978630066 | avg loss: 0.8177418857812881\n",
      "batch loss: 0.8765940070152283 | avg loss: 0.8183486086806071\n",
      "batch loss: 0.6824272871017456 | avg loss: 0.8169616564195983\n",
      "batch loss: 0.7679155468940735 | avg loss: 0.8164662411718657\n",
      "batch loss: 0.9451675415039062 | avg loss: 0.8177532541751862\n",
      "batch loss: 0.6796920895576477 | avg loss: 0.8163863119512501\n",
      "batch loss: 0.8736333847045898 | avg loss: 0.8169475577625573\n",
      "batch loss: 0.7044452428817749 | avg loss: 0.815855302278278\n",
      "batch loss: 0.6426703929901123 | avg loss: 0.8141900627658918\n",
      "batch loss: 0.42343026399612427 | avg loss: 0.8104685408728464\n",
      "batch loss: 0.736937940120697 | avg loss: 0.8097748559600902\n",
      "batch loss: 0.520243763923645 | avg loss: 0.8070689579036748\n",
      "batch loss: 0.6358984112739563 | avg loss: 0.8054840454348812\n",
      "batch loss: 0.7809364795684814 | avg loss: 0.8052588384085839\n",
      "batch loss: 0.5570487976074219 | avg loss: 0.8030023834922096\n",
      "batch loss: 0.7609982490539551 | avg loss: 0.8026239678666398\n",
      "batch loss: 0.6723313331604004 | avg loss: 0.8014606407710484\n",
      "batch loss: 0.8423954248428345 | avg loss: 0.8018228954973474\n",
      "batch loss: 0.6313009858131409 | avg loss: 0.8003270892720473\n",
      "batch loss: 0.9343212246894836 | avg loss: 0.8014922556669816\n",
      "batch loss: 0.6575862169265747 | avg loss: 0.8002516863674953\n",
      "batch loss: 0.7188965678215027 | avg loss: 0.7995563434739398\n",
      "batch loss: 0.8551881313323975 | avg loss: 0.8000277993032487\n",
      "batch loss: 0.7605066895484924 | avg loss: 0.7996956891372424\n",
      "batch loss: 0.7166081666946411 | avg loss: 0.7990032931168874\n",
      "batch loss: 0.850095272064209 | avg loss: 0.7994255408767826\n",
      "batch loss: 0.7978728413581848 | avg loss: 0.7994128138315483\n",
      "batch loss: 0.5562729239463806 | avg loss: 0.7974360667593111\n",
      "batch loss: 0.5232707262039185 | avg loss: 0.7952250559483806\n",
      "batch loss: 0.7551217079162598 | avg loss: 0.7949042291641235\n",
      "batch loss: 0.8760191202163696 | avg loss: 0.7955479981407286\n",
      "\n",
      "  Average training loss: 0.80\n",
      "  Training epoch took: 0:31:00\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.57\n",
      "  Validation took: 0:02:31\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "batch loss: 0.8450045585632324 | avg loss: 0.8450045585632324\n",
      "batch loss: 0.646912693977356 | avg loss: 0.7459586262702942\n",
      "batch loss: 0.7194432020187378 | avg loss: 0.7371201515197754\n",
      "batch loss: 0.7081316709518433 | avg loss: 0.7298730313777924\n",
      "batch loss: 0.7550201416015625 | avg loss: 0.7349024534225463\n",
      "batch loss: 0.793163537979126 | avg loss: 0.7446126341819763\n",
      "batch loss: 0.8327193856239319 | avg loss: 0.7571993129593986\n",
      "batch loss: 0.7157328724861145 | avg loss: 0.752016007900238\n",
      "batch loss: 0.7189503908157349 | avg loss: 0.7483420504464043\n",
      "batch loss: 0.6900436878204346 | avg loss: 0.7425122141838074\n",
      "batch loss: 0.7831981182098389 | avg loss: 0.7462109327316284\n",
      "batch loss: 0.8254199624061584 | avg loss: 0.7528116852045059\n",
      "batch loss: 0.843235194683075 | avg loss: 0.7597673397797805\n",
      "batch loss: 0.9855267405509949 | avg loss: 0.7758930112634387\n",
      "batch loss: 0.8452236652374268 | avg loss: 0.7805150548617045\n",
      "batch loss: 0.8977856040000916 | avg loss: 0.7878444641828537\n",
      "batch loss: 0.619781494140625 | avg loss: 0.7779584071215462\n",
      "batch loss: 0.5224962830543518 | avg loss: 0.7637660668955909\n",
      "batch loss: 0.6254174113273621 | avg loss: 0.7564845587077894\n",
      "batch loss: 0.6040448546409607 | avg loss: 0.748862573504448\n",
      "batch loss: 0.6607434749603271 | avg loss: 0.7446664259547279\n",
      "batch loss: 0.7342862486839294 | avg loss: 0.7441945997151461\n",
      "batch loss: 0.7815763354301453 | avg loss: 0.7458198925723201\n",
      "batch loss: 0.7917436361312866 | avg loss: 0.747733381887277\n",
      "batch loss: 0.7202643156051636 | avg loss: 0.7466346192359924\n",
      "batch loss: 0.7304914593696594 | avg loss: 0.7460137284719027\n",
      "batch loss: 0.6281691789627075 | avg loss: 0.7416491155271177\n",
      "batch loss: 0.8157116174697876 | avg loss: 0.7442942048822131\n",
      "batch loss: 0.6128688454627991 | avg loss: 0.739762295936716\n",
      "batch loss: 0.6069241166114807 | avg loss: 0.7353343566258749\n",
      "batch loss: 0.37716224789619446 | avg loss: 0.7237804176345948\n",
      "batch loss: 0.7324080467224121 | avg loss: 0.7240500310435891\n",
      "batch loss: 0.7444233894348145 | avg loss: 0.724667405540293\n",
      "batch loss: 0.46554747223854065 | avg loss: 0.7170462310314178\n",
      "batch loss: 0.6376950740814209 | avg loss: 0.7147790551185608\n",
      "batch loss: 0.37624791264533997 | avg loss: 0.7053754122720824\n",
      "batch loss: 0.38930410146713257 | avg loss: 0.6968329444124892\n",
      "batch loss: 0.3977968692779541 | avg loss: 0.6889635740142119\n",
      "batch loss: 0.4775810241699219 | avg loss: 0.6835435086335891\n",
      "batch loss: 0.36202311515808105 | avg loss: 0.6755054987967014\n",
      "batch loss: 0.5271086692810059 | avg loss: 0.6718860639304649\n",
      "batch loss: 0.5926456451416016 | avg loss: 0.6699993872926349\n",
      "batch loss: 0.5620139837265015 | avg loss: 0.6674880988376085\n",
      "batch loss: 0.8638495206832886 | avg loss: 0.6719508584250103\n",
      "batch loss: 0.8432072401046753 | avg loss: 0.6757565557956695\n",
      "batch loss: 1.1886752843856812 | avg loss: 0.6869069629389307\n",
      "batch loss: 0.6275340914726257 | avg loss: 0.6856437103545412\n",
      "batch loss: 0.7837315797805786 | avg loss: 0.6876872076342503\n",
      "batch loss: 0.5946064591407776 | avg loss: 0.6857876005221386\n",
      "batch loss: 0.9259127974510193 | avg loss: 0.6905901044607162\n",
      "batch loss: 0.5113443732261658 | avg loss: 0.6870754822796467\n",
      "batch loss: 0.4536385238170624 | avg loss: 0.68258631000152\n",
      "batch loss: 0.5103960037231445 | avg loss: 0.6793374362981545\n",
      "batch loss: 0.5772170424461365 | avg loss: 0.6774463178934874\n",
      "batch loss: 0.42841824889183044 | avg loss: 0.67291853482073\n",
      "batch loss: 0.5181571245193481 | avg loss: 0.6701549382082054\n",
      "batch loss: 0.46756675839424133 | avg loss: 0.6666007596149779\n",
      "batch loss: 0.6491746306419373 | avg loss: 0.6663003091154427\n",
      "batch loss: 0.5894478559494019 | avg loss: 0.664997725163476\n",
      "batch loss: 0.6057862043380737 | avg loss: 0.6640108664830525\n",
      "batch loss: 0.5516189336776733 | avg loss: 0.662168375781325\n",
      "batch loss: 0.37137752771377563 | avg loss: 0.6574782008124936\n",
      "batch loss: 0.4624864459037781 | avg loss: 0.6543830935917203\n",
      "batch loss: 0.4157599210739136 | avg loss: 0.6506546065211296\n",
      "batch loss: 0.6388906240463257 | avg loss: 0.6504736221753634\n",
      "batch loss: 0.5097187757492065 | avg loss: 0.6483409729870883\n",
      "batch loss: 0.7426208257675171 | avg loss: 0.6497481349688857\n",
      "batch loss: 0.809011697769165 | avg loss: 0.6520902461865369\n",
      "batch loss: 0.5699133276939392 | avg loss: 0.6508992763533108\n",
      "batch loss: 0.4089033603668213 | avg loss: 0.6474421918392181\n",
      "batch loss: 0.4288351833820343 | avg loss: 0.6443632198891169\n",
      "batch loss: 0.48786354064941406 | avg loss: 0.6421896132330099\n",
      "batch loss: 0.6257110238075256 | avg loss: 0.641963879131291\n",
      "batch loss: 0.5834065675735474 | avg loss: 0.6411725641102404\n",
      "batch loss: 0.4233045279979706 | avg loss: 0.6382676569620768\n",
      "batch loss: 0.45331472158432007 | avg loss: 0.6358340657071063\n",
      "batch loss: 0.662854790687561 | avg loss: 0.636184984213346\n",
      "batch loss: 0.4680663049221039 | avg loss: 0.6340296165301249\n",
      "batch loss: 0.32587161660194397 | avg loss: 0.6301288823538189\n",
      "batch loss: 0.5355227589607239 | avg loss: 0.6289463058114052\n",
      "batch loss: 0.4482131898403168 | avg loss: 0.6267150327747251\n",
      "batch loss: 0.6929887533187866 | avg loss: 0.627523248878921\n",
      "batch loss: 0.5560619235038757 | avg loss: 0.6266622690551252\n",
      "batch loss: 0.7219147682189941 | avg loss: 0.6277962273785046\n",
      "batch loss: 0.5707224607467651 | avg loss: 0.6271247713004842\n",
      "batch loss: 0.49612969160079956 | avg loss: 0.625601572699325\n",
      "batch loss: 0.3136758804321289 | avg loss: 0.6220162199146446\n",
      "batch loss: 0.6040245890617371 | avg loss: 0.6218117695640434\n",
      "batch loss: 0.4561024606227875 | avg loss: 0.6199498672163888\n",
      "batch loss: 0.5423996448516846 | avg loss: 0.6190881980790033\n",
      "batch loss: 0.37771010398864746 | avg loss: 0.6164356915505378\n",
      "batch loss: 0.28404366970062256 | avg loss: 0.6128227347912996\n",
      "batch loss: 0.5738250017166138 | avg loss: 0.6124034043281309\n",
      "batch loss: 0.39413174986839294 | avg loss: 0.6100813654508996\n",
      "batch loss: 0.3261997103691101 | avg loss: 0.6070931375026702\n",
      "batch loss: 0.575434684753418 | avg loss: 0.6067633619531989\n",
      "batch loss: 0.6358523368835449 | avg loss: 0.607063248292687\n",
      "batch loss: 0.47336143255233765 | avg loss: 0.6056989440504386\n",
      "batch loss: 0.514592170715332 | avg loss: 0.6047786736127102\n",
      "batch loss: 0.5659247040748596 | avg loss: 0.6043901339173317\n",
      "batch loss: 0.426376610994339 | avg loss: 0.6026276237893813\n",
      "batch loss: 0.7210440635681152 | avg loss: 0.6037885692774081\n",
      "batch loss: 0.5376808047294617 | avg loss: 0.6031467463206319\n",
      "batch loss: 0.48806124925613403 | avg loss: 0.602040155002704\n",
      "batch loss: 0.1811477392911911 | avg loss: 0.5980316558054515\n",
      "batch loss: 0.48475566506385803 | avg loss: 0.5969630143833611\n",
      "batch loss: 0.30653733015060425 | avg loss: 0.5942487556522138\n",
      "batch loss: 0.6238718032836914 | avg loss: 0.594523043130283\n",
      "batch loss: 0.5235239267349243 | avg loss: 0.5938716750899586\n",
      "batch loss: 0.5095686912536621 | avg loss: 0.5931052843278105\n",
      "batch loss: 0.5874064564704895 | avg loss: 0.593053943536303\n",
      "batch loss: 0.5677608847618103 | avg loss: 0.592828112654388\n",
      "batch loss: 0.505265474319458 | avg loss: 0.592053222049654\n",
      "batch loss: 0.45118212699890137 | avg loss: 0.5908175106895598\n",
      "batch loss: 0.6368533372879028 | avg loss: 0.5912178222251975\n",
      "batch loss: 0.41793492436408997 | avg loss: 0.589724004140188\n",
      "batch loss: 0.6731460094451904 | avg loss: 0.5904370127325385\n",
      "batch loss: 0.5183038115501404 | avg loss: 0.5898257144174334\n",
      "batch loss: 0.7386661767959595 | avg loss: 0.5910764746054882\n",
      "batch loss: 0.6410142779350281 | avg loss: 0.5914926229665677\n",
      "batch loss: 0.7627209424972534 | avg loss: 0.5929077330453336\n",
      "batch loss: 0.7027148604393005 | avg loss: 0.5938077914665957\n",
      "batch loss: 0.45477157831192017 | avg loss: 0.592677415749891\n",
      "batch loss: 0.34977948665618896 | avg loss: 0.5907185614830063\n",
      "batch loss: 0.6616184115409851 | avg loss: 0.5912857602834701\n",
      "batch loss: 0.5946301221847534 | avg loss: 0.5913123028382422\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:31:14\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.56\n",
      "  Validation took: 0:02:34\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "batch loss: 0.5596676468849182 | avg loss: 0.5596676468849182\n",
      "batch loss: 0.4140596091747284 | avg loss: 0.4868636280298233\n",
      "batch loss: 0.4745253324508667 | avg loss: 0.48275086283683777\n",
      "batch loss: 0.3889572024345398 | avg loss: 0.4593024477362633\n",
      "batch loss: 0.5790651440620422 | avg loss: 0.48325498700141906\n",
      "batch loss: 0.5591989755630493 | avg loss: 0.4959123184283574\n",
      "batch loss: 0.8042097687721252 | avg loss: 0.53995481133461\n",
      "batch loss: 0.6418517827987671 | avg loss: 0.5526919327676296\n",
      "batch loss: 0.6882348656654358 | avg loss: 0.5677522586451637\n",
      "batch loss: 0.5510684251785278 | avg loss: 0.5660838752985\n",
      "batch loss: 0.41445988416671753 | avg loss: 0.5522998761047017\n",
      "batch loss: 0.5345121026039124 | avg loss: 0.5508175616463026\n",
      "batch loss: 0.49014392495155334 | avg loss: 0.5461503588236295\n",
      "batch loss: 0.6776479482650757 | avg loss: 0.5555430437837329\n",
      "batch loss: 0.5822871327400208 | avg loss: 0.5573259830474854\n",
      "batch loss: 0.6047767996788025 | avg loss: 0.5602916590869427\n",
      "batch loss: 0.4583081603050232 | avg loss: 0.5542926297468298\n",
      "batch loss: 0.4480311870574951 | avg loss: 0.548389216264089\n",
      "batch loss: 0.8123499155044556 | avg loss: 0.5622818846451608\n",
      "batch loss: 0.4126014709472656 | avg loss: 0.5547978639602661\n",
      "batch loss: 0.4795200526714325 | avg loss: 0.5512132062798455\n",
      "batch loss: 0.3609135150909424 | avg loss: 0.5425632203167136\n",
      "batch loss: 0.49609580636024475 | avg loss: 0.54054289797078\n",
      "batch loss: 0.5602827072143555 | avg loss: 0.5413653900225958\n",
      "batch loss: 0.42066481709480286 | avg loss: 0.536537367105484\n",
      "batch loss: 0.5065013766288757 | avg loss: 0.5353821367025375\n",
      "batch loss: 0.4894488751888275 | avg loss: 0.5336809047946224\n",
      "batch loss: 0.5845767259597778 | avg loss: 0.535498612693378\n",
      "batch loss: 0.4163925349712372 | avg loss: 0.5313915065650282\n",
      "batch loss: 0.36597147583961487 | avg loss: 0.5258775055408478\n",
      "batch loss: 0.22932805120944977 | avg loss: 0.5163113941108027\n",
      "batch loss: 0.7396124601364136 | avg loss: 0.523289552424103\n",
      "batch loss: 0.3472244143486023 | avg loss: 0.5179542452096939\n",
      "batch loss: 0.38000091910362244 | avg loss: 0.5138967944418683\n",
      "batch loss: 0.45259517431259155 | avg loss: 0.5121453195810318\n",
      "batch loss: 0.1090238019824028 | avg loss: 0.5009474996477365\n",
      "batch loss: 0.0989769920706749 | avg loss: 0.49008343187538356\n",
      "batch loss: 0.2314835488796234 | avg loss: 0.48327817179654775\n",
      "batch loss: 0.35840606689453125 | avg loss: 0.4800763229529063\n",
      "batch loss: 0.31188082695007324 | avg loss: 0.47587143555283545\n",
      "batch loss: 0.3738172948360443 | avg loss: 0.4733823101694991\n",
      "batch loss: 0.43631047010421753 | avg loss: 0.4724996473108019\n",
      "batch loss: 0.469908207654953 | avg loss: 0.4724393812722938\n",
      "batch loss: 0.6429561972618103 | avg loss: 0.47631476345387375\n",
      "batch loss: 0.4121485948562622 | avg loss: 0.474888848596149\n",
      "batch loss: 0.6742157340049744 | avg loss: 0.47922204175721045\n",
      "batch loss: 0.1978183537721634 | avg loss: 0.4732347292468903\n",
      "batch loss: 0.625617504119873 | avg loss: 0.4764093703900774\n",
      "batch loss: 0.3175725042819977 | avg loss: 0.4731678016939942\n",
      "batch loss: 0.7078613638877869 | avg loss: 0.47786167293787\n",
      "batch loss: 0.337251752614975 | avg loss: 0.4751046156766368\n",
      "batch loss: 0.40853458642959595 | avg loss: 0.4738244228065014\n",
      "batch loss: 0.2590217590332031 | avg loss: 0.4697715423579486\n",
      "batch loss: 0.269619345664978 | avg loss: 0.46606502019696766\n",
      "batch loss: 0.26059529185295105 | avg loss: 0.46232920695434915\n",
      "batch loss: 0.186185821890831 | avg loss: 0.4573980750782149\n",
      "batch loss: 0.2621321976184845 | avg loss: 0.4539723579297986\n",
      "batch loss: 0.41977596282958984 | avg loss: 0.45338276491082946\n",
      "batch loss: 0.36584657430648804 | avg loss: 0.45189910066329825\n",
      "batch loss: 0.4244590997695923 | avg loss: 0.45144176731506985\n",
      "batch loss: 0.5897241234779358 | avg loss: 0.45370869118659224\n",
      "batch loss: 0.264198362827301 | avg loss: 0.4506520729872488\n",
      "batch loss: 0.26397505402565 | avg loss: 0.44768894570214407\n",
      "batch loss: 0.33103474974632263 | avg loss: 0.44586622389033437\n",
      "batch loss: 0.6649003028869629 | avg loss: 0.44923597895182094\n",
      "batch loss: 0.19798199832439423 | avg loss: 0.44542910045746603\n",
      "batch loss: 0.6586145758628845 | avg loss: 0.44861097322471105\n",
      "batch loss: 0.43795567750930786 | avg loss: 0.44845427769948454\n",
      "batch loss: 0.37357449531555176 | avg loss: 0.44736906346203625\n",
      "batch loss: 0.39832133054733276 | avg loss: 0.4466683815632548\n",
      "batch loss: 0.3217063844203949 | avg loss: 0.4449083534344821\n",
      "batch loss: 0.24100711941719055 | avg loss: 0.4420763918509086\n",
      "batch loss: 0.48886093497276306 | avg loss: 0.44271727600326277\n",
      "batch loss: 0.39126598834991455 | avg loss: 0.4420219883322716\n",
      "batch loss: 0.24006697535514832 | avg loss: 0.43932925482590995\n",
      "batch loss: 0.3232956826686859 | avg loss: 0.4378024972975254\n",
      "batch loss: 0.4108779728412628 | avg loss: 0.43745282814874276\n",
      "batch loss: 0.31226664781570435 | avg loss: 0.43584787711883205\n",
      "batch loss: 0.1378486007452011 | avg loss: 0.4320757343799253\n",
      "batch loss: 0.27685287594795227 | avg loss: 0.43013544864952563\n",
      "batch loss: 0.2243412435054779 | avg loss: 0.4275947794502164\n",
      "batch loss: 0.28604817390441895 | avg loss: 0.4258686013338042\n",
      "batch loss: 0.26707926392555237 | avg loss: 0.42395547678671686\n",
      "batch loss: 0.2744981348514557 | avg loss: 0.422176222716059\n",
      "batch loss: 0.3845406174659729 | avg loss: 0.4217334508895874\n",
      "batch loss: 0.2931737005710602 | avg loss: 0.4202385700719301\n",
      "batch loss: 0.10392948240041733 | avg loss: 0.41660283343202764\n",
      "batch loss: 0.2796368896961212 | avg loss: 0.41504640225321054\n",
      "batch loss: 0.28866806626319885 | avg loss: 0.413626420949952\n",
      "batch loss: 0.29441219568252563 | avg loss: 0.4123018184469806\n",
      "batch loss: 0.20362645387649536 | avg loss: 0.41000868257257966\n",
      "batch loss: 0.11137674748897552 | avg loss: 0.40676268327819265\n",
      "batch loss: 0.2446592003107071 | avg loss: 0.4050196350742412\n",
      "batch loss: 0.209767147898674 | avg loss: 0.4029424809553522\n",
      "batch loss: 0.17759956419467926 | avg loss: 0.40057045025260823\n",
      "batch loss: 0.21854551136493683 | avg loss: 0.398674357139195\n",
      "batch loss: 0.5245056748390198 | avg loss: 0.3999715872185746\n",
      "batch loss: 0.3243495523929596 | avg loss: 0.3991999338019867\n",
      "batch loss: 0.2666974365711212 | avg loss: 0.3978615247390487\n",
      "batch loss: 0.38126835227012634 | avg loss: 0.39769559301435947\n",
      "batch loss: 0.22478646039962769 | avg loss: 0.39598362140431265\n",
      "batch loss: 0.6199638843536377 | avg loss: 0.39817950633518834\n",
      "batch loss: 0.5327597856521606 | avg loss: 0.3994861109887512\n",
      "batch loss: 0.2668459713459015 | avg loss: 0.39821072503064686\n",
      "batch loss: 0.4606972634792328 | avg loss: 0.39880583492063343\n",
      "batch loss: 0.5908922553062439 | avg loss: 0.4006179709620071\n",
      "batch loss: 0.3143804669380188 | avg loss: 0.3998120129804745\n",
      "batch loss: 0.35575565695762634 | avg loss: 0.3994040837580407\n",
      "batch loss: 0.26409390568733215 | avg loss: 0.39816270597757547\n",
      "batch loss: 0.4794746935367584 | avg loss: 0.39890190586447716\n",
      "batch loss: 0.599644660949707 | avg loss: 0.4007103991535333\n",
      "batch loss: 1.027523398399353 | avg loss: 0.40630694378965665\n",
      "batch loss: 0.7587854862213135 | avg loss: 0.40942622292622\n",
      "batch loss: 0.4822447896003723 | avg loss: 0.4100649822830108\n",
      "batch loss: 0.32551655173301697 | avg loss: 0.40932977853909785\n",
      "batch loss: 0.23021548986434937 | avg loss: 0.40778568984362584\n",
      "batch loss: 0.4332379698753357 | avg loss: 0.40800323069859773\n",
      "batch loss: 0.43003490567207336 | avg loss: 0.40818993980854246\n",
      "batch loss: 0.6844949126243591 | avg loss: 0.4105118303364064\n",
      "batch loss: 0.5374271273612976 | avg loss: 0.41156945781161386\n",
      "batch loss: 0.4306768774986267 | avg loss: 0.4117273703710107\n",
      "batch loss: 0.7781770825386047 | avg loss: 0.4147310565363188\n",
      "batch loss: 0.4865521490573883 | avg loss: 0.41531496785762834\n",
      "batch loss: 0.3144893944263458 | avg loss: 0.41450185839447284\n",
      "batch loss: 0.4843214452266693 | avg loss: 0.4150604150891304\n",
      "batch loss: 0.4483538866043091 | avg loss: 0.4153246489900445\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:31:21\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.54\n",
      "  Validation took: 0:02:34\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "batch loss: 0.3384723663330078 | avg loss: 0.3384723663330078\n",
      "batch loss: 0.33139994740486145 | avg loss: 0.33493615686893463\n",
      "batch loss: 0.4377426505088806 | avg loss: 0.36920498808224994\n",
      "batch loss: 0.15943065285682678 | avg loss: 0.31676140427589417\n",
      "batch loss: 0.30042535066604614 | avg loss: 0.31349419355392455\n",
      "batch loss: 0.3752589523792267 | avg loss: 0.32378832002480823\n",
      "batch loss: 0.3887476325035095 | avg loss: 0.33306822180747986\n",
      "batch loss: 0.37103456258773804 | avg loss: 0.33781401440501213\n",
      "batch loss: 0.451427161693573 | avg loss: 0.3504376974370744\n",
      "batch loss: 0.19073331356048584 | avg loss: 0.3344672590494156\n",
      "batch loss: 0.42085176706314087 | avg loss: 0.34232039614157245\n",
      "batch loss: 0.33558061718940735 | avg loss: 0.34175874789555866\n",
      "batch loss: 0.2799331843852997 | avg loss: 0.3370029353178464\n",
      "batch loss: 0.4585553705692291 | avg loss: 0.34568525212151663\n",
      "batch loss: 0.460830420255661 | avg loss: 0.3533615966637929\n",
      "batch loss: 0.5453770160675049 | avg loss: 0.3653625603765249\n",
      "batch loss: 0.20835301280021667 | avg loss: 0.35612670463674206\n",
      "batch loss: 0.22398525476455688 | avg loss: 0.34878551297717625\n",
      "batch loss: 0.6495692729949951 | avg loss: 0.3646162371886404\n",
      "batch loss: 0.2380465418100357 | avg loss: 0.35828775241971017\n",
      "batch loss: 0.3589874505996704 | avg loss: 0.35832107138066066\n",
      "batch loss: 0.2503473460674286 | avg loss: 0.35341317477551376\n",
      "batch loss: 0.2851145267486572 | avg loss: 0.3504436683395635\n",
      "batch loss: 0.3904394507408142 | avg loss: 0.3521101592729489\n",
      "batch loss: 0.29953324794769287 | avg loss: 0.35000708281993864\n",
      "batch loss: 0.3693229556083679 | avg loss: 0.350750001004109\n",
      "batch loss: 0.335060715675354 | avg loss: 0.3501689163623033\n",
      "batch loss: 0.5021230578422546 | avg loss: 0.3555958499865873\n",
      "batch loss: 0.2190740704536438 | avg loss: 0.3508882024164858\n",
      "batch loss: 0.40320318937301636 | avg loss: 0.3526320353150368\n",
      "batch loss: 0.14168915152549744 | avg loss: 0.3458274261605355\n",
      "batch loss: 0.4866793155670166 | avg loss: 0.35022904770448804\n",
      "batch loss: 0.2329017072916031 | avg loss: 0.34667367375258246\n",
      "batch loss: 0.08758940547704697 | avg loss: 0.33905354821506667\n",
      "batch loss: 0.39407381415367126 | avg loss: 0.34062555581331255\n",
      "batch loss: 0.06597688049077988 | avg loss: 0.3329964259432422\n",
      "batch loss: 0.06080581620335579 | avg loss: 0.3256399229772993\n",
      "batch loss: 0.04765370115637779 | avg loss: 0.31832449608727503\n",
      "batch loss: 0.14867490530014038 | avg loss: 0.3139745065799126\n",
      "batch loss: 0.10919614881277084 | avg loss: 0.30885504763573407\n",
      "batch loss: 0.15306052565574646 | avg loss: 0.3050551812459783\n",
      "batch loss: 0.391986221075058 | avg loss: 0.3071249679085754\n",
      "batch loss: 0.30141007900238037 | avg loss: 0.3069920635154081\n",
      "batch loss: 0.5094484090805054 | avg loss: 0.311593344096433\n",
      "batch loss: 0.22168534994125366 | avg loss: 0.30959538867076236\n",
      "batch loss: 0.6152401566505432 | avg loss: 0.3162398401485837\n",
      "batch loss: 0.13745468854904175 | avg loss: 0.31243590075284877\n",
      "batch loss: 0.35928794741630554 | avg loss: 0.31341198505833745\n",
      "batch loss: 0.3330030143260956 | avg loss: 0.31381180198216924\n",
      "batch loss: 0.7766145467758179 | avg loss: 0.32306785687804224\n",
      "batch loss: 0.1273323893547058 | avg loss: 0.3192299065344474\n",
      "batch loss: 0.14453157782554626 | avg loss: 0.31587032329004544\n",
      "batch loss: 0.10363318026065826 | avg loss: 0.3118658488932646\n",
      "batch loss: 0.18482178449630737 | avg loss: 0.30951318103406167\n",
      "batch loss: 0.19277295470237732 | avg loss: 0.30739063146439466\n",
      "batch loss: 0.08605040609836578 | avg loss: 0.30343812744000126\n",
      "batch loss: 0.43860262632369995 | avg loss: 0.30580943443796094\n",
      "batch loss: 0.24187332391738892 | avg loss: 0.3047070877048476\n",
      "batch loss: 0.20037473738193512 | avg loss: 0.30293874278412025\n",
      "batch loss: 0.21523268520832062 | avg loss: 0.30147697515785693\n",
      "batch loss: 0.2628794312477112 | avg loss: 0.30084422853637915\n",
      "batch loss: 0.05008399486541748 | avg loss: 0.2967997086384604\n",
      "batch loss: 0.23014432191848755 | avg loss: 0.29574168662703226\n",
      "batch loss: 0.3364371061325073 | avg loss: 0.2963775525568053\n",
      "batch loss: 0.4372718036174774 | avg loss: 0.2985451564192772\n",
      "batch loss: 0.08522976189851761 | avg loss: 0.29531310498714447\n",
      "batch loss: 0.5434043407440186 | avg loss: 0.29901595925217245\n",
      "batch loss: 0.2486150711774826 | avg loss: 0.2982747697216623\n",
      "batch loss: 0.30554690957069397 | avg loss: 0.2983801630528077\n",
      "batch loss: 0.39494454860687256 | avg loss: 0.2997596542750086\n",
      "batch loss: 0.18945513665676117 | avg loss: 0.29820606951982204\n",
      "batch loss: 0.18495869636535645 | avg loss: 0.29663318933712113\n",
      "batch loss: 0.4776208698749542 | avg loss: 0.2991124726321599\n",
      "batch loss: 0.1358051598072052 | avg loss: 0.2969056170534443\n",
      "batch loss: 0.08122222125530243 | avg loss: 0.29402983844280245\n",
      "batch loss: 0.16583509743213654 | avg loss: 0.29234306553476735\n",
      "batch loss: 0.2737872004508972 | avg loss: 0.29210208027393786\n",
      "batch loss: 0.1000525951385498 | avg loss: 0.2896399073875867\n",
      "batch loss: 0.2782107889652252 | avg loss: 0.2894952350024936\n",
      "batch loss: 0.20907823741436005 | avg loss: 0.2884900225326419\n",
      "batch loss: 0.046778496354818344 | avg loss: 0.28550592961686627\n",
      "batch loss: 0.13803838193416595 | avg loss: 0.2837075448890285\n",
      "batch loss: 0.22293207049369812 | avg loss: 0.28297531025775946\n",
      "batch loss: 0.0814373716711998 | avg loss: 0.28057604908410994\n",
      "batch loss: 0.12903796136379242 | avg loss: 0.2787932480521062\n",
      "batch loss: 0.2457083910703659 | avg loss: 0.2784085404127836\n",
      "batch loss: 0.04194752499461174 | avg loss: 0.27569059770682763\n",
      "batch loss: 0.1280110776424408 | avg loss: 0.27401242134245957\n",
      "batch loss: 0.15036971867084503 | avg loss: 0.2726231774922167\n",
      "batch loss: 0.11622875183820724 | avg loss: 0.27088546165161664\n",
      "batch loss: 0.15334801375865936 | avg loss: 0.2695938413451006\n",
      "batch loss: 0.1705305576324463 | avg loss: 0.26851706652213697\n",
      "batch loss: 0.15929345786571503 | avg loss: 0.26734261911722923\n",
      "batch loss: 0.3513886332511902 | avg loss: 0.2682367256505692\n",
      "batch loss: 0.38780516386032104 | avg loss: 0.26949534078961923\n",
      "batch loss: 0.17087385058403015 | avg loss: 0.2684680335999777\n",
      "batch loss: 0.6188979744911194 | avg loss: 0.2720807134029792\n",
      "batch loss: 0.2669193148612976 | avg loss: 0.2720280460709212\n",
      "batch loss: 0.1851133108139038 | avg loss: 0.27115011945216344\n",
      "batch loss: 0.07448772341012955 | avg loss: 0.2691834954917431\n",
      "batch loss: 0.07816722244024277 | avg loss: 0.2672922452635104\n",
      "batch loss: 0.31552958488464355 | avg loss: 0.26776516035783526\n",
      "batch loss: 0.389874666929245 | avg loss: 0.26895068954784895\n",
      "batch loss: 0.12950199842453003 | avg loss: 0.26760983674858624\n",
      "batch loss: 0.12427885830402374 | avg loss: 0.266244779811019\n",
      "batch loss: 0.20113521814346313 | avg loss: 0.26563053866321185\n",
      "batch loss: 0.11366931349039078 | avg loss: 0.2642103402971107\n",
      "batch loss: 0.4604231119155884 | avg loss: 0.26602712521950406\n",
      "batch loss: 0.438496470451355 | avg loss: 0.26760941279043843\n",
      "batch loss: 0.17249570786952972 | avg loss: 0.2667447427457029\n",
      "batch loss: 0.5147548913955688 | avg loss: 0.2689790684092152\n",
      "batch loss: 0.4721512198448181 | avg loss: 0.27079310547560453\n",
      "batch loss: 0.48925599455833435 | avg loss: 0.2727264053789915\n",
      "batch loss: 0.1507965326309204 | avg loss: 0.2716568450917277\n",
      "batch loss: 0.16680043935775757 | avg loss: 0.27074505025925844\n",
      "batch loss: 0.20514802634716034 | avg loss: 0.27017955867380933\n",
      "batch loss: 0.31344613432884216 | avg loss: 0.2705493584657327\n",
      "batch loss: 0.3864353597164154 | avg loss: 0.2715314432220944\n",
      "batch loss: 0.48960381746292114 | avg loss: 0.27336398418210134\n",
      "batch loss: 0.3643234968185425 | avg loss: 0.27412198012073835\n",
      "batch loss: 0.2523404359817505 | avg loss: 0.27394196735925913\n",
      "batch loss: 0.6612807512283325 | avg loss: 0.27711687542375973\n",
      "batch loss: 0.40926191210746765 | avg loss: 0.27819122531549717\n",
      "batch loss: 0.2846708297729492 | avg loss: 0.27824348019015405\n",
      "batch loss: 0.556401789188385 | avg loss: 0.2804687466621399\n",
      "batch loss: 0.39017564058303833 | avg loss: 0.2813394362964327\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:31:21\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.56\n",
      "  Validation took: 0:02:34\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "batch loss: 0.32386356592178345 | avg loss: 0.32386356592178345\n",
      "batch loss: 0.12120985239744186 | avg loss: 0.22253670915961266\n",
      "batch loss: 0.12248408794403076 | avg loss: 0.18918583542108536\n",
      "batch loss: 0.06946734338998795 | avg loss: 0.159256212413311\n",
      "batch loss: 0.13771440088748932 | avg loss: 0.15494785010814666\n",
      "batch loss: 0.15555359423160553 | avg loss: 0.1550488074620565\n",
      "batch loss: 0.2546977996826172 | avg loss: 0.16928437777927943\n",
      "batch loss: 0.2988656759262085 | avg loss: 0.18548204004764557\n",
      "batch loss: 0.4216403663158417 | avg loss: 0.21172185407744515\n",
      "batch loss: 0.5178045630455017 | avg loss: 0.2423301249742508\n",
      "batch loss: 0.19021806120872498 | avg loss: 0.23759266463193027\n",
      "batch loss: 0.15310366451740265 | avg loss: 0.2305519146223863\n",
      "batch loss: 0.08196679502725601 | avg loss: 0.2191222900381455\n",
      "batch loss: 0.31403541564941406 | avg loss: 0.22590179901037896\n",
      "batch loss: 0.45738688111305237 | avg loss: 0.24133413781722388\n",
      "batch loss: 0.28845110535621643 | avg loss: 0.2442789482884109\n",
      "batch loss: 0.16404913365840912 | avg loss: 0.23955954742782257\n",
      "batch loss: 0.18113651871681213 | avg loss: 0.2363138236105442\n",
      "batch loss: 0.5455887913703918 | avg loss: 0.25259145349264145\n",
      "batch loss: 0.2577072083950043 | avg loss: 0.2528472412377596\n",
      "batch loss: 0.3555716574192047 | avg loss: 0.2577388801035427\n",
      "batch loss: 0.05911988392472267 | avg loss: 0.24871074391359632\n",
      "batch loss: 0.27987274527549744 | avg loss: 0.2500656135380268\n",
      "batch loss: 0.34276890754699707 | avg loss: 0.2539282507884006\n",
      "batch loss: 0.20266008377075195 | avg loss: 0.25187752410769465\n",
      "batch loss: 0.3747751712799072 | avg loss: 0.25660435669124126\n",
      "batch loss: 0.38757291436195374 | avg loss: 0.2614550440123788\n",
      "batch loss: 0.4032784104347229 | avg loss: 0.2665201642417482\n",
      "batch loss: 0.2855997681617737 | avg loss: 0.2671780816183008\n",
      "batch loss: 0.1547725796699524 | avg loss: 0.26343123155335585\n",
      "batch loss: 0.03862694650888443 | avg loss: 0.256179480422889\n",
      "batch loss: 0.28351399302482605 | avg loss: 0.25703368394169956\n",
      "batch loss: 0.17134414613246918 | avg loss: 0.2544370312808138\n",
      "batch loss: 0.09345826506614685 | avg loss: 0.24970236168626478\n",
      "batch loss: 0.19914104044437408 | avg loss: 0.24825775250792503\n",
      "batch loss: 0.11308500170707703 | avg loss: 0.24450295387456814\n",
      "batch loss: 0.0625220537185669 | avg loss: 0.23958455116764918\n",
      "batch loss: 0.0495828352868557 | avg loss: 0.23458450601289146\n",
      "batch loss: 0.20648060739040375 | avg loss: 0.2338638932276995\n",
      "batch loss: 0.03619777038693428 | avg loss: 0.22892224015668033\n",
      "batch loss: 0.09771721810102463 | avg loss: 0.225722117667518\n",
      "batch loss: 0.10192646086215973 | avg loss: 0.2227746020292952\n",
      "batch loss: 0.12663063406944275 | avg loss: 0.22053869579767071\n",
      "batch loss: 0.5430484414100647 | avg loss: 0.22786846274340694\n",
      "batch loss: 0.16598941385746002 | avg loss: 0.22649337276816367\n",
      "batch loss: 0.3639559745788574 | avg loss: 0.22948169019883094\n",
      "batch loss: 0.09206824004650116 | avg loss: 0.22655799977005797\n",
      "batch loss: 0.2422942966222763 | avg loss: 0.2268858392878125\n",
      "batch loss: 0.4851432144641876 | avg loss: 0.23215639796488138\n",
      "batch loss: 0.6705673933029175 | avg loss: 0.2409246178716421\n",
      "batch loss: 0.27798742055892944 | avg loss: 0.24165133949296147\n",
      "batch loss: 0.022986462339758873 | avg loss: 0.23744624570155373\n",
      "batch loss: 0.04983402043581009 | avg loss: 0.2339063923946529\n",
      "batch loss: 0.20254772901535034 | avg loss: 0.2333256764061473\n",
      "batch loss: 0.23291873931884766 | avg loss: 0.23331827755001458\n",
      "batch loss: 0.02305697463452816 | avg loss: 0.22956361142652376\n",
      "batch loss: 0.32277512550354004 | avg loss: 0.23119890114717317\n",
      "batch loss: 0.07362356036901474 | avg loss: 0.2284820849268601\n",
      "batch loss: 0.13326148688793182 | avg loss: 0.22686817648552232\n",
      "batch loss: 0.1241065040230751 | avg loss: 0.22515548194448154\n",
      "batch loss: 0.09985444694757462 | avg loss: 0.22310136661666338\n",
      "batch loss: 0.03800226375460625 | avg loss: 0.22011589721566246\n",
      "batch loss: 0.2690611481666565 | avg loss: 0.22089280596091634\n",
      "batch loss: 0.24992923438549042 | avg loss: 0.2213465001550503\n",
      "batch loss: 0.4820188879966736 | avg loss: 0.22535684458338298\n",
      "batch loss: 0.056921180337667465 | avg loss: 0.22280478906450848\n",
      "batch loss: 0.19556975364685059 | avg loss: 0.2223982959985733\n",
      "batch loss: 0.17484356462955475 | avg loss: 0.2216989617137348\n",
      "batch loss: 0.2339951992034912 | avg loss: 0.22187716805416605\n",
      "batch loss: 0.08505457639694214 | avg loss: 0.21992255960192\n",
      "batch loss: 0.04552900791168213 | avg loss: 0.21746631239501524\n",
      "batch loss: 0.07410021126270294 | avg loss: 0.21547511654595533\n",
      "batch loss: 0.6124860048294067 | avg loss: 0.22091362186490673\n",
      "batch loss: 0.4254809319972992 | avg loss: 0.22367804497480392\n",
      "batch loss: 0.4123174250125885 | avg loss: 0.22619323670864105\n",
      "batch loss: 0.166996568441391 | avg loss: 0.22541433317880882\n",
      "batch loss: 0.3548103868961334 | avg loss: 0.22709480140890395\n",
      "batch loss: 0.02112310566008091 | avg loss: 0.22445413864289337\n",
      "batch loss: 0.03784555196762085 | avg loss: 0.22209200463434564\n",
      "batch loss: 0.28348881006240845 | avg loss: 0.22285946470219642\n",
      "batch loss: 0.04217987507581711 | avg loss: 0.2206288524845868\n",
      "batch loss: 0.11667770892381668 | avg loss: 0.21936115561189448\n",
      "batch loss: 0.22896045446395874 | avg loss: 0.2194768098149314\n",
      "batch loss: 0.09326598048210144 | avg loss: 0.21797429994192152\n",
      "batch loss: 0.04335547983646393 | avg loss: 0.2159199608818573\n",
      "batch loss: 0.180535227060318 | avg loss: 0.2155085104885836\n",
      "batch loss: 0.07956771552562714 | avg loss: 0.21394597261544618\n",
      "batch loss: 0.07267266511917114 | avg loss: 0.21234059412117032\n",
      "batch loss: 0.13199710845947266 | avg loss: 0.21143785832721865\n",
      "batch loss: 0.04292472451925278 | avg loss: 0.2095654901737968\n",
      "batch loss: 0.36445116996765137 | avg loss: 0.2112675306110919\n",
      "batch loss: 0.0601903572678566 | avg loss: 0.2096253874225785\n",
      "batch loss: 0.04202521964907646 | avg loss: 0.20782323508092793\n",
      "batch loss: 0.19132070243358612 | avg loss: 0.20764767622297747\n",
      "batch loss: 0.17549432814121246 | avg loss: 0.20730921992737997\n",
      "batch loss: 0.05995306000113487 | avg loss: 0.20577425992814824\n",
      "batch loss: 0.38077622652053833 | avg loss: 0.2075784039136368\n",
      "batch loss: 0.23224224150180817 | avg loss: 0.20783007572576098\n",
      "batch loss: 0.3970996141433716 | avg loss: 0.2097418892451308\n",
      "batch loss: 0.02911875583231449 | avg loss: 0.20793565791100263\n",
      "batch loss: 0.03008064068853855 | avg loss: 0.2061747171464238\n",
      "batch loss: 0.3184252083301544 | avg loss: 0.207275212158029\n",
      "batch loss: 0.22469478845596313 | avg loss: 0.20744433425800893\n",
      "batch loss: 0.3188425302505493 | avg loss: 0.2085154707579372\n",
      "batch loss: 0.01769736409187317 | avg loss: 0.20669815545635564\n",
      "batch loss: 0.2367357760667801 | avg loss: 0.20698152923569926\n",
      "batch loss: 0.07732219994068146 | avg loss: 0.20576975980303555\n",
      "batch loss: 0.490761399269104 | avg loss: 0.2084085712795732\n",
      "batch loss: 0.20342767238616943 | avg loss: 0.20836287495945024\n",
      "batch loss: 0.07912550866603851 | avg loss: 0.20718798981132833\n",
      "batch loss: 0.35050368309020996 | avg loss: 0.20847912218321013\n",
      "batch loss: 0.35854098200798035 | avg loss: 0.20981896021735988\n",
      "batch loss: 0.1837986707687378 | avg loss: 0.20958869216914197\n",
      "batch loss: 0.18673379719257355 | avg loss: 0.2093882106342598\n",
      "batch loss: 0.20825237035751343 | avg loss: 0.2093783337622881\n",
      "batch loss: 0.05393197387456894 | avg loss: 0.2080382789356698\n",
      "batch loss: 0.24954445660114288 | avg loss: 0.20839303259093028\n",
      "batch loss: 0.36426204442977905 | avg loss: 0.20971395642007307\n",
      "batch loss: 0.19593068957328796 | avg loss: 0.20959813064825134\n",
      "batch loss: 0.3977009356021881 | avg loss: 0.2111656540228675\n",
      "batch loss: 0.22582803666591644 | avg loss: 0.21128683073892573\n",
      "batch loss: 0.4372102618217468 | avg loss: 0.21313866214124394\n",
      "batch loss: 0.3516390025615692 | avg loss: 0.21426468116905145\n",
      "batch loss: 0.12570448219776154 | avg loss: 0.2135504860160572\n",
      "batch loss: 0.22064687311649323 | avg loss: 0.2136072571128607\n",
      "batch loss: 0.1759389042854309 | avg loss: 0.2133083019316906\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:31:19\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.58\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "batch loss: 0.0885327085852623 | avg loss: 0.0885327085852623\n",
      "batch loss: 0.03689493238925934 | avg loss: 0.06271382048726082\n",
      "batch loss: 0.15193389356136322 | avg loss: 0.09245384484529495\n",
      "batch loss: 0.03584348410367966 | avg loss: 0.07830125465989113\n",
      "batch loss: 0.13605374097824097 | avg loss: 0.0898517519235611\n",
      "batch loss: 0.054091259837150574 | avg loss: 0.08389166990915935\n",
      "batch loss: 0.23281650245189667 | avg loss: 0.10516664598669324\n",
      "batch loss: 0.07028606534004211 | avg loss: 0.10080657340586185\n",
      "batch loss: 0.23418407142162323 | avg loss: 0.11562629540761311\n",
      "batch loss: 0.30234411358833313 | avg loss: 0.13429807722568513\n",
      "batch loss: 0.039808355271816254 | avg loss: 0.12570810250260614\n",
      "batch loss: 0.09087296575307846 | avg loss: 0.12280517444014549\n",
      "batch loss: 0.03572782501578331 | avg loss: 0.11610691679211763\n",
      "batch loss: 0.16832660138607025 | avg loss: 0.11983689426311425\n",
      "batch loss: 0.18434303998947144 | avg loss: 0.12413730397820473\n",
      "batch loss: 0.2374441772699356 | avg loss: 0.1312189835589379\n",
      "batch loss: 0.08953285217285156 | avg loss: 0.12876685818328576\n",
      "batch loss: 0.1079590767621994 | avg loss: 0.12761087032655874\n",
      "batch loss: 0.31498903036117554 | avg loss: 0.1374728787494333\n",
      "batch loss: 0.017986249178647995 | avg loss: 0.13149854727089405\n",
      "batch loss: 0.25806716084480286 | avg loss: 0.13752562410774685\n",
      "batch loss: 0.022333526983857155 | avg loss: 0.13228961969302458\n",
      "batch loss: 0.1371195912361145 | avg loss: 0.13249961845576763\n",
      "batch loss: 0.3525792062282562 | avg loss: 0.14166960127962133\n",
      "batch loss: 0.02922869101166725 | avg loss: 0.13717196486890315\n",
      "batch loss: 0.25113290548324585 | avg loss: 0.1415550779694548\n",
      "batch loss: 0.21719466149806976 | avg loss: 0.14435654402607018\n",
      "batch loss: 0.15828180313110352 | avg loss: 0.14485387470839278\n",
      "batch loss: 0.07785844057798386 | avg loss: 0.14254368732458558\n",
      "batch loss: 0.11014866828918457 | avg loss: 0.1414638533567389\n",
      "batch loss: 0.013079839758574963 | avg loss: 0.13732243356324972\n",
      "batch loss: 0.22537991404533386 | avg loss: 0.14007422982831486\n",
      "batch loss: 0.29313889145851135 | avg loss: 0.14471255290801777\n",
      "batch loss: 0.07846240699291229 | avg loss: 0.14276401920463233\n",
      "batch loss: 0.09920625388622284 | avg loss: 0.14151951162410634\n",
      "batch loss: 0.01504506729543209 | avg loss: 0.1380063326149765\n",
      "batch loss: 0.0342700257897377 | avg loss: 0.1352026486467268\n",
      "batch loss: 0.017968282103538513 | avg loss: 0.13211753373769552\n",
      "batch loss: 0.0858478769659996 | avg loss: 0.13093113228201103\n",
      "batch loss: 0.08564456552267075 | avg loss: 0.12979896811302752\n",
      "batch loss: 0.02072453312575817 | avg loss: 0.1271386160401673\n",
      "batch loss: 0.024470601230859756 | avg loss: 0.12469413949708853\n",
      "batch loss: 0.15659299492835999 | avg loss: 0.1254359733443274\n",
      "batch loss: 0.4747070074081421 | avg loss: 0.1333739513912323\n",
      "batch loss: 0.1691114604473114 | avg loss: 0.13416811825914515\n",
      "batch loss: 0.39530470967292786 | avg loss: 0.1398450006811839\n",
      "batch loss: 0.029058581218123436 | avg loss: 0.13748784282026774\n",
      "batch loss: 0.18908147513866425 | avg loss: 0.13856271016023433\n",
      "batch loss: 0.374387264251709 | avg loss: 0.14337545616210115\n",
      "batch loss: 0.7129257917404175 | avg loss: 0.15476646287366747\n",
      "batch loss: 0.3457527160644531 | avg loss: 0.15851129136760445\n",
      "batch loss: 0.10091038793325424 | avg loss: 0.15740358168617463\n",
      "batch loss: 0.09971718490123749 | avg loss: 0.15631515910532676\n",
      "batch loss: 0.17201881110668182 | avg loss: 0.15660596747572222\n",
      "batch loss: 0.18864236772060394 | avg loss: 0.15718844748017463\n",
      "batch loss: 0.027962183579802513 | avg loss: 0.15488083562481084\n",
      "batch loss: 0.1530642807483673 | avg loss: 0.15484896624101357\n",
      "batch loss: 0.08120197802782059 | avg loss: 0.15357919058216543\n",
      "batch loss: 0.18869338929653168 | avg loss: 0.1541743464925784\n",
      "batch loss: 0.061102379113435745 | avg loss: 0.15262314703625937\n",
      "batch loss: 0.0832771584391594 | avg loss: 0.151486327551061\n",
      "batch loss: 0.018552865833044052 | avg loss: 0.14934223945883493\n",
      "batch loss: 0.10073383897542953 | avg loss: 0.14857067754639994\n",
      "batch loss: 0.022481700405478477 | avg loss: 0.14660053727857303\n",
      "batch loss: 0.2682737708091736 | avg loss: 0.1484724331790438\n",
      "batch loss: 0.03233786299824715 | avg loss: 0.14671281847933476\n",
      "batch loss: 0.028949091210961342 | avg loss: 0.14495515090816502\n",
      "batch loss: 0.07744200527667999 | avg loss: 0.1439623105312314\n",
      "batch loss: 0.07399032264947891 | avg loss: 0.14294822375033645\n",
      "batch loss: 0.08303721249103546 | avg loss: 0.14209235216091787\n",
      "batch loss: 0.018157334998250008 | avg loss: 0.14034678853890845\n",
      "batch loss: 0.04043472930788994 | avg loss: 0.13895912104958874\n",
      "batch loss: 0.1762993484735489 | avg loss: 0.13947063101430054\n",
      "batch loss: 0.3244470953941345 | avg loss: 0.14197031296537937\n",
      "batch loss: 0.02036433108150959 | avg loss: 0.14034889987359445\n",
      "batch loss: 0.11563549190759659 | avg loss: 0.14002372345298922\n",
      "batch loss: 0.17225953936576843 | avg loss: 0.14044237041289542\n",
      "batch loss: 0.009236861020326614 | avg loss: 0.1387602484976061\n",
      "batch loss: 0.011662233620882034 | avg loss: 0.13715141286625515\n",
      "batch loss: 0.1866132616996765 | avg loss: 0.1377696859766729\n",
      "batch loss: 0.01531999371945858 | avg loss: 0.13625796138090485\n",
      "batch loss: 0.028623774647712708 | avg loss: 0.13494534934757324\n",
      "batch loss: 0.3274270296096802 | avg loss: 0.1372644057362733\n",
      "batch loss: 0.008186210878193378 | avg loss: 0.1357277605593914\n",
      "batch loss: 0.01281834952533245 | avg loss: 0.13428176748840248\n",
      "batch loss: 0.11047776788473129 | avg loss: 0.13400497679533654\n",
      "batch loss: 0.012000606395304203 | avg loss: 0.1326026277102787\n",
      "batch loss: 0.009766197763383389 | avg loss: 0.1312067591881549\n",
      "batch loss: 0.22310511767864227 | avg loss: 0.13223932501389069\n",
      "batch loss: 0.09851586818695068 | avg loss: 0.1318646199380358\n",
      "batch loss: 0.1147245243191719 | avg loss: 0.13167626723892742\n",
      "batch loss: 0.011888443492352962 | avg loss: 0.13037422567646464\n",
      "batch loss: 0.1001967340707779 | avg loss: 0.13004973651941426\n",
      "batch loss: 0.25351178646087646 | avg loss: 0.1313631625826213\n",
      "batch loss: 0.04790762811899185 | avg loss: 0.13048468327247784\n",
      "batch loss: 0.0329936221241951 | avg loss: 0.12946915138551654\n",
      "batch loss: 0.33918648958206177 | avg loss: 0.13163118579991392\n",
      "batch loss: 0.2131323367357254 | avg loss: 0.13246283019721813\n",
      "batch loss: 0.4640582799911499 | avg loss: 0.1358122791850356\n",
      "batch loss: 0.16363230347633362 | avg loss: 0.1360904794279486\n",
      "batch loss: 0.026842523366212845 | avg loss: 0.13500881649664428\n",
      "batch loss: 0.21369627118110657 | avg loss: 0.13578026213080568\n",
      "batch loss: 0.46140336990356445 | avg loss: 0.1389416515266577\n",
      "batch loss: 0.07943432033061981 | avg loss: 0.13836946564977273\n",
      "batch loss: 0.007257562596350908 | avg loss: 0.13712078085878776\n",
      "batch loss: 0.4509606957435608 | avg loss: 0.14008153477279506\n",
      "batch loss: 0.18931984901428223 | avg loss: 0.14054170593393045\n",
      "batch loss: 0.3653024137020111 | avg loss: 0.14262282359844972\n",
      "batch loss: 0.23889754712581635 | avg loss: 0.14350607794273748\n",
      "batch loss: 0.056359075009822845 | avg loss: 0.14271383246152916\n",
      "batch loss: 0.38329842686653137 | avg loss: 0.1448812612399526\n",
      "batch loss: 0.4145783483982086 | avg loss: 0.14728927094672276\n",
      "batch loss: 0.24887992441654205 | avg loss: 0.14818830327831406\n",
      "batch loss: 0.015194886364042759 | avg loss: 0.14702169435801343\n",
      "batch loss: 0.2862347960472107 | avg loss: 0.14823224306835428\n",
      "batch loss: 0.01623614877462387 | avg loss: 0.14709434570375315\n",
      "batch loss: 0.2820932865142822 | avg loss: 0.14824818280469784\n",
      "batch loss: 0.5372529625892639 | avg loss: 0.15154483348083825\n",
      "batch loss: 0.03935077786445618 | avg loss: 0.15060202629078462\n",
      "batch loss: 0.2723742723464966 | avg loss: 0.15161679500791556\n",
      "batch loss: 0.06269077211618423 | avg loss: 0.150881869198893\n",
      "batch loss: 0.49657708406448364 | avg loss: 0.15371543653385683\n",
      "batch loss: 0.23253759741783142 | avg loss: 0.1543562671101493\n",
      "batch loss: 0.05483318865299225 | avg loss: 0.1535536616387206\n",
      "batch loss: 0.2520570456981659 | avg loss: 0.1543416887111962\n",
      "batch loss: 0.12050432711839676 | avg loss: 0.15407313822236443\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:31:13\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.58\n",
      "  Validation took: 0:02:30\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "batch loss: 0.058592889457941055 | avg loss: 0.058592889457941055\n",
      "batch loss: 0.07785926014184952 | avg loss: 0.06822607479989529\n",
      "batch loss: 0.20166559517383575 | avg loss: 0.11270591492454211\n",
      "batch loss: 0.10217782855033875 | avg loss: 0.11007389333099127\n",
      "batch loss: 0.08463148027658463 | avg loss: 0.10498541072010995\n",
      "batch loss: 0.028421640396118164 | avg loss: 0.09222478233277798\n",
      "batch loss: 0.1918925940990448 | avg loss: 0.10646304115653038\n",
      "batch loss: 0.10808072984218597 | avg loss: 0.10666525224223733\n",
      "batch loss: 0.0581844225525856 | avg loss: 0.10127849338783158\n",
      "batch loss: 0.192369744181633 | avg loss: 0.11038761846721172\n",
      "batch loss: 0.016526775434613228 | avg loss: 0.10185481455515731\n",
      "batch loss: 0.027284324169158936 | avg loss: 0.09564060702299078\n",
      "batch loss: 0.010245781391859055 | avg loss: 0.0890717742821345\n",
      "batch loss: 0.07253946363925934 | avg loss: 0.08789089495050055\n",
      "batch loss: 0.06549550592899323 | avg loss: 0.0863978690157334\n",
      "batch loss: 0.22600288689136505 | avg loss: 0.09512318263296038\n",
      "batch loss: 0.029594482854008675 | avg loss: 0.09126855323419851\n",
      "batch loss: 0.12977871298789978 | avg loss: 0.09340800655384858\n",
      "batch loss: 0.38455408811569214 | avg loss: 0.10873148453078772\n",
      "batch loss: 0.13663843274116516 | avg loss: 0.1101268319413066\n",
      "batch loss: 0.1627235859632492 | avg loss: 0.11263143927568481\n",
      "batch loss: 0.05560779944062233 | avg loss: 0.11003945564681833\n",
      "batch loss: 0.12682244181632996 | avg loss: 0.11076915069766667\n",
      "batch loss: 0.2993703782558441 | avg loss: 0.11862753517925739\n",
      "batch loss: 0.01984833925962448 | avg loss: 0.11467636734247208\n",
      "batch loss: 0.16465164721012115 | avg loss: 0.1165984934912278\n",
      "batch loss: 0.1631130576133728 | avg loss: 0.11832125512538133\n",
      "batch loss: 0.014997960068285465 | avg loss: 0.11463113744477076\n",
      "batch loss: 0.09360596537590027 | avg loss: 0.11390613151136143\n",
      "batch loss: 0.030539363622665405 | avg loss: 0.1111272392484049\n",
      "batch loss: 0.11848373711109161 | avg loss: 0.11136454563107222\n",
      "batch loss: 0.2835325002670288 | avg loss: 0.11674479421344586\n",
      "batch loss: 0.13879522681236267 | avg loss: 0.11741298914068576\n",
      "batch loss: 0.028211094439029694 | avg loss: 0.11478940400240176\n",
      "batch loss: 0.07910410314798355 | avg loss: 0.1137698239779898\n",
      "batch loss: 0.03094073385000229 | avg loss: 0.11146901591887905\n",
      "batch loss: 0.0210610032081604 | avg loss: 0.10902555611588664\n",
      "batch loss: 0.010565848089754581 | avg loss: 0.10643451116783054\n",
      "batch loss: 0.08335577696561813 | avg loss: 0.10584274875238919\n",
      "batch loss: 0.026227211579680443 | avg loss: 0.10385236032307148\n",
      "batch loss: 0.009034830145537853 | avg loss: 0.10153973763581456\n",
      "batch loss: 0.01532997190952301 | avg loss: 0.09948712416614096\n",
      "batch loss: 0.0998070165514946 | avg loss: 0.09949456352393987\n",
      "batch loss: 0.47697585821151733 | avg loss: 0.10807368385774846\n",
      "batch loss: 0.18132956326007843 | avg loss: 0.10970159228891134\n",
      "batch loss: 0.1954285055398941 | avg loss: 0.11156522083784574\n",
      "batch loss: 0.024898823350667953 | avg loss: 0.10972125493386324\n",
      "batch loss: 0.21565930545330048 | avg loss: 0.11192829765301819\n",
      "batch loss: 0.43131110072135925 | avg loss: 0.118446314042168\n",
      "batch loss: 0.6109628677368164 | avg loss: 0.12829664511606098\n",
      "batch loss: 0.28962627053260803 | avg loss: 0.1314599711046207\n",
      "batch loss: 0.023291317746043205 | avg loss: 0.12937980469387883\n",
      "batch loss: 0.028214598074555397 | avg loss: 0.12747102721049539\n",
      "batch loss: 0.02868424355983734 | avg loss: 0.1256416423280758\n",
      "batch loss: 0.2147042453289032 | avg loss: 0.1272609623826363\n",
      "batch loss: 0.019453495740890503 | avg loss: 0.12533582904974797\n",
      "batch loss: 0.20273518562316895 | avg loss: 0.12669371249840447\n",
      "batch loss: 0.1373639702796936 | avg loss: 0.12687768246015085\n",
      "batch loss: 0.27421677112579346 | avg loss: 0.12937495514939903\n",
      "batch loss: 0.04045679792761803 | avg loss: 0.12789298586236933\n",
      "batch loss: 0.06023765355348587 | avg loss: 0.126783882054027\n",
      "batch loss: 0.008182401768863201 | avg loss: 0.12487095495265338\n",
      "batch loss: 0.04399888217449188 | avg loss: 0.12358727125776192\n",
      "batch loss: 0.014000261202454567 | avg loss: 0.12187497422564775\n",
      "batch loss: 0.15271861851215363 | avg loss: 0.12234949183005553\n",
      "batch loss: 0.010422393679618835 | avg loss: 0.12065362670656407\n",
      "batch loss: 0.013176115229725838 | avg loss: 0.1190494847442232\n",
      "batch loss: 0.02763300947844982 | avg loss: 0.11770512481384418\n",
      "batch loss: 0.012975831516087055 | avg loss: 0.11618730896894915\n",
      "batch loss: 0.04753197729587555 | avg loss: 0.11520651851647666\n",
      "batch loss: 0.012051723897457123 | avg loss: 0.11375363408522286\n",
      "batch loss: 0.01123996451497078 | avg loss: 0.11232983311896937\n",
      "batch loss: 0.26376795768737793 | avg loss: 0.11440432797607085\n",
      "batch loss: 0.3302045166492462 | avg loss: 0.11732054674192458\n",
      "batch loss: 0.013417902402579784 | avg loss: 0.11593517815073331\n",
      "batch loss: 0.007130415644496679 | avg loss: 0.11450353653880914\n",
      "batch loss: 0.06698615103960037 | avg loss: 0.11388642763622202\n",
      "batch loss: 0.005830083973705769 | avg loss: 0.11250108989695899\n",
      "batch loss: 0.08171768486499786 | avg loss: 0.11211142654212404\n",
      "batch loss: 0.03694156929850578 | avg loss: 0.1111718033265788\n",
      "batch loss: 0.005249386187642813 | avg loss: 0.10986411916436972\n",
      "batch loss: 0.02026182971894741 | avg loss: 0.10877140831747432\n",
      "batch loss: 0.24988669157028198 | avg loss: 0.11047159245305033\n",
      "batch loss: 0.005558960605412722 | avg loss: 0.10922263255010226\n",
      "batch loss: 0.0123002203181386 | avg loss: 0.10808236887678505\n",
      "batch loss: 0.013081302866339684 | avg loss: 0.10697770531852405\n",
      "batch loss: 0.005738551262766123 | avg loss: 0.10581403688110154\n",
      "batch loss: 0.06939110904932022 | avg loss: 0.10540013997392221\n",
      "batch loss: 0.007930799387395382 | avg loss: 0.10430497884373652\n",
      "batch loss: 0.03607090562582016 | avg loss: 0.10354682247464855\n",
      "batch loss: 0.11213215440511703 | avg loss: 0.10364116678157678\n",
      "batch loss: 0.22451511025428772 | avg loss: 0.10495501399323669\n",
      "batch loss: 0.038163069635629654 | avg loss: 0.10423682104315489\n",
      "batch loss: 0.29506784677505493 | avg loss: 0.1062669383381751\n",
      "batch loss: 0.008370348252356052 | avg loss: 0.10523644791621911\n",
      "batch loss: 0.0904192104935646 | avg loss: 0.10508210169306646\n",
      "batch loss: 0.3520204424858093 | avg loss: 0.1076278577837133\n",
      "batch loss: 0.07197456061840057 | avg loss: 0.1072640486289652\n",
      "batch loss: 0.4032961130142212 | avg loss: 0.11025427150154354\n",
      "batch loss: 0.012471633963286877 | avg loss: 0.10927644512616098\n",
      "batch loss: 0.0410451665520668 | avg loss: 0.1086008879125561\n",
      "batch loss: 0.3086095452308655 | avg loss: 0.11056175710195128\n",
      "batch loss: 0.441940039396286 | avg loss: 0.11377902197859531\n",
      "batch loss: 0.14122174680233002 | avg loss: 0.11404289433266968\n",
      "batch loss: 0.008533044718205929 | avg loss: 0.11303803862205573\n",
      "batch loss: 0.31862887740135193 | avg loss: 0.11497757483695475\n",
      "batch loss: 0.045325782150030136 | avg loss: 0.11432662350343209\n",
      "batch loss: 0.37129655480384827 | avg loss: 0.1167059747191767\n",
      "batch loss: 0.27459999918937683 | avg loss: 0.11815454375101339\n",
      "batch loss: 0.008089070208370686 | avg loss: 0.11715394853698936\n",
      "batch loss: 0.3084404766559601 | avg loss: 0.11887725059211522\n",
      "batch loss: 0.34828731417655945 | avg loss: 0.12092555473126206\n",
      "batch loss: 0.03241581469774246 | avg loss: 0.12014228269556719\n",
      "batch loss: 0.004980376921594143 | avg loss: 0.11913209053965514\n",
      "batch loss: 0.0832216665148735 | avg loss: 0.1188198259829179\n",
      "batch loss: 0.005041495431214571 | avg loss: 0.11783897830574805\n",
      "batch loss: 0.15213137865066528 | avg loss: 0.1181320757445935\n",
      "batch loss: 0.4510820806026459 | avg loss: 0.12095368595525496\n",
      "batch loss: 0.009546269662678242 | avg loss: 0.12001748917968709\n",
      "batch loss: 0.194278284907341 | avg loss: 0.1206363291440842\n",
      "batch loss: 0.2171357274055481 | avg loss: 0.12143384483219548\n",
      "batch loss: 0.1895965188741684 | avg loss: 0.12199255527516246\n",
      "batch loss: 0.7143557667732239 | avg loss: 0.1268085163442524\n",
      "batch loss: 0.011554310098290443 | avg loss: 0.12587904693904303\n",
      "batch loss: 0.009852910414338112 | avg loss: 0.1249508378468454\n",
      "batch loss: 0.198087677359581 | avg loss: 0.12553128895408933\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:30:57\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.60\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "batch loss: 0.005936924833804369 | avg loss: 0.005936924833804369\n",
      "batch loss: 0.008228867314755917 | avg loss: 0.007082896074280143\n",
      "batch loss: 0.062018703669309616 | avg loss: 0.02539483193928997\n",
      "batch loss: 0.010034718550741673 | avg loss: 0.021554803592152894\n",
      "batch loss: 0.24511213600635529 | avg loss: 0.06626627007499337\n",
      "batch loss: 0.09231680631637573 | avg loss: 0.07060802611522377\n",
      "batch loss: 0.11037828028202057 | avg loss: 0.07628949099619474\n",
      "batch loss: 0.18154096603393555 | avg loss: 0.08944592537591234\n",
      "batch loss: 0.0998619943857193 | avg loss: 0.090603266377002\n",
      "batch loss: 0.162131667137146 | avg loss: 0.0977561064530164\n",
      "batch loss: 0.004671536851674318 | avg loss: 0.08929387285289439\n",
      "batch loss: 0.011781499721109867 | avg loss: 0.08283450842524569\n",
      "batch loss: 0.004289423581212759 | avg loss: 0.07679257882185854\n",
      "batch loss: 0.05896420404314995 | avg loss: 0.07551912348052221\n",
      "batch loss: 0.10641343891620636 | avg loss: 0.07757874450956782\n",
      "batch loss: 0.22360609471797943 | avg loss: 0.08670545389759354\n",
      "batch loss: 0.01596079021692276 | avg loss: 0.0825440030928482\n",
      "batch loss: 0.02542947232723236 | avg loss: 0.07937097360586955\n",
      "batch loss: 0.1799619048833847 | avg loss: 0.08466523314679139\n",
      "batch loss: 0.0042800395749509335 | avg loss: 0.08064597346819938\n",
      "batch loss: 0.1880972683429718 | avg loss: 0.08576270179556948\n",
      "batch loss: 0.005720173008739948 | avg loss: 0.08212440503253178\n",
      "batch loss: 0.10782139003276825 | avg loss: 0.08324166524993337\n",
      "batch loss: 0.21866704523563385 | avg loss: 0.08888438941600423\n",
      "batch loss: 0.007923549041152 | avg loss: 0.08564595580101013\n",
      "batch loss: 0.19930598139762878 | avg loss: 0.09001749524703392\n",
      "batch loss: 0.17969180643558502 | avg loss: 0.09333876603179508\n",
      "batch loss: 0.020729368552565575 | avg loss: 0.09074557326467973\n",
      "batch loss: 0.17061743140220642 | avg loss: 0.09349977526942203\n",
      "batch loss: 0.009416394867002964 | avg loss: 0.09069699592267473\n",
      "batch loss: 0.005539555102586746 | avg loss: 0.0879499817026719\n",
      "batch loss: 0.17830872535705566 | avg loss: 0.09077369244187139\n",
      "batch loss: 0.06069749593734741 | avg loss: 0.0898622925477949\n",
      "batch loss: 0.010728366672992706 | avg loss: 0.08753482413971249\n",
      "batch loss: 0.02351611666381359 | avg loss: 0.08570571821182967\n",
      "batch loss: 0.005200633779168129 | avg loss: 0.08346946586647795\n",
      "batch loss: 0.00477540073916316 | avg loss: 0.08134259924141539\n",
      "batch loss: 0.004553108010441065 | avg loss: 0.07932182315638975\n",
      "batch loss: 0.1571732461452484 | avg loss: 0.08131801348943742\n",
      "batch loss: 0.011837609112262726 | avg loss: 0.07958100338000804\n",
      "batch loss: 0.004114307928830385 | avg loss: 0.07774035227144273\n",
      "batch loss: 0.005970012862235308 | avg loss: 0.0760315346664616\n",
      "batch loss: 0.1217002421617508 | avg loss: 0.07709359763146834\n",
      "batch loss: 0.5039986968040466 | avg loss: 0.08679598624902693\n",
      "batch loss: 0.19362887740135193 | avg loss: 0.08917005049685638\n",
      "batch loss: 0.13485074043273926 | avg loss: 0.09016310897372339\n",
      "batch loss: 0.04774029180407524 | avg loss: 0.08926049584245428\n",
      "batch loss: 0.16472122073173523 | avg loss: 0.09083259427764763\n",
      "batch loss: 0.3571774661540985 | avg loss: 0.09626820390777928\n",
      "batch loss: 0.6092528700828552 | avg loss: 0.10652789723128081\n",
      "batch loss: 0.32352447509765625 | avg loss: 0.11078273209140581\n",
      "batch loss: 0.00806568842381239 | avg loss: 0.10880740432856748\n",
      "batch loss: 0.010838554240763187 | avg loss: 0.10695893545898627\n",
      "batch loss: 0.04855387285351753 | avg loss: 0.10587736022555166\n",
      "batch loss: 0.08681892603635788 | avg loss: 0.10553084324029359\n",
      "batch loss: 0.013414189219474792 | avg loss: 0.10388590298992183\n",
      "batch loss: 0.22977422177791595 | avg loss: 0.10609446998620242\n",
      "batch loss: 0.08079777657985687 | avg loss: 0.10565832009988613\n",
      "batch loss: 0.2650867700576782 | avg loss: 0.10836049721781481\n",
      "batch loss: 0.012874706648290157 | avg loss: 0.1067690673749894\n",
      "batch loss: 0.09539472311735153 | avg loss: 0.10658260271502812\n",
      "batch loss: 0.004639863036572933 | avg loss: 0.10493836497827884\n",
      "batch loss: 0.05573846772313118 | avg loss: 0.10415741422819713\n",
      "batch loss: 0.0063108718022704124 | avg loss: 0.10262856200279202\n",
      "batch loss: 0.11841236054897308 | avg loss: 0.10287138967273327\n",
      "batch loss: 0.00951153039932251 | avg loss: 0.10145684635040886\n",
      "batch loss: 0.013707315549254417 | avg loss: 0.10014715186083939\n",
      "batch loss: 0.015737680718302727 | avg loss: 0.09890583610874326\n",
      "batch loss: 0.004736090544611216 | avg loss: 0.09754105718752397\n",
      "batch loss: 0.02351059764623642 | avg loss: 0.096483479194077\n",
      "batch loss: 0.009669587016105652 | avg loss: 0.09526074831833092\n",
      "batch loss: 0.0055000861175358295 | avg loss: 0.094014072454431\n",
      "batch loss: 0.18101727962493896 | avg loss: 0.09520589721019138\n",
      "batch loss: 0.17318539321422577 | avg loss: 0.09625967418321886\n",
      "batch loss: 0.010839095339179039 | avg loss: 0.09512073313196501\n",
      "batch loss: 0.0036783237010240555 | avg loss: 0.09391754353418946\n",
      "batch loss: 0.01272257324308157 | avg loss: 0.09286306340053871\n",
      "batch loss: 0.02117425948381424 | avg loss: 0.09194397617083712\n",
      "batch loss: 0.008352571167051792 | avg loss: 0.09088585712015629\n",
      "batch loss: 0.09215464442968369 | avg loss: 0.09090171696152538\n",
      "batch loss: 0.004921840038150549 | avg loss: 0.08984023699950841\n",
      "batch loss: 0.02304614707827568 | avg loss: 0.08902567492729825\n",
      "batch loss: 0.30111080408096313 | avg loss: 0.09158091744722192\n",
      "batch loss: 0.15426723659038544 | avg loss: 0.09232718315130721\n",
      "batch loss: 0.05073857679963112 | avg loss: 0.09183790542952278\n",
      "batch loss: 0.011349528096616268 | avg loss: 0.09090199406518666\n",
      "batch loss: 0.00527639826759696 | avg loss: 0.08991779181463964\n",
      "batch loss: 0.020970109850168228 | avg loss: 0.08913429542867975\n",
      "batch loss: 0.026975644752383232 | avg loss: 0.0884358836233281\n",
      "batch loss: 0.006328750867396593 | avg loss: 0.0875235821482622\n",
      "batch loss: 0.18593116104602814 | avg loss: 0.08860498411417171\n",
      "batch loss: 0.015018726699054241 | avg loss: 0.08780513349009435\n",
      "batch loss: 0.1587551087141037 | avg loss: 0.0885680364494923\n",
      "batch loss: 0.201136976480484 | avg loss: 0.08976557836471562\n",
      "batch loss: 0.0065335542894899845 | avg loss: 0.08888945179550271\n",
      "batch loss: 0.11429499089717865 | avg loss: 0.08915409282781184\n",
      "batch loss: 0.31951409578323364 | avg loss: 0.09152893821910485\n",
      "batch loss: 0.2623308300971985 | avg loss: 0.0932718146668405\n",
      "batch loss: 0.3206213414669037 | avg loss: 0.0955682745335078\n",
      "batch loss: 0.006390373222529888 | avg loss: 0.09467649552039802\n",
      "batch loss: 0.01480289176106453 | avg loss: 0.09388566776040462\n",
      "batch loss: 0.10745055973529816 | avg loss: 0.09401865689741339\n",
      "batch loss: 0.23221871256828308 | avg loss: 0.09536040501072279\n",
      "batch loss: 0.1965530663728714 | avg loss: 0.09633341136997423\n",
      "batch loss: 0.1541317254304886 | avg loss: 0.09688387150388389\n",
      "batch loss: 0.18092158436775208 | avg loss: 0.09767668011580717\n",
      "batch loss: 0.025061646476387978 | avg loss: 0.09699803494160698\n",
      "batch loss: 0.3239569365978241 | avg loss: 0.09909950625323863\n",
      "batch loss: 0.20573753118515015 | avg loss: 0.10007783675720112\n",
      "batch loss: 0.01625475473701954 | avg loss: 0.09931580873883583\n",
      "batch loss: 0.13482749462127686 | avg loss: 0.09963573383687584\n",
      "batch loss: 0.30596697330474854 | avg loss: 0.10147797704641041\n",
      "batch loss: 0.1952938288450241 | avg loss: 0.1023082058233893\n",
      "batch loss: 0.004953636787831783 | avg loss: 0.10145421837570898\n",
      "batch loss: 0.006168539170175791 | avg loss: 0.10062564725218259\n",
      "batch loss: 0.0067260474897921085 | avg loss: 0.09981616794388613\n",
      "batch loss: 0.05577674135565758 | avg loss: 0.09943976258843118\n",
      "batch loss: 0.3543964922428131 | avg loss: 0.1016004128397395\n",
      "batch loss: 0.09138466417789459 | avg loss: 0.10151456621232904\n",
      "batch loss: 0.11288938671350479 | avg loss: 0.10160935638317217\n",
      "batch loss: 0.13423335552215576 | avg loss: 0.10187897621076708\n",
      "batch loss: 0.04880276322364807 | avg loss: 0.10144392528464316\n",
      "batch loss: 0.2529328167438507 | avg loss: 0.10267554228837655\n",
      "batch loss: 0.004761861637234688 | avg loss: 0.1018859158315125\n",
      "batch loss: 0.01885971613228321 | avg loss: 0.10122170623391867\n",
      "batch loss: 0.10011658072471619 | avg loss: 0.10121293539654404\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:31:07\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.59\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "batch loss: 0.008010934107005596 | avg loss: 0.008010934107005596\n",
      "batch loss: 0.004774619825184345 | avg loss: 0.006392776966094971\n",
      "batch loss: 0.03179875761270523 | avg loss: 0.014861437181631723\n",
      "batch loss: 0.013485130853950977 | avg loss: 0.014517360599711537\n",
      "batch loss: 0.23876675963401794 | avg loss: 0.05936724040657282\n",
      "batch loss: 0.08700757473707199 | avg loss: 0.06397396279498935\n",
      "batch loss: 0.11129225045442581 | avg loss: 0.07073371817490884\n",
      "batch loss: 0.006664338521659374 | avg loss: 0.06272504571825266\n",
      "batch loss: 0.010567938908934593 | avg loss: 0.05692981162832843\n",
      "batch loss: 0.1126314327120781 | avg loss: 0.06249997373670339\n",
      "batch loss: 0.004182188306003809 | avg loss: 0.05719835687936707\n",
      "batch loss: 0.004542873706668615 | avg loss: 0.05281039994830886\n",
      "batch loss: 0.004140418488532305 | avg loss: 0.049066555220633745\n",
      "batch loss: 0.06914811581373215 | avg loss: 0.05050095240585506\n",
      "batch loss: 0.019909627735614777 | avg loss: 0.04846153076117237\n",
      "batch loss: 0.17109201848506927 | avg loss: 0.05612593624391593\n",
      "batch loss: 0.025602102279663086 | avg loss: 0.05433041659895988\n",
      "batch loss: 0.12516415119171143 | avg loss: 0.05826562407633497\n",
      "batch loss: 0.2459580898284912 | avg loss: 0.06814417490539582\n",
      "batch loss: 0.003997746855020523 | avg loss: 0.06493685350287706\n",
      "batch loss: 0.14870010316371918 | avg loss: 0.06892557967720288\n",
      "batch loss: 0.00561254657804966 | avg loss: 0.06604771453633228\n",
      "batch loss: 0.0854284018278122 | avg loss: 0.0668903531142227\n",
      "batch loss: 0.1780742108821869 | avg loss: 0.07152301385455455\n",
      "batch loss: 0.03142603859305382 | avg loss: 0.06991913484409451\n",
      "batch loss: 0.15731649100780487 | avg loss: 0.07328057161962184\n",
      "batch loss: 0.08951941877603531 | avg loss: 0.0738820104031927\n",
      "batch loss: 0.007738949730992317 | avg loss: 0.0715197582363284\n",
      "batch loss: 0.023549942299723625 | avg loss: 0.06986562665230756\n",
      "batch loss: 0.008384776301681995 | avg loss: 0.06781626497395336\n",
      "batch loss: 0.004601901862770319 | avg loss: 0.06577709197036681\n",
      "batch loss: 0.19246163964271545 | avg loss: 0.06973598408512771\n",
      "batch loss: 0.014158331789076328 | avg loss: 0.06805181280342919\n",
      "batch loss: 0.005497785285115242 | avg loss: 0.06621198846465524\n",
      "batch loss: 0.01252985093742609 | avg loss: 0.06467821310673441\n",
      "batch loss: 0.004501753021031618 | avg loss: 0.06300664477102044\n",
      "batch loss: 0.003635532222688198 | avg loss: 0.061402020107552004\n",
      "batch loss: 0.005055427085608244 | avg loss: 0.05991921502802717\n",
      "batch loss: 0.0663200169801712 | avg loss: 0.060083338155005224\n",
      "batch loss: 0.01599724031984806 | avg loss: 0.05898118570912629\n",
      "batch loss: 0.12279783189296722 | avg loss: 0.06053768927458583\n",
      "batch loss: 0.0065610832534730434 | avg loss: 0.059252531988368856\n",
      "batch loss: 0.062120892107486725 | avg loss: 0.05931923803765067\n",
      "batch loss: 0.35465630888938904 | avg loss: 0.066031444193372\n",
      "batch loss: 0.18170469999313354 | avg loss: 0.06860196098892225\n",
      "batch loss: 0.04640134051442146 | avg loss: 0.06811933880469398\n",
      "batch loss: 0.008731675334274769 | avg loss: 0.06685577149681271\n",
      "batch loss: 0.10669485479593277 | avg loss: 0.06768575239887771\n",
      "batch loss: 0.4496479630470276 | avg loss: 0.0754808995549624\n",
      "batch loss: 0.45342108607292175 | avg loss: 0.08303970328532159\n",
      "batch loss: 0.20577006042003632 | avg loss: 0.08544618087619835\n",
      "batch loss: 0.004446028266102076 | avg loss: 0.0838884856336965\n",
      "batch loss: 0.00395460007712245 | avg loss: 0.08238029911376114\n",
      "batch loss: 0.12871068716049194 | avg loss: 0.08323826926277468\n",
      "batch loss: 0.08037390559911728 | avg loss: 0.08318618992343545\n",
      "batch loss: 0.006065166089683771 | avg loss: 0.08180902878354702\n",
      "batch loss: 0.1774328649044037 | avg loss: 0.08348663994356205\n",
      "batch loss: 0.06373877823352814 | avg loss: 0.08314615956925113\n",
      "batch loss: 0.1803148239850998 | avg loss: 0.08479308608477398\n",
      "batch loss: 0.005436473060399294 | avg loss: 0.08347047586770108\n",
      "batch loss: 0.02006502076983452 | avg loss: 0.08243104217757212\n",
      "batch loss: 0.003914105240255594 | avg loss: 0.08116463996890572\n",
      "batch loss: 0.04235270246863365 | avg loss: 0.0805485774689014\n",
      "batch loss: 0.0036248229444026947 | avg loss: 0.07934664380445611\n",
      "batch loss: 0.025425538420677185 | avg loss: 0.07851708833701336\n",
      "batch loss: 0.0051708160899579525 | avg loss: 0.07740578118175494\n",
      "batch loss: 0.01644757017493248 | avg loss: 0.076495957136877\n",
      "batch loss: 0.010129301808774471 | avg loss: 0.07551997691146373\n",
      "batch loss: 0.006347264628857374 | avg loss: 0.0745174738349042\n",
      "batch loss: 0.006123279221355915 | avg loss: 0.07354041391185352\n",
      "batch loss: 0.0062124039977788925 | avg loss: 0.07259213208207782\n",
      "batch loss: 0.005259686149656773 | avg loss: 0.0716569592219053\n",
      "batch loss: 0.10317064076662064 | avg loss: 0.07208865348964114\n",
      "batch loss: 0.23165379464626312 | avg loss: 0.07424493918094684\n",
      "batch loss: 0.006733453832566738 | avg loss: 0.07334478604296843\n",
      "batch loss: 0.003737248945981264 | avg loss: 0.07242889739695545\n",
      "batch loss: 0.003243036335334182 | avg loss: 0.0715303797208305\n",
      "batch loss: 0.0029913943726569414 | avg loss: 0.0706516747804693\n",
      "batch loss: 0.003041522577404976 | avg loss: 0.0697958500690381\n",
      "batch loss: 0.05529230833053589 | avg loss: 0.06961455579730683\n",
      "batch loss: 0.003376707900315523 | avg loss: 0.06879680458870199\n",
      "batch loss: 0.006685389205813408 | avg loss: 0.06803934830354481\n",
      "batch loss: 0.25905030965805054 | avg loss: 0.07034068518733404\n",
      "batch loss: 0.014905767515301704 | avg loss: 0.06968074569123842\n",
      "batch loss: 0.004874009173363447 | avg loss: 0.06891831349691048\n",
      "batch loss: 0.007590633351355791 | avg loss: 0.06820520093707844\n",
      "batch loss: 0.007223806343972683 | avg loss: 0.06750426536704275\n",
      "batch loss: 0.009446476586163044 | avg loss: 0.06684451776726003\n",
      "batch loss: 0.007950554601848125 | avg loss: 0.06618278784405314\n",
      "batch loss: 0.046727027744054794 | avg loss: 0.06596661273183094\n",
      "batch loss: 0.043625906109809875 | avg loss: 0.06572111046125928\n",
      "batch loss: 0.0037668044678866863 | avg loss: 0.0650476940917661\n",
      "batch loss: 0.04676610603928566 | avg loss: 0.06485111787614803\n",
      "batch loss: 0.14717283844947815 | avg loss: 0.0657268808609707\n",
      "batch loss: 0.0032525465358048677 | avg loss: 0.06506925628912684\n",
      "batch loss: 0.012830003164708614 | avg loss: 0.06452509740241415\n",
      "batch loss: 0.2202562689781189 | avg loss: 0.06613057339803997\n",
      "batch loss: 0.11389651894569397 | avg loss: 0.0666179810056691\n",
      "batch loss: 0.2866740822792053 | avg loss: 0.06884076990742198\n",
      "batch loss: 0.003052310785278678 | avg loss: 0.06818288531620055\n",
      "batch loss: 0.022728510200977325 | avg loss: 0.06773284199822804\n",
      "batch loss: 0.14402826130390167 | avg loss: 0.06848083630514641\n",
      "batch loss: 0.2118213176727295 | avg loss: 0.06987249146405498\n",
      "batch loss: 0.04445691779255867 | avg loss: 0.06962811094798291\n",
      "batch loss: 0.030576270073652267 | avg loss: 0.06925618865394166\n",
      "batch loss: 0.15206293761730194 | avg loss: 0.07003738439887902\n",
      "batch loss: 0.011990049853920937 | avg loss: 0.06949488594518782\n",
      "batch loss: 0.27088475227355957 | avg loss: 0.07135960692970979\n",
      "batch loss: 0.06865076720714569 | avg loss: 0.0713347551891358\n",
      "batch loss: 0.004232855048030615 | avg loss: 0.07072473791512576\n",
      "batch loss: 0.11299781501293182 | avg loss: 0.07110557644753843\n",
      "batch loss: 0.23726877570152283 | avg loss: 0.07258917644087758\n",
      "batch loss: 0.0651279166340828 | avg loss: 0.07252314759302983\n",
      "batch loss: 0.02450428158044815 | avg loss: 0.07210192947011244\n",
      "batch loss: 0.01126425713300705 | avg loss: 0.0715729062323985\n",
      "batch loss: 0.012226144783198833 | avg loss: 0.0710612962199054\n",
      "batch loss: 0.025440823286771774 | avg loss: 0.07067137764782733\n",
      "batch loss: 0.28060808777809143 | avg loss: 0.0724505023099482\n",
      "batch loss: 0.125618115067482 | avg loss: 0.07289728897177622\n",
      "batch loss: 0.12133397907018661 | avg loss: 0.07330092805592964\n",
      "batch loss: 0.021871473640203476 | avg loss: 0.07287589124257653\n",
      "batch loss: 0.09768569469451904 | avg loss: 0.07307925028726459\n",
      "batch loss: 0.32478445768356323 | avg loss: 0.0751256340872345\n",
      "batch loss: 0.0037559140473604202 | avg loss: 0.07455007182884842\n",
      "batch loss: 0.04882152006030083 | avg loss: 0.07434424341470003\n",
      "batch loss: 0.07037033140659332 | avg loss: 0.0743127044305087\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:31:08\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.59\n",
      "  Validation took: 0:02:32\n",
      "[[0.6991342  0.11688312 0.18398268]\n",
      " [0.34313725 0.4754902  0.18137255]\n",
      " [0.3819242  0.11953353 0.49854227]]\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "#function to train the model\n",
    "def training(n_epochs, training_dataloader,\n",
    "             validation_dataloader):\n",
    "    # ========================================\n",
    "    #               Training \n",
    "    # ========================================\n",
    "    print('======= Training =======')\n",
    "    for epoch_i in range(0,n_epochs):\n",
    "        # Perform one full pass over the training set\n",
    "        print(\"\")\n",
    "        print('======= Epoch {:} / {:} ======='.format(\n",
    "             epoch_i + 1, epochs))\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "        # For each batch of training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Clear any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # pull loss value out of the output tuple\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass \n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            # ¿take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            print('batch loss: {0} | avg loss: {1}'.format(\n",
    "                  batch_loss, total_loss/(step+1)))\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".\n",
    "             format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(\n",
    "              format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, \n",
    "        # measure accuracy on the validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"======= Validation =======\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # Model will not to compute gradients\n",
    "            with torch.no_grad():\n",
    "                # Forward pass \n",
    "                # This will return the logits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # The \"logits\" are the output values \n",
    "            # prior to applying an activation function \n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Save batch logits and labels \n",
    "            # We will use thoses in the confusion matrix\n",
    "            predict_labels = np.argmax(\n",
    "                             logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calculate the accuracy for this batch\n",
    "            tmp_eval_accuracy = flat_accuracy(\n",
    "                                logits, b_labels)\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".\n",
    "              format(eval_accuracy / (step+1)))\n",
    "        print(\"  Validation took: {:}\".format(\n",
    "             format_time(time.time() - t0)))\n",
    "\n",
    "    #print the confusion matrix\"\n",
    "    conf = confusion_matrix(\n",
    "           all_labels, all_logits, normalize='true')\n",
    "    print(conf)\n",
    "    print(\"\")\n",
    "    print(\"Training complete\")\n",
    "\n",
    "#call function to train the model\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d268ce55-e625-4816-a560-7807b2407bda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11705/4174074531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YlGnBu_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#plot the confusion matrix\n",
    "conf = [[0.6991342,  0.11688312, 0.18398268],\n",
    " [0.34313725, 0.4754902,  0.18137255],\n",
    " [0.3819242,  0.11953353, 0.49854227]]\n",
    "ticks = ['negative', 'neutral', 'positive']\n",
    "ax = sns.heatmap(conf, annot=True, xticklabels=ticks, yticklabels=ticks, cmap='YlGnBu_r')\n",
    "ax.set(xlabel='Predicted label', ylabel='True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6bd8dd-61c1-423e-88c3-0709c12ab798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model.softmax = torch.nn.Softmax(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f297c61-4f02-452d-90e8-d429afb7c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (softmax): Softmax(dim=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704b935-459d-40ea-9c28-d1a9cdfc72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 1000),\n",
    "        nn.Linear(1000, 128),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.Linear(64, 8)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b35b7a8-511c-4306-af90-d0ccaade13ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3197/2786942801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         )\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model,input_size=(768,1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377eda1-00ad-4781-939a-9af0b605f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
