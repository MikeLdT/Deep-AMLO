{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310df869-d5e3-413f-a72d-3142e7cec6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 4022\n",
      "Validation set length : 1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv('tweets_clean_full2.csv', header=0)\n",
    "reviews = df['text']\n",
    "sentiment = df['category']\n",
    "sentiment = pd.DataFrame(list(map(lambda x: 0 if x==\"M\" else 1, sentiment)))\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(reviews, \n",
    "sentiment, stratify=sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/jupyter/pytorch/\",\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=40,\n",
    "                   truncation=True,pad_to_max_length=True)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d384ef-dad7-4070-bedd-ae235aa929eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 32\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler,\n",
    "                 batch_size=batch_size,\n",
    "                 num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks,\n",
    "                   y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, \n",
    "                 y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ecb7b2-7598-477b-b14b-b85a83083130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jupyter/pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# set random seed\n",
    "def set_seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed_all(value)\n",
    "set_seed(42)\n",
    "\n",
    "# Create model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        \"/home/jupyter/pytorch/\", num_labels=2, output_attentions=False,\n",
    "         output_hidden_states=False)\n",
    "\n",
    "# Fine-tune model\n",
    "#nn.Sequential(\n",
    "        #nn.Linear(model.fc.in_features, 1000),\n",
    "        #nn.Linear(1000, 128),\n",
    "        #nn.Linear(128, 64),\n",
    "        #nn.Linear(64, 8)\n",
    "    #)\n",
    "\n",
    "#model.softmax = torch.nn.Softmax(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9417968b-9fe1-49b8-8f90-b33137c32bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                  lr = 4e-5,\n",
    "                  eps = 1e-6)\n",
    "\n",
    "if run_on == 'cuda':\n",
    "    model.cuda()\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "            num_warmup_steps = 0, \n",
    "            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71750184-c6c0-4add-b549-bd3dd6771234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1edf75-9151-43a9-809d-55b66b26190a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Training =======\n",
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "batch loss: 0.7083100080490112 | avg loss: 0.7083100080490112\n",
      "batch loss: 0.7769396305084229 | avg loss: 0.742624819278717\n",
      "batch loss: 0.7021797299385071 | avg loss: 0.7291431228319804\n",
      "batch loss: 0.6911103129386902 | avg loss: 0.7196349203586578\n",
      "batch loss: 0.6977524757385254 | avg loss: 0.7152584314346313\n",
      "batch loss: 0.6938290596008301 | avg loss: 0.7116868694623312\n",
      "batch loss: 0.5796637535095215 | avg loss: 0.6928264243262154\n",
      "batch loss: 0.7068294286727905 | avg loss: 0.6945767998695374\n",
      "batch loss: 0.7073830366134644 | avg loss: 0.695999715063307\n",
      "batch loss: 0.6898934245109558 | avg loss: 0.6953890860080719\n",
      "batch loss: 0.6756699681282043 | avg loss: 0.693596438928084\n",
      "batch loss: 0.5983133912086487 | avg loss: 0.6856561849514643\n",
      "batch loss: 0.6431620121002197 | avg loss: 0.6823874024244455\n",
      "batch loss: 0.7109085321426392 | avg loss: 0.684424625975745\n",
      "batch loss: 0.655364453792572 | avg loss: 0.6824872811635335\n",
      "batch loss: 0.6546603441238403 | avg loss: 0.6807480975985527\n",
      "batch loss: 0.6636900305747986 | avg loss: 0.679744681891273\n",
      "batch loss: 0.7175993323326111 | avg loss: 0.6818477180269029\n",
      "batch loss: 0.6355711221694946 | avg loss: 0.6794121077186183\n",
      "batch loss: 0.6763104200363159 | avg loss: 0.6792570233345032\n",
      "batch loss: 0.7228298783302307 | avg loss: 0.6813319211914426\n",
      "batch loss: 0.6628745198249817 | avg loss: 0.680492948402058\n",
      "batch loss: 0.656189501285553 | avg loss: 0.6794362767882969\n",
      "batch loss: 0.7095030546188354 | avg loss: 0.6806890591979027\n",
      "batch loss: 0.6739058494567871 | avg loss: 0.6804177308082581\n",
      "batch loss: 0.6961514353752136 | avg loss: 0.6810228732916025\n",
      "batch loss: 0.6410034894943237 | avg loss: 0.6795406738917033\n",
      "batch loss: 0.6574957370758057 | avg loss: 0.678753354719707\n",
      "batch loss: 0.7009973526000977 | avg loss: 0.6795203891293756\n",
      "batch loss: 0.6545180678367615 | avg loss: 0.6786869784196218\n",
      "batch loss: 0.7166807651519775 | avg loss: 0.6799125844432462\n",
      "batch loss: 0.7171465754508972 | avg loss: 0.6810761466622353\n",
      "batch loss: 0.6634525656700134 | avg loss: 0.68054209875338\n",
      "batch loss: 0.6641343235969543 | avg loss: 0.6800595171311322\n",
      "batch loss: 0.6232108473777771 | avg loss: 0.6784352694238935\n",
      "batch loss: 0.6827594637870789 | avg loss: 0.678555385933982\n",
      "batch loss: 0.6918222308158875 | avg loss: 0.6789139493091686\n",
      "batch loss: 0.635502815246582 | avg loss: 0.6777715510443637\n",
      "batch loss: 0.6648234724998474 | avg loss: 0.6774395490304018\n",
      "batch loss: 0.7161272168159485 | avg loss: 0.6784067407250405\n",
      "batch loss: 0.6509695053100586 | avg loss: 0.6777375398612604\n",
      "batch loss: 0.6178114414215088 | avg loss: 0.6763107279936472\n",
      "batch loss: 0.703332781791687 | avg loss: 0.6769391478494157\n",
      "batch loss: 0.6205440759658813 | avg loss: 0.6756574416702444\n",
      "batch loss: 0.7153427600860596 | avg loss: 0.6765393376350403\n",
      "batch loss: 0.8200739026069641 | avg loss: 0.6796596542648647\n",
      "batch loss: 0.6893928647041321 | avg loss: 0.679866743848679\n",
      "batch loss: 0.6262863874435425 | avg loss: 0.6787504864235719\n",
      "batch loss: 0.599337637424469 | avg loss: 0.6771298160358351\n",
      "batch loss: 0.6729069948196411 | avg loss: 0.6770453596115112\n",
      "batch loss: 0.6960299611091614 | avg loss: 0.6774176066997004\n",
      "batch loss: 0.6132748126983643 | avg loss: 0.676184091430444\n",
      "batch loss: 0.6054298877716064 | avg loss: 0.6748491064557489\n",
      "batch loss: 0.6845725774765015 | avg loss: 0.675029170733911\n",
      "batch loss: 0.7146070003509521 | avg loss: 0.6757487676360391\n",
      "batch loss: 0.6734607815742493 | avg loss: 0.6757079107420785\n",
      "batch loss: 0.6754177212715149 | avg loss: 0.6757028196987352\n",
      "batch loss: 0.6726717948913574 | avg loss: 0.6756505606503322\n",
      "batch loss: 0.5978385806083679 | avg loss: 0.6743317135309769\n",
      "batch loss: 0.7004776000976562 | avg loss: 0.6747674783070882\n",
      "batch loss: 0.6706059575080872 | avg loss: 0.6746992566546456\n",
      "batch loss: 0.9012086987495422 | avg loss: 0.6783526347529504\n",
      "batch loss: 0.7257732152938843 | avg loss: 0.6791053423805843\n",
      "batch loss: 0.7006117105484009 | avg loss: 0.6794413793832064\n",
      "batch loss: 0.7030004262924194 | avg loss: 0.6798038262587327\n",
      "batch loss: 0.6287429332733154 | avg loss: 0.6790301763650143\n",
      "batch loss: 0.6511473059654236 | avg loss: 0.6786140141202442\n",
      "batch loss: 0.6832147836685181 | avg loss: 0.6786816724959541\n",
      "batch loss: 0.6839126944541931 | avg loss: 0.6787574844083925\n",
      "batch loss: 0.6702669262886047 | avg loss: 0.6786361907209669\n",
      "batch loss: 0.7022114992141724 | avg loss: 0.6789682373194628\n",
      "batch loss: 0.6569346785545349 | avg loss: 0.6786622156699499\n",
      "batch loss: 0.5461122989654541 | avg loss: 0.6768464633863266\n",
      "batch loss: 0.6621509790420532 | avg loss: 0.6766478757600527\n",
      "batch loss: 0.5570410490036011 | avg loss: 0.6750531180699666\n",
      "batch loss: 0.6377095580101013 | avg loss: 0.6745617554376\n",
      "batch loss: 0.5353347659111023 | avg loss: 0.6727536127164766\n",
      "batch loss: 0.8360311985015869 | avg loss: 0.674846915098337\n",
      "batch loss: 0.7084273099899292 | avg loss: 0.675271983388104\n",
      "batch loss: 0.6372858285903931 | avg loss: 0.6747971564531327\n",
      "batch loss: 0.6841539740562439 | avg loss: 0.6749126727198377\n",
      "batch loss: 0.5804540514945984 | avg loss: 0.6737607383146519\n",
      "batch loss: 0.6080024838447571 | avg loss: 0.6729684701885086\n",
      "batch loss: 0.7133599519729614 | avg loss: 0.673449321162133\n",
      "batch loss: 0.5546690225601196 | avg loss: 0.6720519058844623\n",
      "batch loss: 0.6200152635574341 | avg loss: 0.6714468286481015\n",
      "batch loss: 0.5793976187705994 | avg loss: 0.6703887917529577\n",
      "batch loss: 0.6427025198936462 | avg loss: 0.6700741750272837\n",
      "batch loss: 0.7301948666572571 | avg loss: 0.6707496884163846\n",
      "batch loss: 0.6370025873184204 | avg loss: 0.6703747206264072\n",
      "batch loss: 0.5871295928955078 | avg loss: 0.6694599390029907\n",
      "batch loss: 0.7616557478904724 | avg loss: 0.6704620673604633\n",
      "batch loss: 0.7252340316772461 | avg loss: 0.6710510132133319\n",
      "batch loss: 0.5153437256813049 | avg loss: 0.6693945527076721\n",
      "batch loss: 0.622329831123352 | avg loss: 0.6688991345857319\n",
      "batch loss: 0.6584264039993286 | avg loss: 0.6687900436421236\n",
      "batch loss: 0.5899249315261841 | avg loss: 0.6679770012491757\n",
      "batch loss: 0.7373244762420654 | avg loss: 0.6686846285450215\n",
      "batch loss: 0.6714754104614258 | avg loss: 0.6687128182613489\n",
      "batch loss: 0.589518129825592 | avg loss: 0.6679208713769913\n",
      "batch loss: 0.5370438694953918 | avg loss: 0.6666250594771734\n",
      "batch loss: 0.6002185940742493 | avg loss: 0.6659740156987134\n",
      "batch loss: 0.5733880400657654 | avg loss: 0.6650751227314032\n",
      "batch loss: 0.6614018678665161 | avg loss: 0.6650398029730871\n",
      "batch loss: 0.7143121957778931 | avg loss: 0.6655090638569423\n",
      "batch loss: 0.531186580657959 | avg loss: 0.664241870619216\n",
      "batch loss: 0.5305187106132507 | avg loss: 0.6629921214602817\n",
      "batch loss: 0.7245741486549377 | avg loss: 0.6635623254157879\n",
      "batch loss: 0.6722228527069092 | avg loss: 0.6636417797946055\n",
      "batch loss: 0.4187965393066406 | avg loss: 0.6614159139719876\n",
      "batch loss: 0.6497794985771179 | avg loss: 0.6613110814008627\n",
      "batch loss: 0.6599839329719543 | avg loss: 0.6612992318613189\n",
      "batch loss: 0.6057531237602234 | avg loss: 0.6608076733825481\n",
      "batch loss: 0.7817342877388 | avg loss: 0.6618684331576029\n",
      "batch loss: 0.5095371007919312 | avg loss: 0.6605438128761624\n",
      "batch loss: 0.5918462872505188 | avg loss: 0.6599515928276654\n",
      "batch loss: 0.7180061340332031 | avg loss: 0.6604477854875418\n",
      "batch loss: 0.6061142086982727 | avg loss: 0.6599873314469548\n",
      "batch loss: 0.5793928503990173 | avg loss: 0.6593100669003334\n",
      "batch loss: 0.6108725666999817 | avg loss: 0.6589064210653305\n",
      "batch loss: 0.5344938039779663 | avg loss: 0.6578782176183275\n",
      "batch loss: 0.6969553232192993 | avg loss: 0.6581985217625977\n",
      "batch loss: 0.711207389831543 | avg loss: 0.6586294881696624\n",
      "batch loss: 0.5627537965774536 | avg loss: 0.6578562971084349\n",
      "batch loss: 0.6335527896881104 | avg loss: 0.6576618690490723\n",
      "batch loss: 0.6266340613365173 | avg loss: 0.6574156166069092\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epoch took: 0:12:45\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "batch loss: 0.5798250436782837 | avg loss: 0.5798250436782837\n",
      "batch loss: 0.6373275518417358 | avg loss: 0.6085762977600098\n",
      "batch loss: 0.581779956817627 | avg loss: 0.5996441841125488\n",
      "batch loss: 0.5686841011047363 | avg loss: 0.5919041633605957\n",
      "batch loss: 0.6009998321533203 | avg loss: 0.5937232971191406\n",
      "batch loss: 0.6050043702125549 | avg loss: 0.595603475968043\n",
      "batch loss: 0.44014573097229004 | avg loss: 0.5733952266829354\n",
      "batch loss: 0.5696797966957092 | avg loss: 0.5729307979345322\n",
      "batch loss: 0.7683289051055908 | avg loss: 0.5946416987313164\n",
      "batch loss: 0.5685915946960449 | avg loss: 0.5920366883277893\n",
      "batch loss: 0.5671727657318115 | avg loss: 0.5897763317281549\n",
      "batch loss: 0.5550732612609863 | avg loss: 0.5868844091892242\n",
      "batch loss: 0.5580857992172241 | avg loss: 0.5846691314990704\n",
      "batch loss: 0.5864736437797546 | avg loss: 0.584798025233405\n",
      "batch loss: 0.4763486683368683 | avg loss: 0.5775680681069691\n",
      "batch loss: 0.5571039319038391 | avg loss: 0.5762890595942736\n",
      "batch loss: 0.6989677548408508 | avg loss: 0.5835054534323075\n",
      "batch loss: 0.599897027015686 | avg loss: 0.5844160964091619\n",
      "batch loss: 0.5313639044761658 | avg loss: 0.5816238757811094\n",
      "batch loss: 0.47203174233436584 | avg loss: 0.5761442691087723\n",
      "batch loss: 0.6868404150009155 | avg loss: 0.5814155141512553\n",
      "batch loss: 0.4679839611053467 | avg loss: 0.5762595344673503\n",
      "batch loss: 0.5200929641723633 | avg loss: 0.5738175096719161\n",
      "batch loss: 0.6007289886474609 | avg loss: 0.5749388212958971\n",
      "batch loss: 0.5258625745773315 | avg loss: 0.5729757714271545\n",
      "batch loss: 0.6232792139053345 | avg loss: 0.5749105192147769\n",
      "batch loss: 0.5179218649864197 | avg loss: 0.5727998283174303\n",
      "batch loss: 0.7004520893096924 | avg loss: 0.5773588376385825\n",
      "batch loss: 0.5494949817657471 | avg loss: 0.5763980150222778\n",
      "batch loss: 0.5516359210014343 | avg loss: 0.5755726118882497\n",
      "batch loss: 0.6450568437576294 | avg loss: 0.5778140387227458\n",
      "batch loss: 0.6708230376243591 | avg loss: 0.5807205699384212\n",
      "batch loss: 0.6025605797767639 | avg loss: 0.581382388418371\n",
      "batch loss: 0.5077615976333618 | avg loss: 0.5792170710423413\n",
      "batch loss: 0.532943606376648 | avg loss: 0.577894972051893\n",
      "batch loss: 0.6124768853187561 | avg loss: 0.5788555807537503\n",
      "batch loss: 0.723007321357727 | avg loss: 0.582751573743047\n",
      "batch loss: 0.48823559284210205 | avg loss: 0.5802643110877589\n",
      "batch loss: 0.553035318851471 | avg loss: 0.579566131799649\n",
      "batch loss: 0.6372314691543579 | avg loss: 0.5810077652335167\n",
      "batch loss: 0.5443246364593506 | avg loss: 0.5801130547756101\n",
      "batch loss: 0.4850912094116211 | avg loss: 0.5778506298859915\n",
      "batch loss: 0.5972609519958496 | avg loss: 0.5783020327257555\n",
      "batch loss: 0.5793749690055847 | avg loss: 0.5783264176412062\n",
      "batch loss: 0.569310188293457 | avg loss: 0.578126056989034\n",
      "batch loss: 0.5860950946807861 | avg loss: 0.5782992969388547\n",
      "batch loss: 0.54653000831604 | avg loss: 0.577623354627731\n",
      "batch loss: 0.38584187626838684 | avg loss: 0.5736279071619114\n",
      "batch loss: 0.4193342328071594 | avg loss: 0.5704790566648755\n",
      "batch loss: 0.5370906591415405 | avg loss: 0.5698112887144089\n",
      "batch loss: 0.5891652703285217 | avg loss: 0.5701907785499797\n",
      "batch loss: 0.3951129913330078 | avg loss: 0.5668238980265764\n",
      "batch loss: 0.3247388005256653 | avg loss: 0.5622562546775026\n",
      "batch loss: 0.47346043586730957 | avg loss: 0.560611887662499\n",
      "batch loss: 0.564045250415802 | avg loss: 0.5606743124398318\n",
      "batch loss: 0.5281732082366943 | avg loss: 0.5600939355790615\n",
      "batch loss: 0.39545363187789917 | avg loss: 0.5572055091983393\n",
      "batch loss: 0.47154349088668823 | avg loss: 0.5557285778481384\n",
      "batch loss: 0.4537997841835022 | avg loss: 0.5540009711758566\n",
      "batch loss: 0.4634840786457062 | avg loss: 0.552492356300354\n",
      "batch loss: 0.5157541632652283 | avg loss: 0.5518900908407618\n",
      "batch loss: 1.0410349369049072 | avg loss: 0.5597795238417964\n",
      "batch loss: 0.359283447265625 | avg loss: 0.5565970464358254\n",
      "batch loss: 0.5392279624938965 | avg loss: 0.5563256544992328\n",
      "batch loss: 0.6193364858627319 | avg loss: 0.5572950519048251\n",
      "batch loss: 0.6802597641944885 | avg loss: 0.5591581536061836\n",
      "batch loss: 0.484965443611145 | avg loss: 0.5580507997256606\n",
      "batch loss: 0.7696599364280701 | avg loss: 0.5611626987948137\n",
      "batch loss: 0.3577122986316681 | avg loss: 0.5582141422707102\n",
      "batch loss: 0.6183951497077942 | avg loss: 0.5590738709483828\n",
      "batch loss: 0.604777455329895 | avg loss: 0.5597175834044604\n",
      "batch loss: 0.42221885919570923 | avg loss: 0.5578078789015611\n",
      "batch loss: 0.4297955334186554 | avg loss: 0.5560542851278226\n",
      "batch loss: 0.4483378827571869 | avg loss: 0.5545986580687601\n",
      "batch loss: 0.37270280718803406 | avg loss: 0.552173380057017\n",
      "batch loss: 0.5489475727081299 | avg loss: 0.552130935223479\n",
      "batch loss: 0.46328356862068176 | avg loss: 0.5509770733195466\n",
      "batch loss: 0.6094778776168823 | avg loss: 0.5517270836310509\n",
      "batch loss: 0.5330637693405151 | avg loss: 0.5514908391463605\n",
      "batch loss: 0.5794859528541565 | avg loss: 0.551840778067708\n",
      "batch loss: 0.5855347514152527 | avg loss: 0.5522567530473074\n",
      "batch loss: 0.41610029339790344 | avg loss: 0.5505963084174366\n",
      "batch loss: 0.41073131561279297 | avg loss: 0.5489111880221998\n",
      "batch loss: 0.5272059440612793 | avg loss: 0.5486527922607604\n",
      "batch loss: 0.48434460163116455 | avg loss: 0.5478962253121769\n",
      "batch loss: 0.4068779945373535 | avg loss: 0.5462564784427022\n",
      "batch loss: 0.39389437437057495 | avg loss: 0.544505189890149\n",
      "batch loss: 0.4433380961418152 | avg loss: 0.543355563824827\n",
      "batch loss: 0.508171558380127 | avg loss: 0.5429602379209539\n",
      "batch loss: 0.5580520033836365 | avg loss: 0.5431279242038727\n",
      "batch loss: 0.6466741561889648 | avg loss: 0.5442657948850276\n",
      "batch loss: 0.7158624529838562 | avg loss: 0.5461309759513192\n",
      "batch loss: 0.6250749230384827 | avg loss: 0.5469798355974177\n",
      "batch loss: 0.4895448386669159 | avg loss: 0.5463688249917741\n",
      "batch loss: 0.5215287208557129 | avg loss: 0.5461073502113944\n",
      "batch loss: 0.5871161222457886 | avg loss: 0.546534524920086\n",
      "batch loss: 0.43575382232666016 | avg loss: 0.5453924558211848\n",
      "batch loss: 0.7241923213005066 | avg loss: 0.5472169442444431\n",
      "batch loss: 0.6613625884056091 | avg loss: 0.5483699305491014\n",
      "batch loss: 0.4431297183036804 | avg loss: 0.5473175284266472\n",
      "batch loss: 0.4620736837387085 | avg loss: 0.5464735299643904\n",
      "batch loss: 0.43364986777305603 | avg loss: 0.5453674156291812\n",
      "batch loss: 0.4372616112232208 | avg loss: 0.5443178447126185\n",
      "batch loss: 0.5247027277946472 | avg loss: 0.5441292378191764\n",
      "batch loss: 0.5526895523071289 | avg loss: 0.5442107646238237\n",
      "batch loss: 0.3516506552696228 | avg loss: 0.5423941598185953\n",
      "batch loss: 0.38844022154808044 | avg loss: 0.5409553379655998\n",
      "batch loss: 0.5409976243972778 | avg loss: 0.540955729506634\n",
      "batch loss: 0.4805516302585602 | avg loss: 0.5404015634584864\n",
      "batch loss: 0.3023689389228821 | avg loss: 0.5382376305081628\n",
      "batch loss: 0.5134438872337341 | avg loss: 0.5380142634516364\n",
      "batch loss: 0.39831283688545227 | avg loss: 0.5367669292858669\n",
      "batch loss: 0.3958132266998291 | avg loss: 0.5355195513868753\n",
      "batch loss: 0.7337929010391235 | avg loss: 0.5372587912961057\n",
      "batch loss: 0.2588152289390564 | avg loss: 0.5348375429277835\n",
      "batch loss: 0.5242757201194763 | avg loss: 0.5347464927311602\n",
      "batch loss: 0.9643617868423462 | avg loss: 0.538418418321854\n",
      "batch loss: 0.4265128970146179 | avg loss: 0.537470066446369\n",
      "batch loss: 0.3819066882133484 | avg loss: 0.5361628111670999\n",
      "batch loss: 0.4165326654911041 | avg loss: 0.5351658932864666\n",
      "batch loss: 0.3066326677799225 | avg loss: 0.5332771889434373\n",
      "batch loss: 0.6155352592468262 | avg loss: 0.5339514354213339\n",
      "batch loss: 0.49453359842300415 | avg loss: 0.5336309652018353\n",
      "batch loss: 0.40605875849723816 | avg loss: 0.5326021570832499\n",
      "batch loss: 0.49451178312301636 | avg loss: 0.532297434091568\n",
      "batch loss: 0.3846155107021332 | avg loss: 0.531125355334509\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epoch took: 0:12:41\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:59\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "batch loss: 0.40759050846099854 | avg loss: 0.40759050846099854\n",
      "batch loss: 0.5655401349067688 | avg loss: 0.48656532168388367\n",
      "batch loss: 0.5478100776672363 | avg loss: 0.5069802403450012\n",
      "batch loss: 0.45646965503692627 | avg loss: 0.4943525940179825\n",
      "batch loss: 0.41295409202575684 | avg loss: 0.47807289361953736\n",
      "batch loss: 0.4407042860984802 | avg loss: 0.47184479236602783\n",
      "batch loss: 0.37384387850761414 | avg loss: 0.45784466181482586\n",
      "batch loss: 0.4117867052555084 | avg loss: 0.4520874172449112\n",
      "batch loss: 0.6642078757286072 | avg loss: 0.47565635707643295\n",
      "batch loss: 0.3627637028694153 | avg loss: 0.4643670916557312\n",
      "batch loss: 0.36214885115623474 | avg loss: 0.45507452433759515\n",
      "batch loss: 0.44265860319137573 | avg loss: 0.4540398642420769\n",
      "batch loss: 0.43626946210861206 | avg loss: 0.45267291023181033\n",
      "batch loss: 0.3225221633911133 | avg loss: 0.4433764283146177\n",
      "batch loss: 0.3257194757461548 | avg loss: 0.43553263147672017\n",
      "batch loss: 0.4731804430484772 | avg loss: 0.437885619699955\n",
      "batch loss: 0.5265087485313416 | avg loss: 0.4430987449253307\n",
      "batch loss: 0.4585234820842743 | avg loss: 0.4439556747674942\n",
      "batch loss: 0.3382527232170105 | avg loss: 0.43839236152799504\n",
      "batch loss: 0.291958212852478 | avg loss: 0.4310706540942192\n",
      "batch loss: 0.4811498820781708 | avg loss: 0.43345537923631217\n",
      "batch loss: 0.32973459362983704 | avg loss: 0.42874079807238147\n",
      "batch loss: 0.2451641708612442 | avg loss: 0.4207592055849407\n",
      "batch loss: 0.33866843581199646 | avg loss: 0.41733875684440136\n",
      "batch loss: 0.3427307605743408 | avg loss: 0.4143544369935989\n",
      "batch loss: 0.5302324295043945 | avg loss: 0.4188112828593988\n",
      "batch loss: 0.24501386284828186 | avg loss: 0.41237434137750556\n",
      "batch loss: 0.5633834004402161 | avg loss: 0.41776752205831663\n",
      "batch loss: 0.24129346013069153 | avg loss: 0.4116822095780537\n",
      "batch loss: 0.23028913140296936 | avg loss: 0.4056357736388842\n",
      "batch loss: 0.5078001022338867 | avg loss: 0.4089313971419488\n",
      "batch loss: 0.4460158348083496 | avg loss: 0.41009028581902385\n",
      "batch loss: 0.46797582507133484 | avg loss: 0.4118443930690939\n",
      "batch loss: 0.19374185800552368 | avg loss: 0.4054296126260477\n",
      "batch loss: 0.38540124893188477 | avg loss: 0.40485737366335733\n",
      "batch loss: 0.5188124775886536 | avg loss: 0.40802279321683776\n",
      "batch loss: 0.6253243684768677 | avg loss: 0.4138958087644061\n",
      "batch loss: 0.32598909735679626 | avg loss: 0.41158247425367955\n",
      "batch loss: 0.22455739974975586 | avg loss: 0.40678695952280974\n",
      "batch loss: 0.4670994281768799 | avg loss: 0.40829477123916147\n",
      "batch loss: 0.4012242555618286 | avg loss: 0.4081223196372753\n",
      "batch loss: 0.30459195375442505 | avg loss: 0.4056573109257789\n",
      "batch loss: 0.3600641191005707 | avg loss: 0.4045970041391461\n",
      "batch loss: 0.48357704281806946 | avg loss: 0.4063920050182126\n",
      "batch loss: 0.3848859965801239 | avg loss: 0.40591409371958836\n",
      "batch loss: 0.4166927635669708 | avg loss: 0.4061484126293141\n",
      "batch loss: 0.30401939153671265 | avg loss: 0.4039754547337268\n",
      "batch loss: 0.21177096664905548 | avg loss: 0.3999711945652962\n",
      "batch loss: 0.2593602240085602 | avg loss: 0.39710158292128117\n",
      "batch loss: 0.5290351510047913 | avg loss: 0.39974025428295135\n",
      "batch loss: 0.6817598342895508 | avg loss: 0.4052700499693553\n",
      "batch loss: 0.30616194009780884 | avg loss: 0.40336412477951783\n",
      "batch loss: 0.23837876319885254 | avg loss: 0.4002511934289392\n",
      "batch loss: 0.2302497923374176 | avg loss: 0.3971030193346518\n",
      "batch loss: 0.3327172100543976 | avg loss: 0.395932368256829\n",
      "batch loss: 0.39050501585006714 | avg loss: 0.3958354512495654\n",
      "batch loss: 0.1584218442440033 | avg loss: 0.3916703002494678\n",
      "batch loss: 0.2253742218017578 | avg loss: 0.388803126483128\n",
      "batch loss: 0.26840388774871826 | avg loss: 0.3867624614198329\n",
      "batch loss: 0.2633114755153656 | avg loss: 0.38470494498809177\n",
      "batch loss: 0.33344781398773193 | avg loss: 0.3838646641520203\n",
      "batch loss: 0.9860285520553589 | avg loss: 0.3935769849246548\n",
      "batch loss: 0.14647287130355835 | avg loss: 0.38965469740685965\n",
      "batch loss: 0.22737659513950348 | avg loss: 0.3871191020589322\n",
      "batch loss: 0.4973442554473877 | avg loss: 0.38881487364952383\n",
      "batch loss: 0.37619832158088684 | avg loss: 0.38862371376969596\n",
      "batch loss: 0.20277993381023407 | avg loss: 0.385849926009107\n",
      "batch loss: 0.4171123802661896 | avg loss: 0.3863096679834759\n",
      "batch loss: 0.2693473696708679 | avg loss: 0.38461456221082935\n",
      "batch loss: 0.33801528811454773 | avg loss: 0.38394885829516817\n",
      "batch loss: 0.42158427834510803 | avg loss: 0.38447893463389976\n",
      "batch loss: 0.23057430982589722 | avg loss: 0.38234137040045524\n",
      "batch loss: 0.2658933997154236 | avg loss: 0.3807461927198384\n",
      "batch loss: 0.2450130134820938 | avg loss: 0.378911960567977\n",
      "batch loss: 0.042480021715164185 | avg loss: 0.3744262013832728\n",
      "batch loss: 0.48937517404556274 | avg loss: 0.37593868786567136\n",
      "batch loss: 0.2500608265399933 | avg loss: 0.37430391044585737\n",
      "batch loss: 0.5646854639053345 | avg loss: 0.37674469959277374\n",
      "batch loss: 0.4497845470905304 | avg loss: 0.3776692546243909\n",
      "batch loss: 0.4863278567790985 | avg loss: 0.3790274871513247\n",
      "batch loss: 0.386055052280426 | avg loss: 0.379114247214647\n",
      "batch loss: 0.10193625092506409 | avg loss: 0.37573402774770087\n",
      "batch loss: 0.3902985751628876 | avg loss: 0.3759095042225826\n",
      "batch loss: 0.2665078639984131 | avg loss: 0.37460710374372347\n",
      "batch loss: 0.4258388876914978 | avg loss: 0.3752098306136973\n",
      "batch loss: 0.28930944204330444 | avg loss: 0.3742109888861346\n",
      "batch loss: 0.12875287234783173 | avg loss: 0.37138963122477475\n",
      "batch loss: 0.27046534419059753 | avg loss: 0.3702427643266591\n",
      "batch loss: 0.3343546986579895 | avg loss: 0.3698395276337527\n",
      "batch loss: 0.2962258458137512 | avg loss: 0.3690215978357527\n",
      "batch loss: 0.28557631373405457 | avg loss: 0.368104616691778\n",
      "batch loss: 0.5190038681030273 | avg loss: 0.3697448259462481\n",
      "batch loss: 0.4662494957447052 | avg loss: 0.37078251056773687\n",
      "batch loss: 0.27246329188346863 | avg loss: 0.36973656143279787\n",
      "batch loss: 0.2942062020301819 | avg loss: 0.36894150501803347\n",
      "batch loss: 0.40196746587753296 | avg loss: 0.3692855254436533\n",
      "batch loss: 0.21119415760040283 | avg loss: 0.36765571752774345\n",
      "batch loss: 0.7183433175086975 | avg loss: 0.37123416242550833\n",
      "batch loss: 0.6144711375236511 | avg loss: 0.3736911015679138\n",
      "batch loss: 0.25790518522262573 | avg loss: 0.3725332424044609\n",
      "batch loss: 0.2509244680404663 | avg loss: 0.37132919513353024\n",
      "batch loss: 0.22671401500701904 | avg loss: 0.36991139924993705\n",
      "batch loss: 0.23990881443023682 | avg loss: 0.36864923823227\n",
      "batch loss: 0.558996319770813 | avg loss: 0.3704794986316791\n",
      "batch loss: 0.3527691662311554 | avg loss: 0.37031082879929317\n",
      "batch loss: 0.20496095716953278 | avg loss: 0.36875092434995577\n",
      "batch loss: 0.29920220375061035 | avg loss: 0.36810093630697127\n",
      "batch loss: 0.2562994360923767 | avg loss: 0.3670657372309102\n",
      "batch loss: 0.3194132149219513 | avg loss: 0.3666285581271583\n",
      "batch loss: 0.12105655670166016 | avg loss: 0.36439608538692647\n",
      "batch loss: 0.5028597116470337 | avg loss: 0.36564350544332386\n",
      "batch loss: 0.3628097176551819 | avg loss: 0.365618203766644\n",
      "batch loss: 0.25749507546424866 | avg loss: 0.36466136192326\n",
      "batch loss: 0.7111730575561523 | avg loss: 0.36770093820074146\n",
      "batch loss: 0.11521449685096741 | avg loss: 0.36550540392813474\n",
      "batch loss: 0.21840226650238037 | avg loss: 0.36423727343308515\n",
      "batch loss: 0.6876636743545532 | avg loss: 0.36700160164608914\n",
      "batch loss: 0.2549867331981659 | avg loss: 0.3660523230999203\n",
      "batch loss: 0.1977696716785431 | avg loss: 0.3646381831720096\n",
      "batch loss: 0.3319965600967407 | avg loss: 0.36436616964638235\n",
      "batch loss: 0.22895710170269012 | avg loss: 0.3632470864402361\n",
      "batch loss: 0.3742627501487732 | avg loss: 0.3633373787657159\n",
      "batch loss: 0.37053415179252625 | avg loss: 0.3633958891155274\n",
      "batch loss: 0.33232131600379944 | avg loss: 0.3631452877194651\n",
      "batch loss: 0.25519293546676636 | avg loss: 0.3622816689014435\n",
      "batch loss: 0.18115104734897614 | avg loss: 0.3608441242859477\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:12:51\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:59\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "batch loss: 0.1737815886735916 | avg loss: 0.1737815886735916\n",
      "batch loss: 0.21975769102573395 | avg loss: 0.19676963984966278\n",
      "batch loss: 0.48022592067718506 | avg loss: 0.2912550667921702\n",
      "batch loss: 0.30794087052345276 | avg loss: 0.29542651772499084\n",
      "batch loss: 0.2162403166294098 | avg loss: 0.27958927750587464\n",
      "batch loss: 0.27149948477745056 | avg loss: 0.27824097871780396\n",
      "batch loss: 0.15869523584842682 | avg loss: 0.2611630154507501\n",
      "batch loss: 0.24003149569034576 | avg loss: 0.25852157548069954\n",
      "batch loss: 0.26652050018310547 | avg loss: 0.259410344892078\n",
      "batch loss: 0.16414733231067657 | avg loss: 0.24988404363393785\n",
      "batch loss: 0.18580904603004456 | avg loss: 0.24405904385176572\n",
      "batch loss: 0.19915133714675903 | avg loss: 0.24031673495968184\n",
      "batch loss: 0.19678813219070435 | avg loss: 0.2369683809005297\n",
      "batch loss: 0.15018297731876373 | avg loss: 0.23076942350183213\n",
      "batch loss: 0.25450995564460754 | avg loss: 0.23235212564468383\n",
      "batch loss: 0.15561369061470032 | avg loss: 0.22755597345530987\n",
      "batch loss: 0.20211754739284515 | avg loss: 0.22605959545163548\n",
      "batch loss: 0.22987543046474457 | avg loss: 0.2262715862856971\n",
      "batch loss: 0.23954254388809204 | avg loss: 0.22697005773845472\n",
      "batch loss: 0.11817322671413422 | avg loss: 0.2215302161872387\n",
      "batch loss: 0.5409670472145081 | avg loss: 0.2367414938552039\n",
      "batch loss: 0.31055697798728943 | avg loss: 0.24009674313393506\n",
      "batch loss: 0.115816131234169 | avg loss: 0.23469323826872784\n",
      "batch loss: 0.17303238809108734 | avg loss: 0.23212403617799282\n",
      "batch loss: 0.22279460728168488 | avg loss: 0.23175085902214052\n",
      "batch loss: 0.3252620995044708 | avg loss: 0.23534744519453782\n",
      "batch loss: 0.07558751106262207 | avg loss: 0.22943041059705946\n",
      "batch loss: 0.4299015700817108 | avg loss: 0.23659009486436844\n",
      "batch loss: 0.03159763664007187 | avg loss: 0.2295213894083582\n",
      "batch loss: 0.09567751735448837 | avg loss: 0.22505992700656255\n",
      "batch loss: 0.5153153538703918 | avg loss: 0.23442300529249252\n",
      "batch loss: 0.3098360598087311 | avg loss: 0.23677966324612498\n",
      "batch loss: 0.5482070446014404 | avg loss: 0.24621685662052847\n",
      "batch loss: 0.20271649956703186 | avg loss: 0.24493743435424917\n",
      "batch loss: 0.3889874219894409 | avg loss: 0.2490531482866832\n",
      "batch loss: 0.4844326972961426 | avg loss: 0.2555914690925015\n",
      "batch loss: 0.6194116473197937 | avg loss: 0.26542444688242833\n",
      "batch loss: 0.15311779081821442 | avg loss: 0.26246900856494904\n",
      "batch loss: 0.11936039477586746 | avg loss: 0.25879955692933154\n",
      "batch loss: 0.4059859812259674 | avg loss: 0.26247921753674747\n",
      "batch loss: 0.13227127492427826 | avg loss: 0.2593034140583945\n",
      "batch loss: 0.08875542879104614 | avg loss: 0.2552427477425053\n",
      "batch loss: 0.17992350459098816 | avg loss: 0.2534911374366561\n",
      "batch loss: 0.21375426650047302 | avg loss: 0.252588026733561\n",
      "batch loss: 0.23515063524246216 | avg loss: 0.2522005291448699\n",
      "batch loss: 0.11146967858076096 | avg loss: 0.24914116282825885\n",
      "batch loss: 0.13517449796199799 | avg loss: 0.24671634017152988\n",
      "batch loss: 0.03537706658244133 | avg loss: 0.2423134386384239\n",
      "batch loss: 0.04722214117646217 | avg loss: 0.23833198358817975\n",
      "batch loss: 0.13127315044403076 | avg loss: 0.2361908069252968\n",
      "batch loss: 0.43192505836486816 | avg loss: 0.2400287334241119\n",
      "batch loss: 0.04975418746471405 | avg loss: 0.23636960754027733\n",
      "batch loss: 0.052183959633111954 | avg loss: 0.23289440663636857\n",
      "batch loss: 0.049016863107681274 | avg loss: 0.22948926694139285\n",
      "batch loss: 0.27898678183555603 | avg loss: 0.23038922175765036\n",
      "batch loss: 0.27549490332603455 | avg loss: 0.23119468035708582\n",
      "batch loss: 0.05838920921087265 | avg loss: 0.2281630054246961\n",
      "batch loss: 0.1165887787938118 | avg loss: 0.22623931186209464\n",
      "batch loss: 0.24161753058433533 | avg loss: 0.2264999596370479\n",
      "batch loss: 0.5217034220695496 | avg loss: 0.23142001734425624\n",
      "batch loss: 0.549159049987793 | avg loss: 0.23662885394496996\n",
      "batch loss: 0.37836796045303345 | avg loss: 0.23891496856606775\n",
      "batch loss: 0.21744027733802795 | avg loss: 0.23857410045133698\n",
      "batch loss: 0.23793524503707886 | avg loss: 0.23856411833548918\n",
      "batch loss: 0.4669681787490845 | avg loss: 0.2420780269572368\n",
      "batch loss: 0.3066142201423645 | avg loss: 0.24305584806610236\n",
      "batch loss: 0.24860119819641113 | avg loss: 0.24313861448595772\n",
      "batch loss: 0.48485255241394043 | avg loss: 0.24669323122019277\n",
      "batch loss: 0.3393591046333313 | avg loss: 0.24803621489284694\n",
      "batch loss: 0.5078267455101013 | avg loss: 0.2517475081873792\n",
      "batch loss: 0.18450291454792023 | avg loss: 0.2508004012347107\n",
      "batch loss: 0.10566966235637665 | avg loss: 0.2487846965280672\n",
      "batch loss: 0.16629426181316376 | avg loss: 0.24765469057306852\n",
      "batch loss: 0.03378000855445862 | avg loss: 0.2447644921674116\n",
      "batch loss: 0.14367465674877167 | avg loss: 0.2434166276951631\n",
      "batch loss: 0.6392212510108948 | avg loss: 0.24862458326510692\n",
      "batch loss: 0.6936651468276978 | avg loss: 0.2544043308438419\n",
      "batch loss: 0.5488616824150085 | avg loss: 0.2581794250947543\n",
      "batch loss: 0.5008370280265808 | avg loss: 0.2612510403217394\n",
      "batch loss: 0.257832407951355 | avg loss: 0.2612083074171096\n",
      "batch loss: 0.19553156197071075 | avg loss: 0.26039748339925284\n",
      "batch loss: 0.10659598559141159 | avg loss: 0.258521855377206\n",
      "batch loss: 0.22043512761592865 | avg loss: 0.2580629791391183\n",
      "batch loss: 0.08257199823856354 | avg loss: 0.2559738007950641\n",
      "batch loss: 0.4112715423107147 | avg loss: 0.2578008330481894\n",
      "batch loss: 0.1403769701719284 | avg loss: 0.25643543929381424\n",
      "batch loss: 0.043527040630578995 | avg loss: 0.25398821632067364\n",
      "batch loss: 0.2045125663280487 | avg loss: 0.2534259930253029\n",
      "batch loss: 0.24722279608249664 | avg loss: 0.2533562941832489\n",
      "batch loss: 0.19200947880744934 | avg loss: 0.25267466290129553\n",
      "batch loss: 0.2911776602268219 | avg loss: 0.2530977727620156\n",
      "batch loss: 0.1934826374053955 | avg loss: 0.2524497821603132\n",
      "batch loss: 0.339572548866272 | avg loss: 0.25338658610338805\n",
      "batch loss: 0.31298360228538513 | avg loss: 0.25402059691383483\n",
      "batch loss: 0.22335654497146606 | avg loss: 0.25369781741970465\n",
      "batch loss: 0.1873832494020462 | avg loss: 0.2530070406695207\n",
      "batch loss: 0.12836433947086334 | avg loss: 0.2517220643685036\n",
      "batch loss: 0.4348648488521576 | avg loss: 0.2535908682918062\n",
      "batch loss: 0.3477075397968292 | avg loss: 0.2545415417413519\n",
      "batch loss: 0.3065255284309387 | avg loss: 0.25506138160824776\n",
      "batch loss: 0.11222576349973679 | avg loss: 0.25364716756756944\n",
      "batch loss: 0.09206140786409378 | avg loss: 0.25206299345282945\n",
      "batch loss: 0.07597044110298157 | avg loss: 0.2503533570222484\n",
      "batch loss: 0.18844471871852875 | avg loss: 0.24975808165394342\n",
      "batch loss: 0.16787424683570862 | avg loss: 0.2489782356080555\n",
      "batch loss: 0.08876807987689972 | avg loss: 0.247466819044554\n",
      "batch loss: 0.15184256434440613 | avg loss: 0.24657313442118814\n",
      "batch loss: 0.3527623414993286 | avg loss: 0.24755636782005982\n",
      "batch loss: 0.5743398666381836 | avg loss: 0.2505543815706848\n",
      "batch loss: 0.05076708272099495 | avg loss: 0.248738133399324\n",
      "batch loss: 0.5196400284767151 | avg loss: 0.2511786910126338\n",
      "batch loss: 0.12830767035484314 | avg loss: 0.25008162832818925\n",
      "batch loss: 0.2256382256746292 | avg loss: 0.24986531503037013\n",
      "batch loss: 0.29607489705085754 | avg loss: 0.2502706622410762\n",
      "batch loss: 0.05151638388633728 | avg loss: 0.24854236416842626\n",
      "batch loss: 0.06552745401859283 | avg loss: 0.2469646494257553\n",
      "batch loss: 0.4427328109741211 | avg loss: 0.24863788157574132\n",
      "batch loss: 0.1729263961315155 | avg loss: 0.2479962588177394\n",
      "batch loss: 0.26607370376586914 | avg loss: 0.24814817011982454\n",
      "batch loss: 0.2969079613685608 | avg loss: 0.248554501713564\n",
      "batch loss: 0.032769106328487396 | avg loss: 0.2467711513384807\n",
      "batch loss: 0.16429898142814636 | avg loss: 0.24609514994577306\n",
      "batch loss: 0.26088929176330566 | avg loss: 0.24621542752152537\n",
      "batch loss: 0.2946538031101227 | avg loss: 0.24660605958272372\n",
      "batch loss: 0.1651410311460495 | avg loss: 0.24595433935523034\n",
      "batch loss: 0.31804361939430237 | avg loss: 0.24652647649839757\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:12:43\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.66\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "batch loss: 0.13293728232383728 | avg loss: 0.13293728232383728\n",
      "batch loss: 0.2740744352340698 | avg loss: 0.20350585877895355\n",
      "batch loss: 0.3963997960090637 | avg loss: 0.2678038378556569\n",
      "batch loss: 0.2548926770687103 | avg loss: 0.2645760476589203\n",
      "batch loss: 0.1645994633436203 | avg loss: 0.2445807307958603\n",
      "batch loss: 0.20461037755012512 | avg loss: 0.23791900525490442\n",
      "batch loss: 0.03481469675898552 | avg loss: 0.20890410404120172\n",
      "batch loss: 0.10795353353023529 | avg loss: 0.19628528272733092\n",
      "batch loss: 0.3575800061225891 | avg loss: 0.21420691866013739\n",
      "batch loss: 0.1428835541009903 | avg loss: 0.20707458220422267\n",
      "batch loss: 0.032486479729413986 | avg loss: 0.1912029365246946\n",
      "batch loss: 0.0802423357963562 | avg loss: 0.1819562197973331\n",
      "batch loss: 0.055941808968782425 | avg loss: 0.17226280357975227\n",
      "batch loss: 0.05024677887558937 | avg loss: 0.16354737324374063\n",
      "batch loss: 0.103867307305336 | avg loss: 0.15956870218118033\n",
      "batch loss: 0.07865555584430695 | avg loss: 0.15451163053512573\n",
      "batch loss: 0.06802280247211456 | avg loss: 0.14942405241377213\n",
      "batch loss: 0.2566731870174408 | avg loss: 0.1553823376695315\n",
      "batch loss: 0.07159559428691864 | avg loss: 0.15097250907044663\n",
      "batch loss: 0.058629754930734634 | avg loss: 0.14635537136346102\n",
      "batch loss: 0.22272633016109467 | avg loss: 0.14999208368715786\n",
      "batch loss: 0.1527186781167984 | avg loss: 0.15011601979759606\n",
      "batch loss: 0.028932958841323853 | avg loss: 0.14484719106036684\n",
      "batch loss: 0.17722387611865997 | avg loss: 0.14619621960446239\n",
      "batch loss: 0.14047282934188843 | avg loss: 0.14596728399395942\n",
      "batch loss: 0.27252858877182007 | avg loss: 0.1508350264854156\n",
      "batch loss: 0.009546170011162758 | avg loss: 0.1456021058752581\n",
      "batch loss: 0.7531554698944092 | avg loss: 0.1673004403045135\n",
      "batch loss: 0.016393566504120827 | avg loss: 0.16209675500105167\n",
      "batch loss: 0.2781367003917694 | avg loss: 0.16596475318074227\n",
      "batch loss: 0.21639485657215118 | avg loss: 0.1675915307094974\n",
      "batch loss: 0.15862919390201569 | avg loss: 0.1673114576842636\n",
      "batch loss: 0.5112063884735107 | avg loss: 0.17773251619302866\n",
      "batch loss: 0.24086661636829376 | avg loss: 0.17958940149230115\n",
      "batch loss: 0.4711575508117676 | avg loss: 0.1879199200442859\n",
      "batch loss: 0.3847983479499817 | avg loss: 0.19338876526388857\n",
      "batch loss: 0.5735511779785156 | avg loss: 0.2036634250669866\n",
      "batch loss: 0.21289309859275818 | avg loss: 0.20390631121240163\n",
      "batch loss: 0.03060119040310383 | avg loss: 0.1994625901660094\n",
      "batch loss: 0.5920626521110535 | avg loss: 0.2092775917146355\n",
      "batch loss: 0.13917213678359985 | avg loss: 0.2075677025699761\n",
      "batch loss: 0.3288542330265045 | avg loss: 0.21045547710465534\n",
      "batch loss: 0.07861161231994629 | avg loss: 0.20738934071431325\n",
      "batch loss: 0.055204737931489944 | avg loss: 0.20393059974197636\n",
      "batch loss: 0.11766946315765381 | avg loss: 0.20201368559565808\n",
      "batch loss: 0.22977404296398163 | avg loss: 0.20261717162540424\n",
      "batch loss: 0.2041931450366974 | avg loss: 0.2026507029745807\n",
      "batch loss: 0.022095799446105957 | avg loss: 0.19888914248440415\n",
      "batch loss: 0.08666499704122543 | avg loss: 0.1965988538018903\n",
      "batch loss: 0.054487038403749466 | avg loss: 0.19375661749392747\n",
      "batch loss: 0.16159334778785706 | avg loss: 0.19312596514674962\n",
      "batch loss: 0.009713640436530113 | avg loss: 0.1895988050561685\n",
      "batch loss: 0.011166435666382313 | avg loss: 0.18623215657711592\n",
      "batch loss: 0.023831838741898537 | avg loss: 0.18322474328387114\n",
      "batch loss: 0.3190169632434845 | avg loss: 0.1856936927376823\n",
      "batch loss: 0.17026552557945251 | avg loss: 0.1854181897527139\n",
      "batch loss: 0.08700936287641525 | avg loss: 0.1836917191057613\n",
      "batch loss: 0.40690430998802185 | avg loss: 0.18754021205200716\n",
      "batch loss: 0.07209214568138123 | avg loss: 0.18558346516436944\n",
      "batch loss: 0.21102634072303772 | avg loss: 0.18600751309034724\n",
      "batch loss: 0.1943838894367218 | avg loss: 0.18614483073536978\n",
      "batch loss: 0.31849926710128784 | avg loss: 0.18827957970901363\n",
      "batch loss: 0.09815937280654907 | avg loss: 0.18684910023437132\n",
      "batch loss: 0.08055821806192398 | avg loss: 0.18518830520042684\n",
      "batch loss: 0.37473517656326294 | avg loss: 0.18810441091370125\n",
      "batch loss: 0.07946033030748367 | avg loss: 0.1864582884802737\n",
      "batch loss: 0.1816716343164444 | avg loss: 0.18638684588081356\n",
      "batch loss: 0.20486341416835785 | avg loss: 0.18665856012033627\n",
      "batch loss: 0.01124128419905901 | avg loss: 0.18411628075915834\n",
      "batch loss: 0.25602594017982483 | avg loss: 0.185143561608025\n",
      "batch loss: 0.18018700182437897 | avg loss: 0.18507375090684688\n",
      "batch loss: 0.03346985951066017 | avg loss: 0.1829681413041221\n",
      "batch loss: 0.13831675052642822 | avg loss: 0.1823564784167564\n",
      "batch loss: 0.02914278581738472 | avg loss: 0.1802860231113595\n",
      "batch loss: 0.017550712451338768 | avg loss: 0.1781162189692259\n",
      "batch loss: 0.46068307757377625 | avg loss: 0.1818342039508647\n",
      "batch loss: 0.2418327033519745 | avg loss: 0.1826134052417882\n",
      "batch loss: 0.40891939401626587 | avg loss: 0.18551476407223022\n",
      "batch loss: 0.3516017198562622 | avg loss: 0.18761713060114202\n",
      "batch loss: 0.14534510672092438 | avg loss: 0.1870887303026393\n",
      "batch loss: 0.07369783520698547 | avg loss: 0.1856888427088658\n",
      "batch loss: 0.09776749461889267 | avg loss: 0.18461663114679297\n",
      "batch loss: 0.09165419638156891 | avg loss: 0.18349660181227218\n",
      "batch loss: 0.02863495424389839 | avg loss: 0.18165301076979154\n",
      "batch loss: 0.43236422538757324 | avg loss: 0.1846025544711772\n",
      "batch loss: 0.09586400538682938 | avg loss: 0.18357071087717317\n",
      "batch loss: 0.011099319905042648 | avg loss: 0.18158828109588432\n",
      "batch loss: 0.14748159050941467 | avg loss: 0.1812007050664926\n",
      "batch loss: 0.06712253391742706 | avg loss: 0.17991892786257052\n",
      "batch loss: 0.08512582629919052 | avg loss: 0.17886567117853297\n",
      "batch loss: 0.1158384382724762 | avg loss: 0.17817306422352136\n",
      "batch loss: 0.14424891769886017 | avg loss: 0.1778043235004272\n",
      "batch loss: 0.15242676436901093 | avg loss: 0.17753144652051953\n",
      "batch loss: 0.1607987880706787 | avg loss: 0.17735343951573398\n",
      "batch loss: 0.010789341293275356 | avg loss: 0.17560013321865547\n",
      "batch loss: 0.1505405753850937 | avg loss: 0.1753390961578892\n",
      "batch loss: 0.0907348245382309 | avg loss: 0.17446688717211953\n",
      "batch loss: 0.25263404846191406 | avg loss: 0.17526451126691334\n",
      "batch loss: 0.21424998342990875 | avg loss: 0.1756583039150244\n",
      "batch loss: 0.05048187077045441 | avg loss: 0.1744065395835787\n",
      "batch loss: 0.010717385448515415 | avg loss: 0.17278585488917214\n",
      "batch loss: 0.03701254725456238 | avg loss: 0.1714547440300093\n",
      "batch loss: 0.0937972366809845 | avg loss: 0.17070078764797994\n",
      "batch loss: 0.3182564377784729 | avg loss: 0.17211959197615775\n",
      "batch loss: 0.19633163511753082 | avg loss: 0.17235018286321843\n",
      "batch loss: 0.029180927202105522 | avg loss: 0.17099952950792494\n",
      "batch loss: 0.03474267199635506 | avg loss: 0.16972610093305043\n",
      "batch loss: 0.05089959502220154 | avg loss: 0.16862585550794998\n",
      "batch loss: 0.5133039355278015 | avg loss: 0.1717880397283156\n",
      "batch loss: 0.4255639314651489 | avg loss: 0.17409509328955955\n",
      "batch loss: 0.8170761466026306 | avg loss: 0.1798877153914791\n",
      "batch loss: 0.6982836127281189 | avg loss: 0.18451625018912768\n",
      "batch loss: 0.59394770860672 | avg loss: 0.18813953743176123\n",
      "batch loss: 0.7810428142547607 | avg loss: 0.19334044336880507\n",
      "batch loss: 0.36848342418670654 | avg loss: 0.19486342581069988\n",
      "batch loss: 0.08162631839513779 | avg loss: 0.19388724385022088\n",
      "batch loss: 0.4217105805873871 | avg loss: 0.19583445185652146\n",
      "batch loss: 0.10528832674026489 | avg loss: 0.19506711181316336\n",
      "batch loss: 0.21898433566093445 | avg loss: 0.19526809688751437\n",
      "batch loss: 0.4213448464870453 | avg loss: 0.1971520698008438\n",
      "batch loss: 0.32725924253463745 | avg loss: 0.19822733569120574\n",
      "batch loss: 0.36469706892967224 | avg loss: 0.1995918417013571\n",
      "batch loss: 0.48752570152282715 | avg loss: 0.20193276739096253\n",
      "batch loss: 0.2608860433101654 | avg loss: 0.20240819703547225\n",
      "batch loss: 0.23447991907596588 | avg loss: 0.20266477081179618\n",
      "batch loss: 0.025862904265522957 | avg loss: 0.20126158139476227\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:12:34\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "batch loss: 0.07365673780441284 | avg loss: 0.07365673780441284\n",
      "batch loss: 0.08612874895334244 | avg loss: 0.07989274337887764\n",
      "batch loss: 0.1902993619441986 | avg loss: 0.11669494956731796\n",
      "batch loss: 0.16020533442497253 | avg loss: 0.1275725457817316\n",
      "batch loss: 0.06013970077037811 | avg loss: 0.1140859767794609\n",
      "batch loss: 0.16725358366966248 | avg loss: 0.1229472445944945\n",
      "batch loss: 0.01667727530002594 | avg loss: 0.10776582040957042\n",
      "batch loss: 0.2208118736743927 | avg loss: 0.1218965770676732\n",
      "batch loss: 0.09756362438201904 | avg loss: 0.11919291565815608\n",
      "batch loss: 0.033018384128808975 | avg loss: 0.11057546250522136\n",
      "batch loss: 0.01977502927184105 | avg loss: 0.10232087766582315\n",
      "batch loss: 0.01994815655052662 | avg loss: 0.09545648423954844\n",
      "batch loss: 0.03355908766388893 | avg loss: 0.09069514604142079\n",
      "batch loss: 0.036118749529123306 | avg loss: 0.08679683200482811\n",
      "batch loss: 0.0330234169960022 | avg loss: 0.08321193767090639\n",
      "batch loss: 0.03774275630712509 | avg loss: 0.08037011383567005\n",
      "batch loss: 0.019321342930197716 | avg loss: 0.07677900966475992\n",
      "batch loss: 0.03216749429702759 | avg loss: 0.07430059214433034\n",
      "batch loss: 0.043969038873910904 | avg loss: 0.07270419460378195\n",
      "batch loss: 0.009004157036542892 | avg loss: 0.06951919272542\n",
      "batch loss: 0.17512449622154236 | avg loss: 0.07454801670142583\n",
      "batch loss: 0.18987052142620087 | avg loss: 0.07978994873437015\n",
      "batch loss: 0.034534018486738205 | avg loss: 0.07782229959316876\n",
      "batch loss: 0.02888433076441288 | avg loss: 0.07578321755863726\n",
      "batch loss: 0.09008011966943741 | avg loss: 0.07635509364306926\n",
      "batch loss: 0.4258984327316284 | avg loss: 0.08979906822339846\n",
      "batch loss: 0.002719100331887603 | avg loss: 0.08657388422741658\n",
      "batch loss: 0.36318787932395935 | avg loss: 0.09645295548086454\n",
      "batch loss: 0.018896857276558876 | avg loss: 0.09377860726692297\n",
      "batch loss: 0.007288952823728323 | avg loss: 0.09089561878548315\n",
      "batch loss: 0.18788501620292664 | avg loss: 0.09402430902475552\n",
      "batch loss: 0.03566323220729828 | avg loss: 0.09220052537420997\n",
      "batch loss: 0.40967032313346863 | avg loss: 0.1018208222760057\n",
      "batch loss: 0.4014376997947693 | avg loss: 0.11063308337949873\n",
      "batch loss: 0.2523370087146759 | avg loss: 0.1146817669605038\n",
      "batch loss: 0.5587933659553528 | avg loss: 0.12701820026591626\n",
      "batch loss: 0.029536399990320206 | avg loss: 0.12438355701522448\n",
      "batch loss: 0.2495931088924408 | avg loss: 0.12767854522251965\n",
      "batch loss: 0.1114288717508316 | avg loss: 0.1272618869283738\n",
      "batch loss: 0.24907954037189484 | avg loss: 0.13030732826446184\n",
      "batch loss: 0.005938177462667227 | avg loss: 0.12727393434246684\n",
      "batch loss: 0.3803057074546814 | avg loss: 0.13329850036894814\n",
      "batch loss: 0.011428519152104855 | avg loss: 0.1304643147592541\n",
      "batch loss: 0.2773864269256592 | avg loss: 0.13380345367212695\n",
      "batch loss: 0.017477573826909065 | avg loss: 0.131218434120011\n",
      "batch loss: 0.19407051801681519 | avg loss: 0.1325847837699415\n",
      "batch loss: 0.0045447759330272675 | avg loss: 0.12986052828404973\n",
      "batch loss: 0.004115563817322254 | avg loss: 0.12724084152432624\n",
      "batch loss: 0.03296400234103203 | avg loss: 0.12531682439813657\n",
      "batch loss: 0.041834089905023575 | avg loss: 0.1236471697082743\n",
      "batch loss: 0.23114405572414398 | avg loss: 0.12575495178701684\n",
      "batch loss: 0.006483545061200857 | avg loss: 0.12346127088844347\n",
      "batch loss: 0.005763313267379999 | avg loss: 0.12124055470691396\n",
      "batch loss: 0.16377848386764526 | avg loss: 0.1220282941358164\n",
      "batch loss: 0.37651875615119934 | avg loss: 0.126655393445187\n",
      "batch loss: 0.3559344708919525 | avg loss: 0.1307496626853078\n",
      "batch loss: 0.002738210139796138 | avg loss: 0.128503847728369\n",
      "batch loss: 0.013860221952199936 | avg loss: 0.12652723349084885\n",
      "batch loss: 0.008201244287192822 | avg loss: 0.12452170825010891\n",
      "batch loss: 0.058129385113716125 | avg loss: 0.12341516953116903\n",
      "batch loss: 0.004530179314315319 | avg loss: 0.12146623526531898\n",
      "batch loss: 0.38026219606399536 | avg loss: 0.12564036366529763\n",
      "batch loss: 0.0037525745574384928 | avg loss: 0.12370563685406177\n",
      "batch loss: 0.06727830320596695 | avg loss: 0.12282395976581029\n",
      "batch loss: 0.12441666424274445 | avg loss: 0.12284846291160928\n",
      "batch loss: 0.019235558807849884 | avg loss: 0.12127857042518868\n",
      "batch loss: 0.18352067470550537 | avg loss: 0.12220755705623818\n",
      "batch loss: 0.18758998811244965 | avg loss: 0.12316906339530011\n",
      "batch loss: 0.011808270588517189 | avg loss: 0.12155513886186847\n",
      "batch loss: 0.1952500343322754 | avg loss: 0.12260792308287428\n",
      "batch loss: 0.0940866619348526 | avg loss: 0.12220621517938103\n",
      "batch loss: 0.017988866195082664 | avg loss: 0.12075875199904355\n",
      "batch loss: 0.09538421779870987 | avg loss: 0.12041115564013487\n",
      "batch loss: 0.02429257147014141 | avg loss: 0.11911225585405387\n",
      "batch loss: 0.0030023236759006977 | avg loss: 0.11756412342501184\n",
      "batch loss: 0.11392092704772949 | avg loss: 0.11751618663057391\n",
      "batch loss: 0.36334991455078125 | avg loss: 0.12070883244771946\n",
      "batch loss: 0.4336386024951935 | avg loss: 0.12472075257653323\n",
      "batch loss: 0.1267843246459961 | avg loss: 0.12474687374196947\n",
      "batch loss: 0.14465686678886414 | avg loss: 0.12499574865505565\n",
      "batch loss: 0.011786608025431633 | avg loss: 0.12359810494357881\n",
      "batch loss: 0.09986709803342819 | avg loss: 0.12330870242028429\n",
      "batch loss: 0.020394477993249893 | avg loss: 0.12206877200550074\n",
      "batch loss: 0.04768582805991173 | avg loss: 0.12118326076805326\n",
      "batch loss: 0.5175023078918457 | avg loss: 0.12584583779303904\n",
      "batch loss: 0.12685498595237732 | avg loss: 0.1258575720739616\n",
      "batch loss: 0.1657673716545105 | avg loss: 0.12631630540247366\n",
      "batch loss: 0.07468292117118835 | avg loss: 0.1257295623998454\n",
      "batch loss: 0.015681829303503036 | avg loss: 0.12449307101674043\n",
      "batch loss: 0.13040392100811005 | avg loss: 0.12455874712775565\n",
      "batch loss: 0.07978235185146332 | avg loss: 0.12406669882801617\n",
      "batch loss: 0.04602722451090813 | avg loss: 0.12321844367239543\n",
      "batch loss: 0.35928457975387573 | avg loss: 0.12575678922165867\n",
      "batch loss: 0.3519466519355774 | avg loss: 0.1281630643569131\n",
      "batch loss: 0.1649894416332245 | avg loss: 0.12855071043350585\n",
      "batch loss: 0.06789251416921616 | avg loss: 0.12791885422241953\n",
      "batch loss: 0.06993162631988525 | avg loss: 0.1273210477491975\n",
      "batch loss: 0.19979791343212128 | avg loss: 0.1280606076031049\n",
      "batch loss: 0.10748456418514252 | avg loss: 0.12785276878070123\n",
      "batch loss: 0.14835967123508453 | avg loss: 0.12805783780524507\n",
      "batch loss: 0.008430255576968193 | avg loss: 0.1268734062980344\n",
      "batch loss: 0.03403196856379509 | avg loss: 0.1259631961241693\n",
      "batch loss: 0.0846887156367302 | avg loss: 0.12556247301264079\n",
      "batch loss: 0.21504566073417664 | avg loss: 0.126422888279194\n",
      "batch loss: 0.21343763172626495 | avg loss: 0.12725160012154707\n",
      "batch loss: 0.2342391163110733 | avg loss: 0.1282609163120143\n",
      "batch loss: 0.1026456356048584 | avg loss: 0.12802152116521845\n",
      "batch loss: 0.15572357177734375 | avg loss: 0.12827802163384924\n",
      "batch loss: 0.24983511865139008 | avg loss: 0.1293932243587808\n",
      "batch loss: 0.023139365017414093 | avg loss: 0.1284272801829502\n",
      "batch loss: 0.3538195490837097 | avg loss: 0.13045784116403814\n",
      "batch loss: 0.2780924141407013 | avg loss: 0.1317760069941869\n",
      "batch loss: 0.1536998152732849 | avg loss: 0.1319700229966568\n",
      "batch loss: 0.4001530408859253 | avg loss: 0.13432250560972056\n",
      "batch loss: 0.10477941483259201 | avg loss: 0.1340656091681803\n",
      "batch loss: 0.09015475958585739 | avg loss: 0.13368706736143615\n",
      "batch loss: 0.3840852677822113 | avg loss: 0.13582722292058808\n",
      "batch loss: 0.030923139303922653 | avg loss: 0.13493820526281972\n",
      "batch loss: 0.10256895422935486 | avg loss: 0.13466619474993347\n",
      "batch loss: 0.21657529473304749 | avg loss: 0.13534877058312608\n",
      "batch loss: 0.009304611943662167 | avg loss: 0.1343070833216429\n",
      "batch loss: 0.1351911425590515 | avg loss: 0.13431432970883478\n",
      "batch loss: 0.08907807618379593 | avg loss: 0.13394655528993202\n",
      "batch loss: 0.08312651515007019 | avg loss: 0.13353671625654603\n",
      "batch loss: 0.08723294734954834 | avg loss: 0.13316628610529005\n",
      "batch loss: 0.014842674136161804 | avg loss: 0.13222720981982078\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:12:41\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:57\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "batch loss: 0.01617070473730564 | avg loss: 0.01617070473730564\n",
      "batch loss: 0.11269134283065796 | avg loss: 0.0644310237839818\n",
      "batch loss: 0.2930620312690735 | avg loss: 0.1406413596123457\n",
      "batch loss: 0.03046073578298092 | avg loss: 0.1130962036550045\n",
      "batch loss: 0.017337260767817497 | avg loss: 0.0939444150775671\n",
      "batch loss: 0.22182147204875946 | avg loss: 0.11525725790609916\n",
      "batch loss: 0.048898275941610336 | avg loss: 0.10577740333974361\n",
      "batch loss: 0.1816822737455368 | avg loss: 0.11526551214046776\n",
      "batch loss: 0.012697603553533554 | avg loss: 0.10386907785303062\n",
      "batch loss: 0.034434594213962555 | avg loss: 0.09692562948912382\n",
      "batch loss: 0.01089742872864008 | avg loss: 0.08910488396544348\n",
      "batch loss: 0.0160727147012949 | avg loss: 0.08301886986009777\n",
      "batch loss: 0.01334116980433464 | avg loss: 0.07765904677888522\n",
      "batch loss: 0.006575512699782848 | avg loss: 0.07258165148752076\n",
      "batch loss: 0.03540907800197601 | avg loss: 0.07010347992181779\n",
      "batch loss: 0.013339423574507236 | avg loss: 0.06655572640011087\n",
      "batch loss: 0.02046177163720131 | avg loss: 0.06384431729641031\n",
      "batch loss: 0.019299911335110664 | avg loss: 0.061369628076338105\n",
      "batch loss: 0.009391101077198982 | avg loss: 0.058633916129014994\n",
      "batch loss: 0.01169136818498373 | avg loss: 0.056286788731813434\n",
      "batch loss: 0.014603056944906712 | avg loss: 0.05430184912291311\n",
      "batch loss: 0.04693346098065376 | avg loss: 0.05396692238917405\n",
      "batch loss: 0.004212601110339165 | avg loss: 0.051803691029224705\n",
      "batch loss: 0.012643640860915184 | avg loss: 0.05017202227221181\n",
      "batch loss: 0.014482660219073296 | avg loss: 0.04874444779008627\n",
      "batch loss: 0.3065662682056427 | avg loss: 0.05866067165222306\n",
      "batch loss: 0.0024039028212428093 | avg loss: 0.05657708762144601\n",
      "batch loss: 0.2661384642124176 | avg loss: 0.064061422499695\n",
      "batch loss: 0.002264777896925807 | avg loss: 0.06193050372028916\n",
      "batch loss: 0.1987132877111435 | avg loss: 0.06648992985331764\n",
      "batch loss: 0.1545226275920868 | avg loss: 0.06932969429650374\n",
      "batch loss: 0.15598155558109283 | avg loss: 0.07203756496164715\n",
      "batch loss: 0.21922795474529266 | avg loss: 0.07649787980357581\n",
      "batch loss: 0.0638406053185463 | avg loss: 0.07612560702460434\n",
      "batch loss: 0.056683290749788284 | avg loss: 0.07557011227389532\n",
      "batch loss: 0.5678911209106445 | avg loss: 0.08924569584713835\n",
      "batch loss: 0.14399468898773193 | avg loss: 0.09072539836445169\n",
      "batch loss: 0.07196138054132462 | avg loss: 0.09023160842173782\n",
      "batch loss: 0.11117143929004669 | avg loss: 0.09076852716195087\n",
      "batch loss: 0.20532026886940002 | avg loss: 0.09363232070463709\n",
      "batch loss: 0.02240128256380558 | avg loss: 0.09189497831095828\n",
      "batch loss: 0.14050771296024323 | avg loss: 0.09305242437403649\n",
      "batch loss: 0.1321004331111908 | avg loss: 0.09396051760048194\n",
      "batch loss: 0.1911999136209488 | avg loss: 0.09617050387367437\n",
      "batch loss: 0.037062156945466995 | avg loss: 0.09485698505304754\n",
      "batch loss: 0.25003936886787415 | avg loss: 0.09823051513597855\n",
      "batch loss: 0.008452610112726688 | avg loss: 0.09632034694399447\n",
      "batch loss: 0.0027746742125600576 | avg loss: 0.09437147876208958\n",
      "batch loss: 0.02314526028931141 | avg loss: 0.09291788246672676\n",
      "batch loss: 0.012331505306065083 | avg loss: 0.09130615492351353\n",
      "batch loss: 0.36790135502815247 | avg loss: 0.09672959021968293\n",
      "batch loss: 0.005822823382914066 | avg loss: 0.09498138316512968\n",
      "batch loss: 0.0029802618082612753 | avg loss: 0.09324551295084914\n",
      "batch loss: 0.005218369420617819 | avg loss: 0.09161538066325227\n",
      "batch loss: 0.19656722247600555 | avg loss: 0.09352359596893868\n",
      "batch loss: 0.08943600952625275 | avg loss: 0.09345060335389073\n",
      "batch loss: 0.0232698954641819 | avg loss: 0.09221936286459759\n",
      "batch loss: 0.21473388373851776 | avg loss: 0.09433168219001001\n",
      "batch loss: 0.020277727395296097 | avg loss: 0.0930765304138284\n",
      "batch loss: 0.19430384039878845 | avg loss: 0.09476365224691108\n",
      "batch loss: 0.00875597819685936 | avg loss: 0.0933536903772381\n",
      "batch loss: 0.29326531291007996 | avg loss: 0.09657807138583233\n",
      "batch loss: 0.008501959033310413 | avg loss: 0.09518003785642722\n",
      "batch loss: 0.012246758677065372 | avg loss: 0.09388420536924968\n",
      "batch loss: 0.17030656337738037 | avg loss: 0.09505993395399015\n",
      "batch loss: 0.014215339906513691 | avg loss: 0.09383501586236173\n",
      "batch loss: 0.09869061410427094 | avg loss: 0.09390748747791261\n",
      "batch loss: 0.007583059370517731 | avg loss: 0.09263801059398033\n",
      "batch loss: 0.012659091502428055 | avg loss: 0.09147889582453754\n",
      "batch loss: 0.09686898440122604 | avg loss: 0.0915558970899188\n",
      "batch loss: 0.0460527203977108 | avg loss: 0.0909150072773525\n",
      "batch loss: 0.08188130706548691 | avg loss: 0.09078953921885437\n",
      "batch loss: 0.0195695199072361 | avg loss: 0.08981392251595549\n",
      "batch loss: 0.004479822237044573 | avg loss: 0.0886607589986729\n",
      "batch loss: 0.00271739368326962 | avg loss: 0.0875148474611342\n",
      "batch loss: 0.08276879042387009 | avg loss: 0.08745239934222282\n",
      "batch loss: 0.20171155035495758 | avg loss: 0.08893628442031029\n",
      "batch loss: 0.3392789363861084 | avg loss: 0.09214580559935899\n",
      "batch loss: 0.16287769377231598 | avg loss: 0.09304114595597869\n",
      "batch loss: 0.18715809285640717 | avg loss: 0.09421760779223405\n",
      "batch loss: 0.14956669509410858 | avg loss: 0.09490092985768929\n",
      "batch loss: 0.004957988392561674 | avg loss: 0.09380406471787066\n",
      "batch loss: 0.007408624514937401 | avg loss: 0.09276315579976303\n",
      "batch loss: 0.0037561485078185797 | avg loss: 0.09170354857009702\n",
      "batch loss: 0.1799713522195816 | avg loss: 0.09274199331891449\n",
      "batch loss: 0.09378480166196823 | avg loss: 0.0927541189973221\n",
      "batch loss: 0.0646434873342514 | avg loss: 0.09243100828855116\n",
      "batch loss: 0.16098293662071228 | avg loss: 0.0932100074741439\n",
      "batch loss: 0.027634594589471817 | avg loss: 0.09247320508218129\n",
      "batch loss: 0.02889290824532509 | avg loss: 0.09176675733954956\n",
      "batch loss: 0.11439528316259384 | avg loss: 0.09201542245848411\n",
      "batch loss: 0.07467585802078247 | avg loss: 0.09182694893198735\n",
      "batch loss: 0.09944628179073334 | avg loss: 0.09190887724229645\n",
      "batch loss: 0.13927586376667023 | avg loss: 0.09241278135425787\n",
      "batch loss: 0.15685820579528809 | avg loss: 0.09309115424311083\n",
      "batch loss: 0.005314720328897238 | avg loss: 0.09217681638983777\n",
      "batch loss: 0.1527736485004425 | avg loss: 0.09280152599922545\n",
      "batch loss: 0.29061245918273926 | avg loss: 0.0948200049092613\n",
      "batch loss: 0.26725804805755615 | avg loss: 0.09656180332490065\n",
      "batch loss: 0.29933685064315796 | avg loss: 0.09858955379808322\n",
      "batch loss: 0.010023526847362518 | avg loss: 0.09771266244213549\n",
      "batch loss: 0.004386621527373791 | avg loss: 0.09679770125669665\n",
      "batch loss: 0.007132058963179588 | avg loss: 0.09592716104025473\n",
      "batch loss: 0.01722722314298153 | avg loss: 0.09517043086816557\n",
      "batch loss: 0.058785997331142426 | avg loss: 0.09482391245352725\n",
      "batch loss: 0.005507167894393206 | avg loss: 0.09398130165579957\n",
      "batch loss: 0.005771359894424677 | avg loss: 0.0931569096767213\n",
      "batch loss: 0.16353578865528107 | avg loss: 0.09380856596355981\n",
      "batch loss: 0.14190658926963806 | avg loss: 0.09424983223242292\n",
      "batch loss: 0.014078141190111637 | avg loss: 0.09352099867749282\n",
      "batch loss: 0.3189913332462311 | avg loss: 0.09555226295288685\n",
      "batch loss: 0.03092060796916485 | avg loss: 0.09497519460481792\n",
      "batch loss: 0.10832623392343521 | avg loss: 0.09509334539524815\n",
      "batch loss: 0.030296683311462402 | avg loss: 0.09452495362258337\n",
      "batch loss: 0.02191876247525215 | avg loss: 0.09389359543869352\n",
      "batch loss: 0.003176925703883171 | avg loss: 0.09311155518235896\n",
      "batch loss: 0.11913281679153442 | avg loss: 0.09333395912773652\n",
      "batch loss: 0.036402374505996704 | avg loss: 0.09285148807162008\n",
      "batch loss: 0.018101900815963745 | avg loss: 0.09222334027955574\n",
      "batch loss: 0.25735917687416077 | avg loss: 0.09359947225117746\n",
      "batch loss: 0.003359099617227912 | avg loss: 0.09285368404759109\n",
      "batch loss: 0.1499757021665573 | avg loss: 0.09332189731086131\n",
      "batch loss: 0.00543288653716445 | avg loss: 0.09260735250782312\n",
      "batch loss: 0.06263105571269989 | avg loss: 0.09236560817883019\n",
      "batch loss: 0.06722062826156616 | avg loss: 0.09216444833949208\n",
      "batch loss: 0.00227635120972991 | avg loss: 0.09145105074322413\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:13:40\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:01:52\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "batch loss: 0.03720793128013611 | avg loss: 0.03720793128013611\n",
      "batch loss: 0.10999059677124023 | avg loss: 0.07359926402568817\n",
      "batch loss: 0.20833081007003784 | avg loss: 0.11850977937380473\n",
      "batch loss: 0.06402771174907684 | avg loss: 0.10488926246762276\n",
      "batch loss: 0.011455235071480274 | avg loss: 0.08620245698839427\n",
      "batch loss: 0.33635494112968445 | avg loss: 0.12789453767860928\n",
      "batch loss: 0.005031169392168522 | avg loss: 0.11034262792340346\n",
      "batch loss: 0.07987075299024582 | avg loss: 0.10653364355675876\n",
      "batch loss: 0.07399703562259674 | avg loss: 0.10291846489740743\n",
      "batch loss: 0.0065382253378629684 | avg loss: 0.09328044094145298\n",
      "batch loss: 0.00316034653224051 | avg loss: 0.08508770508607003\n",
      "batch loss: 0.003936430439352989 | avg loss: 0.07832509886551027\n",
      "batch loss: 0.07536861300468445 | avg loss: 0.07809767687621598\n",
      "batch loss: 0.0023419579956680536 | avg loss: 0.07268655409903399\n",
      "batch loss: 0.05947296693921089 | avg loss: 0.07180564828837911\n",
      "batch loss: 0.005076249595731497 | avg loss: 0.06763506087008864\n",
      "batch loss: 0.006250241305679083 | avg loss: 0.06402418913100572\n",
      "batch loss: 0.01583844982087612 | avg loss: 0.0613472036137763\n",
      "batch loss: 0.024771887809038162 | avg loss: 0.05942218699247429\n",
      "batch loss: 0.0023433684837073088 | avg loss: 0.05656824606703594\n",
      "batch loss: 0.1265777200460434 | avg loss: 0.05990203054222677\n",
      "batch loss: 0.044537175446748734 | avg loss: 0.05920362803788686\n",
      "batch loss: 0.008997524157166481 | avg loss: 0.05702075395611641\n",
      "batch loss: 0.00397072359919548 | avg loss: 0.05481033602457804\n",
      "batch loss: 0.1121230497956276 | avg loss: 0.057102844575420024\n",
      "batch loss: 0.3100263774394989 | avg loss: 0.06683067276249997\n",
      "batch loss: 0.02090638130903244 | avg loss: 0.06512977307903822\n",
      "batch loss: 0.2556374669075012 | avg loss: 0.07193361928719762\n",
      "batch loss: 0.002494591986760497 | avg loss: 0.06953917006994116\n",
      "batch loss: 0.01681622676551342 | avg loss: 0.06778173862646024\n",
      "batch loss: 0.06433441489934921 | avg loss: 0.06767053463526311\n",
      "batch loss: 0.08148925751447678 | avg loss: 0.06810236972523853\n",
      "batch loss: 0.21758529543876648 | avg loss: 0.0726321553529212\n",
      "batch loss: 0.14173734188079834 | avg loss: 0.07466466083903522\n",
      "batch loss: 0.01024225540459156 | avg loss: 0.07282402068376541\n",
      "batch loss: 0.5002230405807495 | avg loss: 0.08469621568090385\n",
      "batch loss: 0.08410438895225525 | avg loss: 0.08468022036391336\n",
      "batch loss: 0.022393614053726196 | avg loss: 0.08304109914522421\n",
      "batch loss: 0.03411545231938362 | avg loss: 0.08178659538045907\n",
      "batch loss: 0.1421957165002823 | avg loss: 0.08329682340845465\n",
      "batch loss: 0.004771094769239426 | avg loss: 0.08138156173432745\n",
      "batch loss: 0.010892883874475956 | avg loss: 0.07970325988052147\n",
      "batch loss: 0.01730072684586048 | avg loss: 0.07825203818204098\n",
      "batch loss: 0.3271375596523285 | avg loss: 0.0839085273063657\n",
      "batch loss: 0.014312978833913803 | avg loss: 0.08236195956253343\n",
      "batch loss: 0.19449138641357422 | avg loss: 0.08479955579842562\n",
      "batch loss: 0.005656730383634567 | avg loss: 0.08311566589598327\n",
      "batch loss: 0.00164388632401824 | avg loss: 0.08141833715490066\n",
      "batch loss: 0.016080809757113457 | avg loss: 0.08008491822841521\n",
      "batch loss: 0.005322190001606941 | avg loss: 0.07858966366387904\n",
      "batch loss: 0.3735736012458801 | avg loss: 0.08437366243999671\n",
      "batch loss: 0.017090432345867157 | avg loss: 0.08307975416895576\n",
      "batch loss: 0.0018030267674475908 | avg loss: 0.08154623101043673\n",
      "batch loss: 0.003104503033682704 | avg loss: 0.08009360641827462\n",
      "batch loss: 0.2356937676668167 | avg loss: 0.0829227002591572\n",
      "batch loss: 0.052759800106287 | avg loss: 0.08238407704214167\n",
      "batch loss: 0.0028349447529762983 | avg loss: 0.08098847823005104\n",
      "batch loss: 0.06195025518536568 | avg loss: 0.08066023300514268\n",
      "batch loss: 0.00263084564357996 | avg loss: 0.07933770101596364\n",
      "batch loss: 0.16144844889640808 | avg loss: 0.08070621348063772\n",
      "batch loss: 0.0020677002612501383 | avg loss: 0.07941705752622152\n",
      "batch loss: 0.32101085782051086 | avg loss: 0.08331373172451652\n",
      "batch loss: 0.002688976004719734 | avg loss: 0.08203397369721815\n",
      "batch loss: 0.02773412875831127 | avg loss: 0.08118553862004774\n",
      "batch loss: 0.14615319669246674 | avg loss: 0.08218504105193111\n",
      "batch loss: 0.12896192073822021 | avg loss: 0.08289378165323852\n",
      "batch loss: 0.06410130858421326 | avg loss: 0.08261329698056649\n",
      "batch loss: 0.003722412744536996 | avg loss: 0.08145313691827194\n",
      "batch loss: 0.0170728899538517 | avg loss: 0.08052008986081657\n",
      "batch loss: 0.00350602762773633 | avg loss: 0.07941988897177257\n",
      "batch loss: 0.004900387953966856 | avg loss: 0.07837031853490208\n",
      "batch loss: 0.0031987472902983427 | avg loss: 0.07732626893428257\n",
      "batch loss: 0.006947176996618509 | avg loss: 0.07636217178445157\n",
      "batch loss: 0.012886656448245049 | avg loss: 0.0755043945501785\n",
      "batch loss: 0.0015109998639672995 | avg loss: 0.07451781595436235\n",
      "batch loss: 0.11186976730823517 | avg loss: 0.07500928899849225\n",
      "batch loss: 0.20914314687252045 | avg loss: 0.07675128715270042\n",
      "batch loss: 0.1642892062664032 | avg loss: 0.07787356816697866\n",
      "batch loss: 0.2201423943042755 | avg loss: 0.07967443938390646\n",
      "batch loss: 0.00856783427298069 | avg loss: 0.0787856068200199\n",
      "batch loss: 0.027451638132333755 | avg loss: 0.07815185412017192\n",
      "batch loss: 0.004358479753136635 | avg loss: 0.07725193492057393\n",
      "batch loss: 0.0020530016627162695 | avg loss: 0.07634592367650335\n",
      "batch loss: 0.0024980581365525723 | avg loss: 0.07546678242007536\n",
      "batch loss: 0.1511034220457077 | avg loss: 0.07635662523920045\n",
      "batch loss: 0.012446234934031963 | avg loss: 0.07561348116588454\n",
      "batch loss: 0.003172491444274783 | avg loss: 0.07478082611161316\n",
      "batch loss: 0.03932345286011696 | avg loss: 0.07437790141557343\n",
      "batch loss: 0.14885734021663666 | avg loss: 0.07521474904255167\n",
      "batch loss: 0.07008568197488785 | avg loss: 0.07515775940846652\n",
      "batch loss: 0.11814077198505402 | avg loss: 0.07563010020601144\n",
      "batch loss: 0.010352982208132744 | avg loss: 0.07492056631473014\n",
      "batch loss: 0.006176070310175419 | avg loss: 0.0741813781856489\n",
      "batch loss: 0.029675837606191635 | avg loss: 0.07370791498799512\n",
      "batch loss: 0.0555235855281353 | avg loss: 0.0735165009936808\n",
      "batch loss: 0.0015657616313546896 | avg loss: 0.07276701412532323\n",
      "batch loss: 0.0026924749836325645 | avg loss: 0.07204459619602745\n",
      "batch loss: 0.1626966893672943 | avg loss: 0.07296961755491793\n",
      "batch loss: 0.13300421833992004 | avg loss: 0.07357602766385735\n",
      "batch loss: 0.19008778035640717 | avg loss: 0.07474114519078284\n",
      "batch loss: 0.0019101303769275546 | avg loss: 0.07402004603421002\n",
      "batch loss: 0.0018178201280534267 | avg loss: 0.07331218107434574\n",
      "batch loss: 0.0015894395764917135 | avg loss: 0.07261584377825007\n",
      "batch loss: 0.0030088804196566343 | avg loss: 0.07194654605364821\n",
      "batch loss: 0.003755164798349142 | avg loss: 0.07129710432740727\n",
      "batch loss: 0.0014451127499341965 | avg loss: 0.07063812327478959\n",
      "batch loss: 0.0014443431282415986 | avg loss: 0.0699914524322985\n",
      "batch loss: 0.11288100481033325 | avg loss: 0.0703885779172803\n",
      "batch loss: 0.06182102486491203 | avg loss: 0.07030997651313013\n",
      "batch loss: 0.0050027635879814625 | avg loss: 0.06971627457744696\n",
      "batch loss: 0.3452768921852112 | avg loss: 0.0721988026640034\n",
      "batch loss: 0.14862105250358582 | avg loss: 0.07288114418042824\n",
      "batch loss: 0.0918223038315773 | avg loss: 0.07304876506229681\n",
      "batch loss: 0.001603456330485642 | avg loss: 0.07242205182780724\n",
      "batch loss: 0.002239107619971037 | avg loss: 0.07181176535643476\n",
      "batch loss: 0.00183878222014755 | avg loss: 0.07120854998457021\n",
      "batch loss: 0.09607531130313873 | avg loss: 0.07142108640609643\n",
      "batch loss: 0.008164336904883385 | avg loss: 0.07088501225778107\n",
      "batch loss: 0.014895934611558914 | avg loss: 0.07041451580697249\n",
      "batch loss: 0.2046772688627243 | avg loss: 0.07153337208243708\n",
      "batch loss: 0.0016003729542717338 | avg loss: 0.07095541341195637\n",
      "batch loss: 0.03514094278216362 | avg loss: 0.07066185217728595\n",
      "batch loss: 0.0040909117087721825 | avg loss: 0.07012062501900534\n",
      "batch loss: 0.006013884674757719 | avg loss: 0.06960363517751947\n",
      "batch loss: 0.051854681223630905 | avg loss: 0.06946164354588837\n",
      "batch loss: 0.0013463463401421905 | avg loss: 0.06892104594901736\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:13:59\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:56\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "batch loss: 0.002822360023856163 | avg loss: 0.002822360023856163\n",
      "batch loss: 0.018234839662909508 | avg loss: 0.010528599843382835\n",
      "batch loss: 0.16314613819122314 | avg loss: 0.06140111262599627\n",
      "batch loss: 0.0014707647496834397 | avg loss: 0.046418525656918064\n",
      "batch loss: 0.0026519987732172012 | avg loss: 0.037665220280177894\n",
      "batch loss: 0.2459610402584076 | avg loss: 0.0723811902765495\n",
      "batch loss: 0.03184394910931587 | avg loss: 0.06659015582408756\n",
      "batch loss: 0.0030606756918132305 | avg loss: 0.05864897080755327\n",
      "batch loss: 0.018228407949209213 | avg loss: 0.05415779715662615\n",
      "batch loss: 0.0013658811803907156 | avg loss: 0.04887860555900261\n",
      "batch loss: 0.0011655527632683516 | avg loss: 0.04454105530484495\n",
      "batch loss: 0.0019571962766349316 | avg loss: 0.040992400385827445\n",
      "batch loss: 0.1061682179570198 | avg loss: 0.046005924814380705\n",
      "batch loss: 0.0010190425673499703 | avg loss: 0.04279257608244994\n",
      "batch loss: 0.0023986392188817263 | avg loss: 0.04009964695821206\n",
      "batch loss: 0.012005051597952843 | avg loss: 0.03834373474819586\n",
      "batch loss: 0.03805365040898323 | avg loss: 0.03832667096353629\n",
      "batch loss: 0.0028818428982049227 | avg loss: 0.03635751384879566\n",
      "batch loss: 0.08876009285449982 | avg loss: 0.03911554432278009\n",
      "batch loss: 0.001123677589930594 | avg loss: 0.037215950986137614\n",
      "batch loss: 0.10406596958637238 | avg loss: 0.04039928520519641\n",
      "batch loss: 0.03021264635026455 | avg loss: 0.03993625616633587\n",
      "batch loss: 0.0012145891087129712 | avg loss: 0.03825270542470009\n",
      "batch loss: 0.002200313610956073 | avg loss: 0.03675052243246076\n",
      "batch loss: 0.14501246809959412 | avg loss: 0.04108100025914609\n",
      "batch loss: 0.22863620519638062 | avg loss: 0.04829466198750127\n",
      "batch loss: 0.002810672391206026 | avg loss: 0.04661006978023107\n",
      "batch loss: 0.3467543423175812 | avg loss: 0.057329508085136434\n",
      "batch loss: 0.005178103223443031 | avg loss: 0.0555311837795608\n",
      "batch loss: 0.0072382655926048756 | avg loss: 0.053921419839995606\n",
      "batch loss: 0.1528209000825882 | avg loss: 0.057111725654272784\n",
      "batch loss: 0.11209490895271301 | avg loss: 0.05882995013234904\n",
      "batch loss: 0.15470212697982788 | avg loss: 0.06173516761257567\n",
      "batch loss: 0.10232803970575333 | avg loss: 0.06292907561531619\n",
      "batch loss: 0.016852661967277527 | avg loss: 0.061612606653943656\n",
      "batch loss: 0.3915161192417145 | avg loss: 0.07077659311471507\n",
      "batch loss: 0.031939130276441574 | avg loss: 0.0697269319569239\n",
      "batch loss: 0.04075070470571518 | avg loss: 0.06896439966083945\n",
      "batch loss: 0.009870234876871109 | avg loss: 0.06744916466637872\n",
      "batch loss: 0.1439330279827118 | avg loss: 0.06936126124928706\n",
      "batch loss: 0.0048481859266757965 | avg loss: 0.06778777160727215\n",
      "batch loss: 0.17479553818702698 | avg loss: 0.07033557557345678\n",
      "batch loss: 0.1840893179178238 | avg loss: 0.07298101144193044\n",
      "batch loss: 0.4089927673339844 | avg loss: 0.08061764225765894\n",
      "batch loss: 0.002333537209779024 | avg loss: 0.07887799547881716\n",
      "batch loss: 0.2722136080265045 | avg loss: 0.08308094357767992\n",
      "batch loss: 0.0026490739546716213 | avg loss: 0.0813696272027223\n",
      "batch loss: 0.0010236753150820732 | avg loss: 0.07969575320506313\n",
      "batch loss: 0.005629743915051222 | avg loss: 0.07818420199506289\n",
      "batch loss: 0.0016570044681429863 | avg loss: 0.0766536580445245\n",
      "batch loss: 0.3931805193424225 | avg loss: 0.08286006708958131\n",
      "batch loss: 0.0015259626088663936 | avg loss: 0.08129594969572142\n",
      "batch loss: 0.0017677926225587726 | avg loss: 0.07979541843019004\n",
      "batch loss: 0.0032856487669050694 | avg loss: 0.07837857084383291\n",
      "batch loss: 0.18644165992736816 | avg loss: 0.08034335428171537\n",
      "batch loss: 0.29113513231277466 | avg loss: 0.08410749317512714\n",
      "batch loss: 0.001797224278561771 | avg loss: 0.08266345336992424\n",
      "batch loss: 0.17490994930267334 | avg loss: 0.08425391019635095\n",
      "batch loss: 0.05983375012874603 | avg loss: 0.08384000917825596\n",
      "batch loss: 0.16300849616527557 | avg loss: 0.08515948396137295\n",
      "batch loss: 0.006900737527757883 | avg loss: 0.08387655369196942\n",
      "batch loss: 0.3048533797264099 | avg loss: 0.08744069604736363\n",
      "batch loss: 0.0025563014205545187 | avg loss: 0.08609332470408095\n",
      "batch loss: 0.005761151667684317 | avg loss: 0.08483813450038724\n",
      "batch loss: 0.015606758184731007 | avg loss: 0.0837730364032233\n",
      "batch loss: 0.06066667661070824 | avg loss: 0.08342294004273065\n",
      "batch loss: 0.07307860255241394 | avg loss: 0.08326854694586025\n",
      "batch loss: 0.004370011854916811 | avg loss: 0.08210827437099344\n",
      "batch loss: 0.05784526839852333 | avg loss: 0.08175663660327648\n",
      "batch loss: 0.13463273644447327 | avg loss: 0.08251200945815071\n",
      "batch loss: 0.008905578404664993 | avg loss: 0.08147529916162274\n",
      "batch loss: 0.024322761222720146 | avg loss: 0.08068151391247132\n",
      "batch loss: 0.0025714680086821318 | avg loss: 0.07961151328365229\n",
      "batch loss: 0.0019478400936350226 | avg loss: 0.0785620041864899\n",
      "batch loss: 0.0015609721885994077 | avg loss: 0.07753532375985135\n",
      "batch loss: 0.008445372804999352 | avg loss: 0.07662624545781382\n",
      "batch loss: 0.05799975246191025 | avg loss: 0.07638434295137353\n",
      "batch loss: 0.12326756119728088 | avg loss: 0.07698540985196207\n",
      "batch loss: 0.05693347752094269 | avg loss: 0.07673158792372133\n",
      "batch loss: 0.009072139859199524 | avg loss: 0.0758858448229148\n",
      "batch loss: 0.07952066510915756 | avg loss: 0.07593071914743632\n",
      "batch loss: 0.0033421586267650127 | avg loss: 0.07504549279962326\n",
      "batch loss: 0.005929512437433004 | avg loss: 0.0742127701446571\n",
      "batch loss: 0.003476829268038273 | avg loss: 0.07337067561041165\n",
      "batch loss: 0.1138506606221199 | avg loss: 0.07384691072819645\n",
      "batch loss: 0.06650014221668243 | avg loss: 0.07376148318736489\n",
      "batch loss: 0.0013053123839199543 | avg loss: 0.0729286536379\n",
      "batch loss: 0.020761599764227867 | avg loss: 0.07233584620751737\n",
      "batch loss: 0.01947055011987686 | avg loss: 0.071741854116645\n",
      "batch loss: 0.17291530966758728 | avg loss: 0.07286600362276659\n",
      "batch loss: 0.01639486663043499 | avg loss: 0.07224544167779591\n",
      "batch loss: 0.00606527179479599 | avg loss: 0.0715260920051546\n",
      "batch loss: 0.01808045618236065 | avg loss: 0.07095140774899553\n",
      "batch loss: 0.004615574609488249 | avg loss: 0.07024570739644757\n",
      "batch loss: 0.04196673631668091 | avg loss: 0.06994803401666057\n",
      "batch loss: 0.0014856195775792003 | avg loss: 0.06923488386625347\n",
      "batch loss: 0.004055483266711235 | avg loss: 0.06856293128275302\n",
      "batch loss: 0.1257113367319107 | avg loss: 0.06914607827713219\n",
      "batch loss: 0.05666764825582504 | avg loss: 0.06902003352944222\n",
      "batch loss: 0.14009374380111694 | avg loss: 0.06973077063215896\n",
      "batch loss: 0.0014085185248404741 | avg loss: 0.06905431269050234\n",
      "batch loss: 0.002080636564642191 | avg loss: 0.06839770802260176\n",
      "batch loss: 0.001502411556430161 | avg loss: 0.06774823912487193\n",
      "batch loss: 0.05011003464460373 | avg loss: 0.06757864100486935\n",
      "batch loss: 0.0075627099722623825 | avg loss: 0.06700706070932072\n",
      "batch loss: 0.0010305484756827354 | avg loss: 0.06638464078258828\n",
      "batch loss: 0.001519939280115068 | avg loss: 0.06577842861901377\n",
      "batch loss: 0.030029693618416786 | avg loss: 0.06544742181345269\n",
      "batch loss: 0.09734735637903214 | avg loss: 0.06574008176359561\n",
      "batch loss: 0.010256124660372734 | avg loss: 0.06523568215356632\n",
      "batch loss: 0.18564888834953308 | avg loss: 0.06632048581298944\n",
      "batch loss: 0.0019337953999638557 | avg loss: 0.06574560464858743\n",
      "batch loss: 0.09633606672286987 | avg loss: 0.06601631670234213\n",
      "batch loss: 0.011510089971125126 | avg loss: 0.06553819190645427\n",
      "batch loss: 0.0010435385629534721 | avg loss: 0.0649773688339021\n",
      "batch loss: 0.0017058891244232655 | avg loss: 0.06443192504330314\n",
      "batch loss: 0.005557554308325052 | avg loss: 0.06392872529343152\n",
      "batch loss: 0.030777396634221077 | avg loss: 0.06364778183021788\n",
      "batch loss: 0.0026204127352684736 | avg loss: 0.06313494679580654\n",
      "batch loss: 0.06574388593435287 | avg loss: 0.06315668795529443\n",
      "batch loss: 0.0011313711293041706 | avg loss: 0.06264408203111269\n",
      "batch loss: 0.0149209750816226 | avg loss: 0.06225290902332998\n",
      "batch loss: 0.04260748624801636 | avg loss: 0.062093190138977836\n",
      "batch loss: 0.012627936899662018 | avg loss: 0.06169427680640271\n",
      "batch loss: 0.057416561990976334 | avg loss: 0.0616600550878793\n",
      "batch loss: 0.0008873302722349763 | avg loss: 0.06117773187505673\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:12:35\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:57\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "batch loss: 0.004318834748119116 | avg loss: 0.004318834748119116\n",
      "batch loss: 0.001971802208572626 | avg loss: 0.003145318478345871\n",
      "batch loss: 0.2195529341697693 | avg loss: 0.07528119037548701\n",
      "batch loss: 0.0010117944329977036 | avg loss: 0.05671384138986468\n",
      "batch loss: 0.0012741356622427702 | avg loss: 0.0456259002443403\n",
      "batch loss: 0.2018287181854248 | avg loss: 0.07165970323452105\n",
      "batch loss: 0.016734007745981216 | avg loss: 0.06381317530758679\n",
      "batch loss: 0.015652157366275787 | avg loss: 0.057793048064922914\n",
      "batch loss: 0.011745515279471874 | avg loss: 0.052676655533206135\n",
      "batch loss: 0.001506028464064002 | avg loss: 0.04755959282629192\n",
      "batch loss: 0.0014225180493667722 | avg loss: 0.043365313301116905\n",
      "batch loss: 0.0012907140189781785 | avg loss: 0.03985909669427201\n",
      "batch loss: 0.024577386677265167 | avg loss: 0.03868358053911764\n",
      "batch loss: 0.0008293288410641253 | avg loss: 0.0359797054178281\n",
      "batch loss: 0.0015729345614090562 | avg loss: 0.03368592069406683\n",
      "batch loss: 0.005570999812334776 | avg loss: 0.03192873813895858\n",
      "batch loss: 0.0647704228758812 | avg loss: 0.03386060194701285\n",
      "batch loss: 0.0016264015575870872 | avg loss: 0.032069813036489196\n",
      "batch loss: 0.02086123265326023 | avg loss: 0.03147988775316136\n",
      "batch loss: 0.0034643870312720537 | avg loss: 0.03007911271706689\n",
      "batch loss: 0.0032751031685620546 | avg loss: 0.028802731309995232\n",
      "batch loss: 0.010190149769186974 | avg loss: 0.02795670487632213\n",
      "batch loss: 0.0009856477845460176 | avg loss: 0.02678405022015795\n",
      "batch loss: 0.0012686108238995075 | avg loss: 0.025720906911980517\n",
      "batch loss: 0.12267355620861053 | avg loss: 0.029599012883845716\n",
      "batch loss: 0.19419579207897186 | avg loss: 0.035929658237504415\n",
      "batch loss: 0.0008547037141397595 | avg loss: 0.03463058584775017\n",
      "batch loss: 0.28845155239105225 | avg loss: 0.04369562036715381\n",
      "batch loss: 0.000856920494697988 | avg loss: 0.04221842381982775\n",
      "batch loss: 0.0075808060355484486 | avg loss: 0.04106383656035177\n",
      "batch loss: 0.05649572238326073 | avg loss: 0.04156163932883271\n",
      "batch loss: 0.13065871596336365 | avg loss: 0.0443459229736618\n",
      "batch loss: 0.16951385140419006 | avg loss: 0.04813889050185963\n",
      "batch loss: 0.060082271695137024 | avg loss: 0.048490166419308964\n",
      "batch loss: 0.008026385679841042 | avg loss: 0.047334058398181306\n",
      "batch loss: 0.31206727027893066 | avg loss: 0.05468775872820212\n",
      "batch loss: 0.016518551856279373 | avg loss: 0.05365615854247448\n",
      "batch loss: 0.03691742196679115 | avg loss: 0.05321566547469334\n",
      "batch loss: 0.0012603754876181483 | avg loss: 0.051883478551947824\n",
      "batch loss: 0.10098636150360107 | avg loss: 0.053111050625739155\n",
      "batch loss: 0.0012571157421916723 | avg loss: 0.05184632050662824\n",
      "batch loss: 0.2722170948982239 | avg loss: 0.05709324370642813\n",
      "batch loss: 0.07038676738739014 | avg loss: 0.05740239541993888\n",
      "batch loss: 0.3359726667404175 | avg loss: 0.06373353794994975\n",
      "batch loss: 0.0018603680655360222 | avg loss: 0.06235857861918501\n",
      "batch loss: 0.15967388451099396 | avg loss: 0.06447412874726781\n",
      "batch loss: 0.0011369955027475953 | avg loss: 0.06312653016759717\n",
      "batch loss: 0.0011104450095444918 | avg loss: 0.06183452839347107\n",
      "batch loss: 0.004495457746088505 | avg loss: 0.06066434327821837\n",
      "batch loss: 0.0012309971498325467 | avg loss: 0.059475676355650646\n",
      "batch loss: 0.47523921728134155 | avg loss: 0.06762790264831126\n",
      "batch loss: 0.000933794945012778 | avg loss: 0.06634532365401705\n",
      "batch loss: 0.0011189093347638845 | avg loss: 0.06511463659138964\n",
      "batch loss: 0.0016400606837123632 | avg loss: 0.0639391814819882\n",
      "batch loss: 0.12207241356372833 | avg loss: 0.06499614933801984\n",
      "batch loss: 0.011104961857199669 | avg loss: 0.06403380670443377\n",
      "batch loss: 0.0011004430707544088 | avg loss: 0.0629297126055973\n",
      "batch loss: 0.0030227224342525005 | avg loss: 0.06189683346471203\n",
      "batch loss: 0.0011800163192674518 | avg loss: 0.06086773486902653\n",
      "batch loss: 0.15171292424201965 | avg loss: 0.062381821358576416\n",
      "batch loss: 0.0014244482154026628 | avg loss: 0.06138252015950799\n",
      "batch loss: 0.27447980642318726 | avg loss: 0.06481957316376089\n",
      "batch loss: 0.0013415541034191847 | avg loss: 0.06381198555962848\n",
      "batch loss: 0.001561446813866496 | avg loss: 0.06283932089172595\n",
      "batch loss: 0.004460450261831284 | avg loss: 0.0619411844204968\n",
      "batch loss: 0.03214007988572121 | avg loss: 0.06148965253360626\n",
      "batch loss: 0.0039709871634840965 | avg loss: 0.060631164990768614\n",
      "batch loss: 0.003169739618897438 | avg loss: 0.059786144029417565\n",
      "batch loss: 0.05002165958285332 | avg loss: 0.05964462976207606\n",
      "batch loss: 0.004517485853284597 | avg loss: 0.058857099134807606\n",
      "batch loss: 0.007281104568392038 | avg loss: 0.058130676676125696\n",
      "batch loss: 0.015930235385894775 | avg loss: 0.057544559435983605\n",
      "batch loss: 0.001349442871287465 | avg loss: 0.056774763318659\n",
      "batch loss: 0.006493454799056053 | avg loss: 0.0560952861765022\n",
      "batch loss: 0.0009512767428532243 | avg loss: 0.05536003271738688\n",
      "batch loss: 0.004057691432535648 | avg loss: 0.05468500191100726\n",
      "batch loss: 0.15151673555374146 | avg loss: 0.05594255689338043\n",
      "batch loss: 0.10940346121788025 | avg loss: 0.05662795310266889\n",
      "batch loss: 0.10717790573835373 | avg loss: 0.05726782592084212\n",
      "batch loss: 0.007728526368737221 | avg loss: 0.05664858467644081\n",
      "batch loss: 0.047588083893060684 | avg loss: 0.05653672664207809\n",
      "batch loss: 0.0011348160915076733 | avg loss: 0.055861093586583324\n",
      "batch loss: 0.007900835946202278 | avg loss: 0.055283259157181144\n",
      "batch loss: 0.001346016419120133 | avg loss: 0.05464114912458518\n",
      "batch loss: 0.03714054822921753 | avg loss: 0.054435259702286735\n",
      "batch loss: 0.008505321107804775 | avg loss: 0.05390119064886253\n",
      "batch loss: 0.0032232594676315784 | avg loss: 0.053318685692756426\n",
      "batch loss: 0.03277280181646347 | avg loss: 0.05308520973961673\n",
      "batch loss: 0.14251050353050232 | avg loss: 0.054089988321536796\n",
      "batch loss: 0.12445148825645447 | avg loss: 0.0548717827652581\n",
      "batch loss: 0.18883280456066132 | avg loss: 0.05634388190586693\n",
      "batch loss: 0.0017683658516034484 | avg loss: 0.055750669774842325\n",
      "batch loss: 0.00218701409175992 | avg loss: 0.05517471648792746\n",
      "batch loss: 0.006565493065863848 | avg loss: 0.0546575970898204\n",
      "batch loss: 0.05672942474484444 | avg loss: 0.05467940580197855\n",
      "batch loss: 0.002776859560981393 | avg loss: 0.05413875427863483\n",
      "batch loss: 0.001367876771837473 | avg loss: 0.0535947246136163\n",
      "batch loss: 0.0076792361214756966 | avg loss: 0.05312619922083935\n",
      "batch loss: 0.05280057713389397 | avg loss: 0.05312291010885001\n",
      "batch loss: 0.043731529265642166 | avg loss: 0.05302899630041793\n",
      "batch loss: 0.0008992067887447774 | avg loss: 0.052512859770599385\n",
      "batch loss: 0.0021726349368691444 | avg loss: 0.05201932815458242\n",
      "batch loss: 0.001203865627758205 | avg loss: 0.05152597414946762\n",
      "batch loss: 0.0009976071305572987 | avg loss: 0.051040124466593485\n",
      "batch loss: 0.00392401497811079 | avg loss: 0.05059139961432222\n",
      "batch loss: 0.0013319577556103468 | avg loss: 0.05012668789867399\n",
      "batch loss: 0.001626312849111855 | avg loss: 0.04967341336550052\n",
      "batch loss: 0.004226914141327143 | avg loss: 0.04925261244675817\n",
      "batch loss: 0.06107162684202194 | avg loss: 0.04936104377148536\n",
      "batch loss: 0.0038073514588177204 | avg loss: 0.04894691929591566\n",
      "batch loss: 0.09588555991649628 | avg loss: 0.0493697899321371\n",
      "batch loss: 0.002916162833571434 | avg loss: 0.04895502540447134\n",
      "batch loss: 0.058934397995471954 | avg loss: 0.049043338436250104\n",
      "batch loss: 0.002024868968874216 | avg loss: 0.048630895721624004\n",
      "batch loss: 0.0016884448705241084 | avg loss: 0.048222700496831826\n",
      "batch loss: 0.0024642557837069035 | avg loss: 0.04782823114585661\n",
      "batch loss: 0.011655369773507118 | avg loss: 0.04751906139053739\n",
      "batch loss: 0.030911486595869064 | avg loss: 0.047378319231260534\n",
      "batch loss: 0.005291852634400129 | avg loss: 0.0470246514447323\n",
      "batch loss: 0.09827406704425812 | avg loss: 0.04745172990806168\n",
      "batch loss: 0.0009444419993087649 | avg loss: 0.04706737215674967\n",
      "batch loss: 0.023697156459093094 | avg loss: 0.04687581301168691\n",
      "batch loss: 0.0026552677154541016 | avg loss: 0.046516296383262254\n",
      "batch loss: 0.005527750123292208 | avg loss: 0.04618574359084314\n",
      "batch loss: 0.039369598031044006 | avg loss: 0.04613121442636475\n",
      "batch loss: 0.0008281435584649444 | avg loss: 0.04577166624487348\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:12:40\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:58\n",
      "[[0.65292842 0.34707158]\n",
      " [0.30275229 0.69724771]]\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "#function to train the model\n",
    "def training(n_epochs, training_dataloader,\n",
    "             validation_dataloader):\n",
    "    # ========================================\n",
    "    #               Training \n",
    "    # ========================================\n",
    "    print('======= Training =======')\n",
    "    for epoch_i in range(0,n_epochs):\n",
    "        # Perform one full pass over the training set\n",
    "        print(\"\")\n",
    "        print('======= Epoch {:} / {:} ======='.format(\n",
    "             epoch_i + 1, epochs))\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "        # For each batch of training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Clear any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # pull loss value out of the output tuple\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass \n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            # ¿take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            print('batch loss: {0} | avg loss: {1}'.format(\n",
    "                  batch_loss, total_loss/(step+1)))\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".\n",
    "             format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(\n",
    "              format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, \n",
    "        # measure accuracy on the validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"======= Validation =======\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # Model will not to compute gradients\n",
    "            with torch.no_grad():\n",
    "                # Forward pass \n",
    "                # This will return the logits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # The \"logits\" are the output values \n",
    "            # prior to applying an activation function \n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Save batch logits and labels \n",
    "            # We will use thoses in the confusion matrix\n",
    "            predict_labels = np.argmax(\n",
    "                             logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calculate the accuracy for this batch\n",
    "            tmp_eval_accuracy = flat_accuracy(\n",
    "                                logits, b_labels)\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".\n",
    "              format(eval_accuracy / (step+1)))\n",
    "        print(\"  Validation took: {:}\".format(\n",
    "             format_time(time.time() - t0)))\n",
    "\n",
    "    #print the confusion matrix\"\n",
    "    conf = confusion_matrix(\n",
    "           all_labels, all_logits, normalize='true')\n",
    "    print(conf)\n",
    "    print(\"\")\n",
    "    print(\"Training complete\")\n",
    "\n",
    "#call function to train the model\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc933b5c-1c7c-4243-b1f5-7dbebf6d56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('Models/model2')  \n",
    "torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d268ce55-e625-4816-a560-7807b2407bda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6959/3090116859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YlGnBu_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#plot the confusion matrix\n",
    "conf = [[0.66160521, 0.33839479],\n",
    " [0.29357798, 0.70642202]]\n",
    "ticks = ['negative', 'positive']\n",
    "ax = sns.heatmap(conf, annot=True, xticklabels=ticks, yticklabels=ticks, cmap='YlGnBu_r')\n",
    "ax.set(xlabel='Predicted label', ylabel='True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a3789d-a545-4596-b753-dd21fdfa2fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graphing the validation accuracy and training loss\n",
    "\n",
    "epochs1 = [1, 2, 3, 4, 5]\n",
    "validation1 = [.66, .68, .69, .68, .69]\n",
    "training1 = [1-.66, 1-.53, 1-.36, 1-.23, 1-.13]\n",
    "\n",
    "epochs2 = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "training2 = [1-.66, 1-.55, 1-.36, 1-.24, 1-.23, 1-.14, 1-.08, 1-.06]\n",
    "validation2 = [.61, .64, .60, .67, .67, .67, .66]\n",
    "\n",
    "\n",
    "epochs3 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "training3 = [1-.66, 1-.55, 1-.36, 1-.24, 1-.23, 1-.14, 1-.08, 1-.06]\n",
    "validation3 = [.61, .64, .60, .67, .67, .67, .66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36e46866-9d9d-4bde-bbdd-1d88d46707f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's further analyze the positive ones and the negatives\n",
    "model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "        # Evaluate data for one epoch\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                    t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # Model will not to compute gradients\n",
    "        with torch.no_grad():\n",
    "                # Forward pass \n",
    "                # This will return the logits \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "            # The \"logits\" are the output values \n",
    "            # prior to applying an activation function \n",
    "        logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Save batch logits and labels \n",
    "            # We will use thoses in the confusion matrix\n",
    "        predict_labels = np.argmax(\n",
    "                         logits, axis=1).flatten()\n",
    "        all_logits.extend(predict_labels.tolist())\n",
    "        all_labels.extend(b_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82520fcb-e15b-4e30-b9d5-166672997eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_val.tolist()\n",
    "pred_frame = pd.DataFrame (list(zip(x_val,all_labels,all_logits)), \n",
    "                           columns = ['text_val', 'true_label', 'pred_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ba4377e-cec1-482f-b0f4-fa13b618d9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('Deep-AMLO/csvs/out.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "pred_frame.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6bd8dd-61c1-423e-88c3-0709c12ab798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model.softmax = torch.nn.Softmax(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f297c61-4f02-452d-90e8-d429afb7c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (softmax): Softmax(dim=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
